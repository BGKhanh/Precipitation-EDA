# -*- coding: utf-8 -*-
"""DS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10rzBvQ6nKOlvyehzrWCZo5L6bNgU7Hol

# CRAWL DATA
"""

import requests
import pandas as pd
import time
import os
from datetime import datetime
import json

class NASAPowerCrawlerCitySimplified:
    """
    Class Ä‘á»ƒ crawl dá»¯ liá»‡u tá»« NASA POWER API cho toÃ n thÃ nh phá»‘ TP.HCM
    Version Ä‘Æ¡n giáº£n vá»›i 20 parameters - crawl 1 láº§n duy nháº¥t
    """

    def __init__(self):
        self.base_url = "https://power.larc.nasa.gov/api/temporal/daily/point"

        self.parameters = [
            "PRECTOTCORR",           # Precipitation
            "QV2M",                  # Specific Humidity at 2 Meters
            "RH2M",                  # Relative Humidity at 2 Meters
            "T2M",                   # Temperature at 2 Meters
            "T2MDEW",                # Dew/Frost Point at 2 Meters
            "T2MWET",                # Wet Bulb Temperature at 2 Meters
            "T2M_MAX",               # Temperature at 2 Meters Maximum
            "TS",                    # Earth Skin Temperature
            "T2M_MIN",               # Temperature at 2 Meters Minimum
            "ALLSKY_SFC_LW_DWN",     # All Sky Surface Longwave Downward Irradiance
            "PS",                    # Surface Pressure
            "WS10M",                 # Wind Speed at 10 Meters
            "WD10M",                 # Wind Direction at 10 Meters
            "WS10M_MAX",             # Wind Speed at 10 Meters Maximum
            'WS2M_MAX',              # Wind Speed at 2 Meters Maximum
            'WS2M',                  # Wind Speed at 2 Meters
            'WD2M',                  # Wind Direction at 2 Meters
            "GWETPROF",              # Profile Soil Moisture
            "GWETTOP",               # Surface Soil Wetness
            "GWETROOT"               # Root Zone Soil Wetness
        ]

        # Tá»a Ä‘á»™ trung tÃ¢m TP.HCM
        self.hcmc_center = {
            "lat": 10.78,  # Trung tÃ¢m TP.HCM (chÃ­nh xÃ¡c hÆ¡n)
            "lon": 106.7
        }

        # Táº¡o thÆ° má»¥c lÆ°u dá»¯ liá»‡u
        self.output_dir = "nasa_power_hcmc_data"
        os.makedirs(self.output_dir, exist_ok=True)

    def crawl_data(self, start_date, end_date):
        """
        Crawl táº¥t cáº£ dá»¯ liá»‡u trong 1 láº§n duy nháº¥t

        Args:
            start_date: NgÃ y báº¯t Ä‘áº§u YYYYMMDD
            end_date: NgÃ y káº¿t thÃºc YYYYMMDD
        """
        print("ğŸš€ Báº®T Äáº¦U CRAWL Dá»® LIá»†U CHO TP.Há»’ CHÃ MINH")
        print("="*80)
        print(f"ğŸ“… Thá»i gian: {start_date} â†’ {end_date}")
        print(f"ğŸ“Š Tá»•ng sá»‘ parameters: {len(self.parameters)}")
        print(f"ğŸ“ Vá»‹ trÃ­: lat={self.hcmc_center['lat']}, lon={self.hcmc_center['lon']}")

        # Validate sá»‘ lÆ°á»£ng parameters
        if len(self.parameters) > 20:
            print(f"âš ï¸ Cáº¢NH BÃO: {len(self.parameters)} parameters vÆ°á»£t quÃ¡ giá»›i háº¡n API (20)")
            return None

        # Setup request parameters
        params = {
            "parameters": ",".join(self.parameters),
            "community": "RE",
            "longitude": self.hcmc_center["lon"],
            "latitude": self.hcmc_center["lat"],
            "start": start_date,
            "end": end_date,
            "format": "JSON"
        }

        print(f"\nğŸŒ Äang crawl dá»¯ liá»‡u tá»« NASA POWER API...")
        print(f"   ğŸ“Š Parameters: {len(self.parameters)} indicators")

        try:
            # Make API request
            response = requests.get(self.base_url, params=params, timeout=120)
            response.raise_for_status()

            data = response.json()

            # Kiá»ƒm tra lá»—i API
            if "messages" in data and data["messages"]:
                print(f"   âŒ Lá»—i API: {data['messages']}")
                return None

            # Parse JSON data
            properties = data.get("properties", {})
            parameters_data = properties.get("parameter", {})

            if not parameters_data:
                print(f"   âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘Æ°á»£c tráº£ vá»")
                return None

            # Táº¡o DataFrame tá»« JSON data
            df_dict = {}
            for param, values in parameters_data.items():
                df_dict[param] = values

            df = pd.DataFrame(df_dict)

            # ThÃªm metadata columns
            df["DATE"] = pd.to_datetime(df.index)
            df["LATITUDE"] = self.hcmc_center["lat"]
            df["LONGITUDE"] = self.hcmc_center["lon"]

            print(f"   âœ… Crawl thÃ nh cÃ´ng: {df.shape[0]:,} days Ã— {df.shape[1]} columns")

            # LÆ°u dataset
            saved_df = self.save_dataset(df, start_date, end_date)

            return saved_df

        except requests.exceptions.RequestException as e:
            print(f"   âŒ Lá»—i network request: {str(e)}")
            return None
        except Exception as e:
            print(f"   âŒ Lá»—i xá»­ lÃ½ dá»¯ liá»‡u: {str(e)}")
            return None

    def save_dataset(self, df, start_date, end_date):
        """
        LÆ°u dataset vÃ  táº¡o bÃ¡o cÃ¡o tÃ³m táº¯t
        """
        try:
            # Táº¡o tÃªn file
            output_filename = f"hcmc_weather_data_{start_date}_{end_date}.csv"
            output_filepath = os.path.join(self.output_dir, output_filename)

            # LÆ°u CSV file
            df.to_csv(output_filepath, index=False, encoding='utf-8')

            print(f"\nğŸ’¾ ÄÃƒ LÆ¯U DATASET THÃ€NH CÃ”NG!")
            print(f"   ğŸ“ File: {output_filename}")
            print(f"   ğŸ“‚ Path: {output_filepath}")
            print(f"   ğŸ“Š Shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns")
            print(f"   ğŸ“… Date range: {df['DATE'].min()} â†’ {df['DATE'].max()}")

            # Táº¡o bÃ¡o cÃ¡o chi tiáº¿t
            self.generate_data_summary(df)

            return df

        except Exception as e:
            print(f"   âŒ Lá»—i khi lÆ°u dataset: {str(e)}")
            return None

    def generate_data_summary(self, df):
        """
        Táº¡o bÃ¡o cÃ¡o tÃ³m táº¯t chi tiáº¿t vá» dá»¯ liá»‡u
        """
        print("\nğŸ“‹ BÃO CÃO TÃ“M Táº®T Dá»® LIá»†U WEATHER TP.HCM")
        print("="*60)

        # Basic info
        print(f"ğŸ“Š KÃ­ch thÆ°á»›c dataset: {df.shape[0]:,} rows Ã— {df.shape[1]} columns")
        print(f"ğŸ“… Khoáº£ng thá»i gian: {df['DATE'].min().strftime('%Y-%m-%d')} â†’ {df['DATE'].max().strftime('%Y-%m-%d')}")
        print(f"ğŸ“ˆ Sá»‘ ngÃ y dá»¯ liá»‡u: {df['DATE'].nunique():,} days")

        # TÃ­nh sá»‘ nÄƒm
        years = (df['DATE'].max() - df['DATE'].min()).days / 365.25
        print(f"â±ï¸ Tá»•ng thá»i gian: {years:.1f} nÄƒm")

        # Weather parameters info
        weather_params = [col for col in df.columns if col not in ['DATE', 'LATITUDE', 'LONGITUDE']]
        print(f"ğŸŒ¤ï¸ Sá»‘ weather parameters: {len(weather_params)}")

        print(f"\nğŸ“‹ Danh sÃ¡ch parameters:")
        for i, param in enumerate(weather_params, 1):
            print(f"   {i:2d}. {param}")

        # Data quality checks
        print(f"\nğŸ” KIá»‚M TRA CHáº¤T LÆ¯á»¢NG Dá»® LIá»†U:")

        # Missing values
        missing_info = df.isnull().sum()
        missing_cols = missing_info[missing_info > 0]

        if len(missing_cols) > 0:
            print(f"âš ï¸ Missing values tÃ¬m tháº¥y:")
            for col, count in missing_cols.items():
                pct = (count / len(df)) * 100
                print(f"   - {col}: {count:,} ({pct:.1f}%)")
        else:
            print(f"âœ… KhÃ´ng cÃ³ missing values")

        # Check for -999 (NASA missing indicator)
        print(f"\nğŸ” Kiá»ƒm tra giÃ¡ trá»‹ -999 (NASA missing indicator):")
        has_999 = False
        numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns

        for col in numeric_cols:
            count_999 = (df[col] == -999).sum()
            if count_999 > 0:
                pct = (count_999 / len(df)) * 100
                print(f"   âš ï¸ {col}: {count_999:,} values = -999 ({pct:.1f}%)")
                has_999 = True

        if not has_999:
            print(f"âœ… KhÃ´ng cÃ³ giÃ¡ trá»‹ -999")

    def validate_parameters(self):
        """
        Validate danh sÃ¡ch parameters
        """
        print("ğŸ” VALIDATION PARAMETERS:")
        print(f"   ğŸ“Š Tá»•ng sá»‘: {len(self.parameters)}")

        print(f"\nğŸ“‹ Danh sÃ¡ch parameters sáº½ crawl:")
        for i, param in enumerate(self.parameters, 1):
            print(f"   {i:2d}. {param}")

        return len(self.parameters) <= 20

def main():
    """
    HÃ m chÃ­nh Ä‘á»ƒ thá»±c hiá»‡n crawl dá»¯ liá»‡u
    """
    print("ğŸŒŸ NASA POWER WEATHER DATA CRAWLER FOR HO CHI MINH CITY")
    print("ğŸ”„ SIMPLIFIED VERSION - SINGLE REQUEST")
    print("="*80)

    # Khá»Ÿi táº¡o crawler
    crawler = NASAPowerCrawlerCitySimplified()

    # Validate parameters trÆ°á»›c khi crawl
    if not crawler.validate_parameters():
        print("âŒ Parameters validation failed!")
        return None

    # Äá»‹nh nghÄ©a khoáº£ng thá»i gian
    start_date_str = "20000101"  # 1/1/2000
    end_date_str = "20250430"    # 30/04/2025

    print(f"\nğŸ¯ Má»¥c tiÃªu: Dá»± Ä‘oÃ¡n thá»i tiáº¿t toÃ n TP.HCM")
    print(f"ğŸ“ Vá»‹ trÃ­: Trung tÃ¢m TP.HCM")
    print(f"âš ï¸ ÄÃ£ loáº¡i bá» 2 chá»‰ sá»‘ bá»©c xáº¡: ALLSKY_SFC_SW_DWN, TOA_SW_DWN")

    # Thá»±c hiá»‡n crawl
    final_dataset = crawler.crawl_data(start_date_str, end_date_str)

    if final_dataset is not None:
        print("\nğŸ‰ HOÃ€N Táº¤T CRAWL Dá»® LIá»†U THÃ€NH CÃ”NG!")
        print("="*80)
        print(f"âœ¨ Dataset sáºµn sÃ ng cho EDA vÃ  modeling")
        print(f"ğŸ“„ File Ä‘Ã£ lÆ°u trong thÆ° má»¥c: {crawler.output_dir}")
        return final_dataset
    else:
        print("\nâŒ CRAWL Dá»® LIá»†U THáº¤T Báº I!")
        return None

# Cháº¡y crawler
if __name__ == "__main__":
    final_dataset = main()
    if final_dataset is not None:
        print(f"\nğŸ¯ Sá»­ dá»¥ng biáº¿n 'final_dataset' Ä‘á»ƒ truy cáº­p dá»¯ liá»‡u")
        print(f"ğŸ“Š Shape: {final_dataset.shape}")
    print("HoÃ n táº¥t quÃ¡ trÃ¬nh crawl dá»¯ liá»‡u")

"""# ThÃ´ng tin bá»™ dá»¯ liá»‡u

**Dá»¯ liá»‡u NASA POWER API cho ThÃ nh phá»‘ Há»“ ChÃ­ Minh**

**a) Chá»©c nÄƒng (ná»™i dung) cá»§a bá»™ dá»¯ liá»‡u lÃ  gÃ¬?**

Bá»™ dá»¯ liá»‡u nÃ y chá»©a thÃ´ng tin khÃ­ tÆ°á»£ng vÃ  bá» máº·t Ä‘áº¥t hÃ ng ngÃ y cho ThÃ nh phá»‘ Há»“ ChÃ­ Minh, Ä‘Æ°á»£c thu tháº­p tá»± Ä‘á»™ng thÃ´ng qua API NASA Prediction Of Worldwide Energy Resources (POWER). Dá»¯ liá»‡u bao gá»“m nhiá»u tham sá»‘ quan trá»ng nhÆ°:

*   **Nhiá»‡t Ä‘á»™**: trung bÃ¬nh, tá»‘i thiá»ƒu, tá»‘i Ä‘a, Ä‘iá»ƒm sÆ°Æ¡ng, báº§u Æ°á»›t, nhiá»‡t Ä‘á»™ bá» máº·t Ä‘áº¥t
*   **Äá»™ áº©m**: tÆ°Æ¡ng Ä‘á»‘i, tuyá»‡t Ä‘á»‘i  
*   **LÆ°á»£ng mÆ°a**: má»¥c tiÃªu dá»± Ä‘oÃ¡n chÃ­nh
*   **Ãp suáº¥t bá» máº·t**
*   **GiÃ³**: tá»‘c Ä‘á»™ trung bÃ¬nh, tá»‘i Ä‘a, hÆ°á»›ng giÃ³ á»Ÿ cÃ¡c Ä‘á»™ cao khÃ¡c nhau
*   **Bá»©c xáº¡ máº·t trá»i**: bá»©c xáº¡ sÃ³ng dÃ i xuá»‘ng bá» máº·t
*   **Äá»™ áº©m Ä‘áº¥t**: bá» máº·t, vÃ¹ng rá»…, toÃ n bá»™ máº·t cáº¯t

Dá»¯ liá»‡u Ä‘Æ°á»£c cung cáº¥p cho má»—i ngÃ y trong khoáº£ng thá»i gian Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh tá»« 01/01/2000 Ä‘áº¿n 30/04/2025, phá»¥c vá»¥ má»¥c tiÃªu dá»± Ä‘oÃ¡n thá»i tiáº¿t cho toÃ n thÃ nh phá»‘.

**b) Nguá»“n gá»‘c cá»§a bá»™ dá»¯ liá»‡u?**

Dá»¯ liá»‡u gá»‘c Ä‘Æ°á»£c cung cáº¥p bá»Ÿi dá»± Ã¡n NASA POWER. Dá»¯ liá»‡u cá»¥ thá»ƒ trong bá»™ dá»¯ liá»‡u nÃ y Ä‘Æ°á»£c thu tháº­p báº±ng cÃ¡ch gá»i Ä‘áº¿n API cÃ´ng khai cá»§a NASA POWER:

*   **Nguá»“n dá»¯ liá»‡u:** NASA POWER (Prediction Of Worldwide Energy Resources)
*   **API Endpoint:** `https://power.larc.nasa.gov/api/temporal/daily/point`
*   **PhÆ°Æ¡ng phÃ¡p:** Single request crawl vá»›i 20 parameters (trong giá»›i háº¡n API)
*   **Vá»‹ trÃ­:** Tá»a Ä‘á»™ trung tÃ¢m TP.HCM (lat: 10.78, lon: 106.7)

**c) Cáº¥u trÃºc cÃ¡c táº­p tin cá»§a bá»™ dá»¯ liá»‡u? Ã nghÄ©a tá»«ng táº­p tin.**

Dá»¯ liá»‡u sau khi thu tháº­p Ä‘Æ°á»£c lÆ°u trá»¯ trong thÆ° má»¥c `nasa_power_hcmc_data` vá»›i cáº¥u trÃºc Ä‘Æ¡n giáº£n:

1.  **Tá»‡p dá»¯ liá»‡u chÃ­nh:**
    *   **TÃªn tá»‡p:** `hcmc_weather_data_{NgÃ y_Báº¯t_Äáº§u}_{NgÃ y_Káº¿t_ThÃºc}.csv`
    *   **Ná»™i dung:** Chá»©a dá»¯ liá»‡u hÃ ng ngÃ y cá»§a táº¥t cáº£ 20 tham sá»‘ khÃ­ tÆ°á»£ng cho TP.HCM trong khoáº£ng thá»i gian xÃ¡c Ä‘á»‹nh. Má»—i hÃ ng Ä‘áº¡i diá»‡n cho má»™t ngÃ y.

**d) CÃ¡c thÃ´ng tin nÃ o mÃ  cÃ¡c record (báº£n ghi) lÆ°u trá»¯?**

Má»—i record (hÃ ng trong tá»‡p CSV) tÆ°Æ¡ng á»©ng vá»›i dá»¯ liá»‡u cá»§a má»™t ngÃ y cho TP.HCM vÃ  chá»©a cÃ¡c cá»™t sau:

**ThÃ´ng tin Ä‘á»‹a lÃ½ vÃ  thá»i gian:**
*   **DATE:** NgÃ y ghi nháº­n dá»¯ liá»‡u (Ä‘á»‹nh dáº¡ng datetime)
*   **LATITUDE:** VÄ© Ä‘á»™ Ä‘iá»ƒm Ä‘áº¡i diá»‡n TP.HCM (10.78)
*   **LONGITUDE:** Kinh Ä‘á»™ Ä‘iá»ƒm Ä‘áº¡i diá»‡n TP.HCM (106.7)

**NhÃ³m 1 - Nhiá»‡t Ä‘á»™, Ä‘á»™ áº©m, mÆ°a, Ã¡p suáº¥t (11 chá»‰ sá»‘):**
*   **PRECTOTCORR:** LÆ°á»£ng mÆ°a Ä‘Ã£ hiá»‡u chá»‰nh (mm/ngÃ y) - *Biáº¿n má»¥c tiÃªu*
*   **QV2M:** Äá»™ áº©m tuyá»‡t Ä‘á»‘i á»Ÿ 2m (g/kg)
*   **RH2M:** Äá»™ áº©m tÆ°Æ¡ng Ä‘á»‘i á»Ÿ 2m (%)
*   **T2M:** Nhiá»‡t Ä‘á»™ trung bÃ¬nh á»Ÿ 2m (Â°C)
*   **T2MDEW:** Äiá»ƒm sÆ°Æ¡ng/sÆ°Æ¡ng giÃ¡ á»Ÿ 2m (Â°C)
*   **T2MWET:** Nhiá»‡t Ä‘á»™ báº§u Æ°á»›t á»Ÿ 2m (Â°C)
*   **T2M_MAX:** Nhiá»‡t Ä‘á»™ tá»‘i Ä‘a á»Ÿ 2m (Â°C)
*   **TS:** Nhiá»‡t Ä‘á»™ bá» máº·t Ä‘áº¥t (Â°C)
*   **T2M_MIN:** Nhiá»‡t Ä‘á»™ tá»‘i thiá»ƒu á»Ÿ 2m (Â°C)
*   **ALLSKY_SFC_LW_DWN:** Bá»©c xáº¡ sÃ³ng dÃ i xuá»‘ng bá» máº·t (MJ/mÂ²/ngÃ y)
*   **PS:** Ãp suáº¥t bá» máº·t (kPa)

**NhÃ³m 2 - GiÃ³ vÃ  Ä‘á»™ áº©m Ä‘áº¥t (9 chá»‰ sá»‘):**
*   **WS10M:** Tá»‘c Ä‘á»™ giÃ³ á»Ÿ 10m (m/s)
*   **WD10M:** HÆ°á»›ng giÃ³ á»Ÿ 10m (Äá»™)
*   **WS10M_MAX:** Tá»‘c Ä‘á»™ giÃ³ tá»‘i Ä‘a á»Ÿ 10m (m/s)
*   **WS2M_MAX:** Tá»‘c Ä‘á»™ giÃ³ tá»‘i Ä‘a á»Ÿ 2m (m/s)
*   **WS2M:** Tá»‘c Ä‘á»™ giÃ³ á»Ÿ 2m (m/s)
*   **WD2M:** HÆ°á»›ng giÃ³ á»Ÿ 2m (Äá»™)
*   **GWETPROF:** Äá»™ áº©m Ä‘áº¥t theo máº·t cáº¯t (tá»· lá»‡ 0-1)
*   **GWETTOP:** Äá»™ áº©m bá» máº·t Ä‘áº¥t (tá»· lá»‡ 0-1)
*   **GWETROOT:** Äá»™ áº©m vÃ¹ng rá»… (tá»· lá»‡ 0-1)

**Ghi chÃº quan trá»ng:**
- GiÃ¡ trá»‹ `-999` biá»ƒu thá»‹ dá»¯ liá»‡u thiáº¿u hoáº·c khÃ´ng thá»ƒ tÃ­nh toÃ¡n
- Dá»¯ liá»‡u Ä‘Æ°á»£c crawl trong 1 láº§n duy nháº¥t vá»›i 20 parameters (tuÃ¢n thá»§ giá»›i háº¡n API)
- Tá»•ng cá»™ng cÃ³ 20 weather parameters + 3 metadata columns cho viá»‡c dá»± Ä‘oÃ¡n thá»i tiáº¿t TP.HCM

### Tham kháº£o

*   **NASA POWER Data Access Viewer:** [https://power.larc.nasa.gov/data-access-viewer/](https://power.larc.nasa.gov/data-access-viewer/)
*   **NASA POWER Documentation:** [https://power.larc.nasa.gov/docs/](https://power.larc.nasa.gov/docs/)
*   **NASA POWER API Documentation:** [https://power.larc.nasa.gov/docs/services/api/](https://power.larc.nasa.gov/docs/services/api/)
"""

data_dir = "nasa_power_hcmc_data"
start_date_str = "20000101"  # 1/1/2000
end_date_str = "20250430"    # 30/04/2025

import pandas as pd
import numpy as np
import os
from datetime import datetime

# --- 1. Rename Columns (Vietnamese Standard) ---
column_mapping = {
    "PRECTOTCORR": "LÆ°á»£ng mÆ°a",           # Precipitation
    "QV2M": "Äá»™ áº©m tuyá»‡t Ä‘á»‘i 2m",                  # Specific Humidity at 2 Meters
    "RH2M": "Äá»™ áº©m tÆ°Æ¡ng Ä‘á»‘i 2m",                  # Relative Humidity at 2 Meters
    "T2M": "Nhiá»‡t Ä‘á»™ 2m",                   # Temperature at 2 Meters
    "T2MDEW": "Äiá»ƒm sÆ°Æ¡ng 2m",                # Dew/Frost Point at 2 Meters
    "T2MWET": "Nhiá»‡t Ä‘á»™ báº§u Æ°á»›t 2m",                # Wet Bulb Temperature at 2 Meters
    "T2M_MAX": "Nhiá»‡t Ä‘á»™ tá»‘i Ä‘a 2m",               # Temperature at 2 Meters Maximum
    "TS": "Nhiá»‡t Ä‘á»™ bá» máº·t Ä‘áº¥t",                    # Earth Skin Temperature
    "T2M_MIN": "Nhiá»‡t Ä‘á»™ tá»‘i thiá»ƒu 2m",               # Temperature at 2 Meters Minimum
    "ALLSKY_SFC_LW_DWN": "Bá»©c xáº¡ sÃ³ng dÃ i xuá»‘ng",     # All Sky Surface Longwave Downward Irradiance
    "PS": "Ãp suáº¥t bá» máº·t",                    # Surface Pressure
    "WS10M": "Tá»‘c Ä‘á»™ giÃ³ 10m",                 # Wind Speed at 10 Meters
    "WD10M": "HÆ°á»›ng giÃ³ 10m",                 # Wind Direction at 10 Meters
    "WS10M_MAX": "Tá»‘c Ä‘á»™ giÃ³ tá»‘i Ä‘a 10m",             # Wind Speed at 10 Meters Maximum
    'WS2M_MAX': "Tá»‘c Ä‘á»™ giÃ³ tá»‘i Ä‘a 2m",              # MERRA-2 Wind Speed at 2 Meters Maximum (m/s)
    'WS2M': "Tá»‘c Ä‘á»™ giÃ³ 2m",                  # MERRA-2 Wind Speed at 2 Meters (m/s)
    'WD2M': "HÆ°á»›ng giÃ³ 2m",                  # MERRA-2 Wind Direction at 2 Meters (Degrees)
    "GWETPROF": "Äá»™ áº©m Ä‘áº¥t máº·t cáº¯t",              # Profile Soil Moisture (surface to bedrock)
    "GWETTOP": "Äá»™ áº©m Ä‘áº¥t bá» máº·t",               # Surface Soil Wetness (surface to 5 cm below)
    "GWETROOT": "Äá»™ áº©m Ä‘áº¥t vÃ¹ng rá»…",               # Root Zone Soil Wetness (surface to 100 cm below)
    # Cá»™t thÃ´ng tin
    "DATE": "NgÃ y",
    "LATITUDE": "VÄ© Ä‘á»™",
    "LONGITUDE": "Kinh Ä‘á»™",
}


# --- 2. Load Data ---
hcmc_complete_file_path = os.path.join(data_dir, f"hcmc_weather_data_{start_date_str}_{end_date_str}.csv")

print("ğŸŒŸ LOADING WEATHER DATA FOR HO CHI MINH CITY")
print("="*60)

# Load dá»¯ liá»‡u hoÃ n chá»‰nh cho TP.HCM
if final_dataset is None:
  try:
      df_all = pd.read_csv(hcmc_complete_file)
      print(f"âœ… ÄÃ£ táº£i thÃ nh cÃ´ng file: {os.path.basename(hcmc_complete_file)}")
      print(f"ğŸ“Š KÃ­ch thÆ°á»›c ban Ä‘áº§u: {df_all.shape[0]:,} rows Ã— {df_all.shape[1]} columns")

      # Rename columns
      original_columns = len(df_all.columns)
      df_all.rename(columns=column_mapping, inplace=True)
      print(f"ğŸ“ ÄÃ£ Ä‘á»•i tÃªn {original_columns} cá»™t sang tiáº¿ng Viá»‡t")

  except FileNotFoundError:
      print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file '{hcmc_complete_file}'")
      print(f"ğŸ’¡ Vui lÃ²ng cháº¡y crawler trÆ°á»›c Ä‘á»ƒ táº¡o dá»¯ liá»‡u!")
      df_all = None
  except Exception as e:
      print(f"âŒ Lá»—i khi táº£i file '{hcmc_complete_file}': {e}")
      df_all = None
else:
  df_all = final_dataset
  original_columns = len(df_all.columns)
  df_all.rename(columns=column_mapping, inplace=True)

if df_all is not None:
    # --- 3. Check Basic Info ---
    print("\nğŸ“‹ THÃ”NG TIN CÆ  Báº¢N Cá»¦A Dá»® LIá»†U TP.HCM")
    print("="*60)

    print("\n1. ğŸ“ KÃ­ch thÆ°á»›c (Shape):")
    print(f"   {df_all.shape[0]:,} rows Ã— {df_all.shape[1]} columns")

    print("\n2. ğŸ“Š ThÃ´ng tin biáº¿n (Info):")
    with pd.option_context('display.max_rows', None, 'display.max_columns', None):
        df_all.info(verbose=True, show_counts=True)

    print("\n3. ğŸ“ˆ Thá»‘ng kÃª mÃ´ táº£ (Describe):")
    numeric_cols = df_all.select_dtypes(include=[np.number]).columns
    print(f"   Hiá»ƒn thá»‹ thá»‘ng kÃª cho {len(numeric_cols)} cá»™t sá»‘:")
    print(df_all[numeric_cols].describe().T)

    # --- 4. Correct Date Format ---
    print("\nğŸ“… CHá»ˆNH Sá»¬A Äá»ŠNH Dáº NG NGÃ€Y")
    print("="*40)

    def correct_date_format(df, date_column='NgÃ y'):
        """Converts a date column to datetime objects."""
        if date_column in df.columns:
            # Check if already datetime
            if pd.api.types.is_datetime64_any_dtype(df[date_column]):
                print(f"âœ… Cá»™t '{date_column}' Ä‘Ã£ á»Ÿ Ä‘á»‹nh dáº¡ng datetime")
                return df
            try:
                # Try different date formats
                if df[date_column].dtype == 'object':
                    # If string, try to parse
                    df[date_column] = pd.to_datetime(df[date_column])
                else:
                    # If numeric (YYYYMMDD), convert to string first
                    df[date_column] = df[date_column].astype(str)
                    df[date_column] = pd.to_datetime(df[date_column], format='%Y%m%d')

                print(f"âœ… ÄÃ£ chuyá»ƒn Ä‘á»•i cá»™t '{date_column}' sang datetime")
                print(f"   ğŸ“… Khoáº£ng thá»i gian: {df[date_column].min()} â†’ {df[date_column].max()}")
                print(f"   ğŸ“Š Sá»‘ ngÃ y dá»¯ liá»‡u: {df[date_column].nunique():,} days")

            except Exception as e:
                print(f"âŒ Lá»—i khi chuyá»ƒn Ä‘á»•i cá»™t '{date_column}': {e}")
                print("ğŸ’¡ Kiá»ƒm tra láº¡i Ä‘á»‹nh dáº¡ng dá»¯ liá»‡u trong file CSV")
        else:
            print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y cá»™t '{date_column}' Ä‘á»ƒ chuyá»ƒn Ä‘á»•i")
        return df

    df_all = correct_date_format(df_all)

    print(f"ğŸ“Š Tá»•ng sá»‘ weather parameters: {len([col for col in df_all.columns if col not in ['NgÃ y', 'VÄ© Ä‘á»™', 'Kinh Ä‘á»™', 'NhÃ³m']])}")

else:
    print("\nâŒ KhÃ´ng thá»ƒ load dá»¯ liá»‡u. Vui lÃ²ng kiá»ƒm tra láº¡i!")

"""# Data Quality Assessment"""

# -*- coding: utf-8 -*-
"""
Data Quality Assessment Module for Weather Time Series Data
Purpose: Comprehensive data quality evaluation for precipitation prediction project
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class DataQualityAssessment:
    """
    Comprehensive Data Quality Assessment for Weather Data
    """

    def __init__(self, df, target_col='LÆ°á»£ng mÆ°a', date_col='NgÃ y'):
        """
        Initialize Data Quality Assessment

        Args:
            df: DataFrame to assess
            target_col: Target variable column name
            date_col: Date column name
        """
        self.df = df.copy()
        self.target_col = target_col
        self.date_col = date_col
        self.n_rows = len(self.df)
        self.n_cols = len(self.df.columns)

        print("ğŸ” DATA QUALITY ASSESSMENT INITIALIZED")
        print("="*60)
        print(f"ğŸ“Š Dataset Overview:")
        print(f"   - Shape: {self.df.shape}")
        print(f"   - Target Variable: {self.target_col}")
        print(f"   - Date Column: {self.date_col}")
        print(f"   - Memory Usage: {self.df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

    def basic_info_analysis(self):
        """
        1. PhÃ¢n tÃ­ch thÃ´ng tin cÆ¡ báº£n vá» dataset
        """
        print("\n" + "="*60)
        print("ğŸ“‹ 1. BASIC DATASET INFORMATION")
        print("="*60)

        # Dataset shape and size
        print(f"ğŸ“ KÃ­ch thÆ°á»›c Dataset:")
        print(f"   - Sá»‘ dÃ²ng (observations): {self.n_rows:,}")
        print(f"   - Sá»‘ cá»™t (variables): {self.n_cols}")
        print(f"   - Tá»•ng Ã´ dá»¯ liá»‡u: {self.n_rows * self.n_cols:,}")

        # Data types analysis
        print(f"\nğŸ“Š PhÃ¢n tÃ­ch kiá»ƒu dá»¯ liá»‡u:")
        dtype_counts = self.df.dtypes.value_counts()
        for dtype, count in dtype_counts.items():
            print(f"   - {dtype}: {count} columns ({count/self.n_cols*100:.1f}%)")

        # Column categories
        self.numerical_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()
        self.categorical_cols = self.df.select_dtypes(include=['object']).columns.tolist()
        self.datetime_cols = self.df.select_dtypes(include=['datetime64']).columns.tolist()

        print(f"\nğŸ“‚ PhÃ¢n loáº¡i cá»™t:")
        print(f"   - Numerical columns: {len(self.numerical_cols)}")
        print(f"   - Categorical columns: {len(self.categorical_cols)}")
        print(f"   - Datetime columns: {len(self.datetime_cols)}")

        return {
            'shape': self.df.shape,
            'memory_usage_mb': self.df.memory_usage(deep=True).sum() / 1024**2,
            'dtype_counts': dtype_counts,
            'column_categories': {
                'numerical': self.numerical_cols,
                'categorical': self.categorical_cols,
                'datetime': self.datetime_cols
            }
        }

    def missing_values_analysis(self):
        """
        2. PhÃ¢n tÃ­ch chi tiáº¿t vá» missing values
        """
        print("\n" + "="*60)
        print("â“ 2. MISSING VALUES ANALYSIS")
        print("="*60)

        # Calculate missing values for each column
        missing_data = []
        for col in self.df.columns:
            missing_count = self.df[col].isnull().sum()
            missing_percentage = (missing_count / self.n_rows) * 100
            non_null_count = self.df[col].notna().sum()
            unique_count = self.df[col].nunique()

            missing_data.append({
                'Column': col,
                'Missing_Count': missing_count,
                'Missing_Percentage': missing_percentage,
                'Non_Null_Count': non_null_count,
                'Unique_Values': unique_count,
                'Data_Type': str(self.df[col].dtype)
            })

        missing_df = pd.DataFrame(missing_data)
        missing_df = missing_df.sort_values('Missing_Percentage', ascending=False)

        # Overall missing data statistics
        total_missing = self.df.isnull().sum().sum()
        total_cells = self.n_rows * self.n_cols
        overall_missing_pct = (total_missing / total_cells) * 100

        print(f"ğŸ“Š Tá»•ng quan Missing Data:")
        print(f"   - Tá»•ng sá»‘ Ã´ thiáº¿u dá»¯ liá»‡u: {total_missing:,}")
        print(f"   - Tá»· lá»‡ thiáº¿u dá»¯ liá»‡u tá»•ng thá»ƒ: {overall_missing_pct:.2f}%")

        # Columns with missing data
        cols_with_missing = missing_df[missing_df['Missing_Count'] > 0]

        # Initialize empty DataFrames to avoid UnboundLocalError
        high_missing = pd.DataFrame()
        medium_missing = pd.DataFrame()
        low_missing = pd.DataFrame()

        if len(cols_with_missing) > 0:
            print(f"\nğŸš¨ CÃ¡c cá»™t cÃ³ dá»¯ liá»‡u thiáº¿u ({len(cols_with_missing)} cá»™t):")
            print(cols_with_missing[['Column', 'Missing_Count', 'Missing_Percentage', 'Data_Type']].to_string(index=False))

            # Categorize missing data severity
            high_missing = cols_with_missing[cols_with_missing['Missing_Percentage'] > 50]
            medium_missing = cols_with_missing[(cols_with_missing['Missing_Percentage'] > 10) &
                                             (cols_with_missing['Missing_Percentage'] <= 50)]
            low_missing = cols_with_missing[cols_with_missing['Missing_Percentage'] <= 10]

            print(f"\nğŸ“‹ PhÃ¢n loáº¡i má»©c Ä‘á»™ thiáº¿u dá»¯ liá»‡u:")
            print(f"   - Thiáº¿u nghiÃªm trá»ng (>50%): {len(high_missing)} cá»™t")
            print(f"   - Thiáº¿u trung bÃ¬nh (10-50%): {len(medium_missing)} cá»™t")
            print(f"   - Thiáº¿u Ã­t (<10%): {len(low_missing)} cá»™t")

            if len(high_missing) > 0:
                print(f"   âš ï¸ Cá»™t thiáº¿u nghiÃªm trá»ng: {high_missing['Column'].tolist()}")
        else:
            print("âœ… KhÃ´ng cÃ³ cá»™t nÃ o thiáº¿u dá»¯ liá»‡u!")

        # Check target variable missing data
        if self.target_col in missing_df['Column'].values:
            target_missing = missing_df[missing_df['Column'] == self.target_col]['Missing_Percentage'].iloc[0]
            if target_missing > 0:
                print(f"\nğŸ¯ Biáº¿n má»¥c tiÃªu '{self.target_col}' thiáº¿u {target_missing:.2f}% dá»¯ liá»‡u")
            else:
                print(f"\nâœ… Biáº¿n má»¥c tiÃªu '{self.target_col}' khÃ´ng thiáº¿u dá»¯ liá»‡u")

        return missing_df, {
            'total_missing': total_missing,
            'overall_missing_pct': overall_missing_pct,
            'columns_with_missing': len(cols_with_missing),
            'high_missing_cols': high_missing['Column'].tolist() if len(high_missing) > 0 else [],
            'medium_missing_cols': medium_missing['Column'].tolist() if len(medium_missing) > 0 else [],
            'low_missing_cols': low_missing['Column'].tolist() if len(low_missing) > 0 else []
        }

    def duplicate_analysis(self):
        """
        3. PhÃ¢n tÃ­ch dá»¯ liá»‡u trÃ¹ng láº·p
        """
        print("\n" + "="*60)
        print("ğŸ”„ 3. DUPLICATE DATA ANALYSIS")
        print("="*60)

        # Full row duplicates
        full_duplicates = self.df.duplicated().sum()
        full_duplicate_pct = (full_duplicates / self.n_rows) * 100

        print(f"ğŸ“Š DÃ²ng trÃ¹ng láº·p hoÃ n toÃ n:")
        print(f"   - Sá»‘ dÃ²ng trÃ¹ng láº·p: {full_duplicates:,}")
        print(f"   - Tá»· lá»‡ trÃ¹ng láº·p: {full_duplicate_pct:.2f}%")

        # Check for duplicates excluding certain columns (like ID columns)
        key_columns = [col for col in self.df.columns
                      if col not in ['VÄ© Ä‘á»™', 'Kinh Ä‘á»™']]  # Exclude coordinate columns

        key_duplicates = 0
        if len(key_columns) < len(self.df.columns):
            key_duplicates = self.df.duplicated(subset=key_columns).sum()
            key_duplicate_pct = (key_duplicates / self.n_rows) * 100

            print(f"\nğŸ“Š DÃ²ng trÃ¹ng láº·p (loáº¡i trá»« tá»a Ä‘á»™):")
            print(f"   - Sá»‘ dÃ²ng trÃ¹ng láº·p: {key_duplicates:,}")
            print(f"   - Tá»· lá»‡ trÃ¹ng láº·p: {key_duplicate_pct:.2f}%")

        # Check for date + location duplicates (if applicable)
        date_location_duplicates = 0
        if self.date_col in self.df.columns and 'Quáº­n Huyá»‡n' in self.df.columns:
            date_location_duplicates = self.df.duplicated(subset=[self.date_col, 'Quáº­n Huyá»‡n']).sum()
            date_location_dup_pct = (date_location_duplicates / self.n_rows) * 100

            print(f"\nğŸ“Š TrÃ¹ng láº·p ngÃ y + Ä‘á»‹a Ä‘iá»ƒm:")
            print(f"   - Sá»‘ dÃ²ng trÃ¹ng láº·p: {date_location_duplicates:,}")
            print(f"   - Tá»· lá»‡ trÃ¹ng láº·p: {date_location_dup_pct:.2f}%")

            if date_location_duplicates > 0:
                print("   âš ï¸ CÃ³ thá»ƒ cÃ³ dá»¯ liá»‡u Ä‘o Ä‘áº¡c trÃ¹ng láº·p cho cÃ¹ng ngÃ y vÃ  Ä‘á»‹a Ä‘iá»ƒm!")

        # Check for potential duplicate patterns in target variable
        if self.target_col in self.df.columns:
            target_value_counts = self.df[self.target_col].value_counts()
            most_common_value = target_value_counts.iloc[0]
            most_common_pct = (most_common_value / self.n_rows) * 100

            print(f"\nğŸ¯ PhÃ¢n tÃ­ch giÃ¡ trá»‹ trÃ¹ng láº·p cá»§a biáº¿n má»¥c tiÃªu:")
            print(f"   - GiÃ¡ trá»‹ phá»• biáº¿n nháº¥t: {target_value_counts.index[0]}")
            print(f"   - Sá»‘ láº§n xuáº¥t hiá»‡n: {most_common_value:,} ({most_common_pct:.2f}%)")

            # Check for excessive zeros in precipitation data
            if target_value_counts.index[0] == 0:
                zero_pct = (target_value_counts.iloc[0] / self.n_rows) * 100
                print(f"   ğŸ“Š Tá»· lá»‡ ngÃ y khÃ´ng mÆ°a: {zero_pct:.2f}%")
                if zero_pct > 70:
                    print("   âš ï¸ Tá»· lá»‡ ngÃ y khÃ´ng mÆ°a ráº¥t cao - cáº§n xem xÃ©t!")

        return {
            'full_duplicates': full_duplicates,
            'full_duplicate_pct': full_duplicate_pct,
            'key_duplicates': key_duplicates,
            'date_location_duplicates': date_location_duplicates
        }


    def data_consistency_check(self):
        """
        5. Kiá»ƒm tra tÃ­nh nháº¥t quÃ¡n cá»§a dá»¯ liá»‡u
        """
        print("\n" + "="*60)
        print("ğŸ” 4. DATA CONSISTENCY CHECK")
        print("="*60)

        consistency_issues = []

        # Check for negative values where they shouldn't exist
        weather_vars_positive = [
            'LÆ°á»£ng mÆ°a', 'Äá»™ áº©m tÆ°Æ¡ng Ä‘á»‘i 2m', 'Tá»‘c Ä‘á»™ giÃ³ 10m', 'Tá»‘c Ä‘á»™ giÃ³ 2m',
            'Tá»‘c Ä‘á»™ giÃ³ tá»‘i Ä‘a 10m', 'Tá»‘c Ä‘á»™ giÃ³ tá»‘i Ä‘a 2m'
        ]

        print("ğŸ” Kiá»ƒm tra giÃ¡ trá»‹ Ã¢m cho cÃ¡c biáº¿n pháº£i dÆ°Æ¡ng:")
        for col in weather_vars_positive:
            if col in self.df.columns:
                negative_count = (self.df[col] < 0).sum()
                if negative_count > 0:
                    negative_pct = (negative_count / len(self.df)) * 100
                    print(f"   âš ï¸ {col}: {negative_count} giÃ¡ trá»‹ Ã¢m ({negative_pct:.2f}%)")
                    consistency_issues.append(f"{col} has {negative_count} negative values")
                else:
                    print(f"   âœ… {col}: KhÃ´ng cÃ³ giÃ¡ trá»‹ Ã¢m")

        # Check humidity ranges (should be 0-100%)
        humidity_cols = ['Äá»™ áº©m tÆ°Æ¡ng Ä‘á»‘i 2m']
        print(f"\nğŸ” Kiá»ƒm tra pháº¡m vi Ä‘á»™ áº©m (0-100%):")
        for col in humidity_cols:
            if col in self.df.columns:
                data = self.df[col].dropna()
                out_of_range = data[(data < 0) | (data > 100)]
                if len(out_of_range) > 0:
                    print(f"   âš ï¸ {col}: {len(out_of_range)} giÃ¡ trá»‹ ngoÃ i pháº¡m vi [0,100]")
                    print(f"      - Min: {data.min():.2f}, Max: {data.max():.2f}")
                    consistency_issues.append(f"{col} has values outside [0,100] range")
                else:
                    print(f"   âœ… {col}: Táº¥t cáº£ giÃ¡ trá»‹ trong pháº¡m vi [0,100]")

        # Check wind direction ranges (should be 0-360 degrees)
        wind_direction_cols = ['HÆ°á»›ng giÃ³ 10m', 'HÆ°á»›ng giÃ³ 2m']
        print(f"\nğŸ” Kiá»ƒm tra pháº¡m vi hÆ°á»›ng giÃ³ (0-360 Ä‘á»™):")
        for col in wind_direction_cols:
            if col in self.df.columns:
                data = self.df[col].dropna()
                out_of_range = data[(data < 0) | (data > 360)]
                if len(out_of_range) > 0:
                    print(f"   âš ï¸ {col}: {len(out_of_range)} giÃ¡ trá»‹ ngoÃ i pháº¡m vi [0,360]")
                    print(f"      - Min: {data.min():.2f}, Max: {data.max():.2f}")
                    consistency_issues.append(f"{col} has values outside [0,360] range")
                else:
                    print(f"   âœ… {col}: Táº¥t cáº£ giÃ¡ trá»‹ trong pháº¡m vi [0,360]")

        # Check temperature consistency (min <= avg <= max)
        temp_cols = {
            'min': 'Nhiá»‡t Ä‘á»™ tá»‘i thiá»ƒu 2m',
            'avg': 'Nhiá»‡t Ä‘á»™ 2m',
            'max': 'Nhiá»‡t Ä‘á»™ tá»‘i Ä‘a 2m'
        }

        if all(col in self.df.columns for col in temp_cols.values()):
            print(f"\nğŸ” Kiá»ƒm tra tÃ­nh nháº¥t quÃ¡n nhiá»‡t Ä‘á»™ (min â‰¤ avg â‰¤ max):")
            temp_min = self.df[temp_cols['min']]
            temp_avg = self.df[temp_cols['avg']]
            temp_max = self.df[temp_cols['max']]

            # Check min <= avg
            min_avg_violations = (temp_min > temp_avg).sum()
            # Check avg <= max
            avg_max_violations = (temp_avg > temp_max).sum()
            # Check min <= max
            min_max_violations = (temp_min > temp_max).sum()

            total_violations = min_avg_violations + avg_max_violations + min_max_violations

            if total_violations > 0:
                print(f"   âš ï¸ PhÃ¡t hiá»‡n {total_violations} vi pháº¡m tÃ­nh nháº¥t quÃ¡n nhiá»‡t Ä‘á»™:")
                if min_avg_violations > 0:
                    print(f"      - T_min > T_avg: {min_avg_violations} trÆ°á»ng há»£p")
                if avg_max_violations > 0:
                    print(f"      - T_avg > T_max: {avg_max_violations} trÆ°á»ng há»£p")
                if min_max_violations > 0:
                    print(f"      - T_min > T_max: {min_max_violations} trÆ°á»ng há»£p")
                consistency_issues.append(f"Temperature consistency violations: {total_violations}")
            else:
                print(f"   âœ… Táº¥t cáº£ dá»¯ liá»‡u nhiá»‡t Ä‘á»™ nháº¥t quÃ¡n")

        # Check for extremely unrealistic values
        print(f"\nğŸ” Kiá»ƒm tra giÃ¡ trá»‹ cá»±c Ä‘oan khÃ´ng thá»±c táº¿:")

        # Precipitation > 500mm/day (extremely rare)
        if self.target_col in self.df.columns:
            extreme_rain = (self.df[self.target_col] > 500).sum()
            if extreme_rain > 0:
                print(f"   âš ï¸ {self.target_col}: {extreme_rain} ngÃ y cÃ³ mÆ°a >500mm (cá»±c Ä‘oan)")
                max_rain = self.df[self.target_col].max()
                print(f"      - LÆ°á»£ng mÆ°a tá»‘i Ä‘a: {max_rain:.2f}mm")
                consistency_issues.append(f"Extreme precipitation: {extreme_rain} days >500mm")
            else:
                print(f"   âœ… {self.target_col}: KhÃ´ng cÃ³ giÃ¡ trá»‹ cá»±c Ä‘oan")

        # Temperature extremes
        if 'Nhiá»‡t Ä‘á»™ 2m' in self.df.columns:
            temp_data = self.df['Nhiá»‡t Ä‘á»™ 2m'].dropna()
            extreme_hot = (temp_data > 45).sum()  # >45Â°C
            extreme_cold = (temp_data < 10).sum()  # <10Â°C for tropical climate

            if extreme_hot > 0 or extreme_cold > 0:
                print(f"   âš ï¸ Nhiá»‡t Ä‘á»™ cá»±c Ä‘oan:")
                if extreme_hot > 0:
                    print(f"      - QuÃ¡ nÃ³ng (>45Â°C): {extreme_hot} ngÃ y")
                if extreme_cold > 0:
                    print(f"      - QuÃ¡ láº¡nh (<10Â°C): {extreme_cold} ngÃ y")
                consistency_issues.append(f"Extreme temperatures: {extreme_hot} hot, {extreme_cold} cold days")
            else:
                print(f"   âœ… Nhiá»‡t Ä‘á»™: KhÃ´ng cÃ³ giÃ¡ trá»‹ cá»±c Ä‘oan")

        return {
            'consistency_issues': consistency_issues,
            'total_issues': len(consistency_issues)
        }

    def temporal_quality_check(self):
        """
        5. Kiá»ƒm tra cháº¥t lÆ°á»£ng dá»¯ liá»‡u thá»i gian
        """
        print("\n" + "="*60)
        print("ğŸ“… 5. TEMPORAL DATA QUALITY CHECK")
        print("="*60)

        if self.date_col not in self.df.columns:
            print(f"âŒ KhÃ´ng tÃ¬m tháº¥y cá»™t ngÃ y '{self.date_col}'")
            return {
                'invalid_dates': 0,
                'date_range_days': 0,
                'missing_dates_count': 0,
                'duplicate_dates': 0
            }

        # Ensure datetime format
        df_temp = self.df.copy()
        if not pd.api.types.is_datetime64_any_dtype(df_temp[self.date_col]):
            print(f"ğŸ”„ Chuyá»ƒn Ä‘á»•i cá»™t '{self.date_col}' sang datetime...")
            df_temp[self.date_col] = pd.to_datetime(df_temp[self.date_col], errors='coerce')

        # Check for invalid dates
        invalid_dates = df_temp[self.date_col].isnull().sum()
        if invalid_dates > 0:
            print(f"âš ï¸ PhÃ¡t hiá»‡n {invalid_dates} ngÃ y khÃ´ng há»£p lá»‡")
        else:
            print(f"âœ… Táº¥t cáº£ ngÃ y Ä‘á»u há»£p lá»‡")

        # Date range analysis
        valid_dates = df_temp[self.date_col].dropna()
        missing_dates_count = 0
        duplicate_dates = 0
        date_range_days = 0

        if len(valid_dates) > 0:
            min_date = valid_dates.min()
            max_date = valid_dates.max()
            date_range_days = (max_date - min_date).days

            print(f"\nğŸ“Š Pháº¡m vi thá»i gian:")
            print(f"   - NgÃ y báº¯t Ä‘áº§u: {min_date.strftime('%Y-%m-%d')}")
            print(f"   - NgÃ y káº¿t thÃºc: {max_date.strftime('%Y-%m-%d')}")
            print(f"   - Tá»•ng khoáº£ng: {date_range_days:,} ngÃ y ({date_range_days/365.25:.1f} nÄƒm)")

            # Check for missing dates in sequence
            expected_dates = pd.date_range(start=min_date, end=max_date, freq='D')
            actual_dates = set(valid_dates.dt.date)
            expected_dates_set = set(expected_dates.date)
            missing_dates = expected_dates_set - actual_dates
            missing_dates_count = len(missing_dates)

            if missing_dates:
                print(f"\nâš ï¸ Thiáº¿u {len(missing_dates)} ngÃ y trong chuá»—i thá»i gian:")
                if len(missing_dates) <= 10:
                    for date in sorted(missing_dates)[:10]:
                        print(f"   - {date}")
                else:
                    print(f"   - Hiá»ƒn thá»‹ 10 ngÃ y Ä‘áº§u: {sorted(missing_dates)[:10]}")
            else:
                print(f"\nâœ… Chuá»—i thá»i gian liÃªn tá»¥c, khÃ´ng thiáº¿u ngÃ y nÃ o")

            # Check for duplicate dates
            duplicate_dates = valid_dates[valid_dates.duplicated()].nunique()
            if duplicate_dates > 0:
                print(f"\nâš ï¸ PhÃ¡t hiá»‡n {duplicate_dates} ngÃ y bá»‹ trÃ¹ng láº·p")
            else:
                print(f"\nâœ… KhÃ´ng cÃ³ ngÃ y trÃ¹ng láº·p")

        return {
            'invalid_dates': invalid_dates,
            'date_range_days': date_range_days,
            'missing_dates_count': missing_dates_count,
            'duplicate_dates': duplicate_dates
        }

    def generate_quality_report(self):
        # Run all assessments
        basic_info = self.basic_info_analysis()
        missing_df, missing_summary = self.missing_values_analysis()
        duplicate_summary = self.duplicate_analysis()
        consistency_summary = self.data_consistency_check()
        temporal_summary = self.temporal_quality_check()

        # Overall quality score calculation
        quality_score = 100.0

        # Deduct points for issues
        if missing_summary['overall_missing_pct'] > 0:
            quality_score -= min(missing_summary['overall_missing_pct'] * 2, 30)

        if duplicate_summary['full_duplicate_pct'] > 0:
            quality_score -= min(duplicate_summary['full_duplicate_pct'] * 3, 20)

        if consistency_summary['total_issues'] > 0:
            quality_score -= min(consistency_summary['total_issues'] * 5, 25)

        if temporal_summary and temporal_summary['invalid_dates'] > 0:
            invalid_date_pct = (temporal_summary['invalid_dates'] / self.n_rows) * 100
            quality_score -= min(invalid_date_pct * 2, 15)

        quality_score = max(quality_score, 0)  # Ensure non-negative

        print(f"\nğŸ† OVERALL DATA QUALITY SCORE: {quality_score:.1f}/100")

        # Quality interpretation
        if quality_score >= 90:
            quality_level = "ğŸŸ¢ EXCELLENT"
            recommendation = "Dá»¯ liá»‡u cÃ³ cháº¥t lÆ°á»£ng ráº¥t tá»‘t, cÃ³ thá»ƒ tiáº¿n hÃ nh phÃ¢n tÃ­ch ngay."
        elif quality_score >= 80:
            quality_level = "ğŸŸ¡ GOOD"
            recommendation = "Dá»¯ liá»‡u cÃ³ cháº¥t lÆ°á»£ng tá»‘t, cÃ³ thá»ƒ cáº§n má»™t sá»‘ xá»­ lÃ½ nhá»."
        elif quality_score >= 70:
            quality_level = "ğŸŸ  FAIR"
            recommendation = "Dá»¯ liá»‡u cáº§n Ä‘Æ°á»£c xá»­ lÃ½ vÃ  lÃ m sáº¡ch trÆ°á»›c khi phÃ¢n tÃ­ch."
        elif quality_score >= 60:
            quality_level = "ğŸ”´ POOR"
            recommendation = "Dá»¯ liá»‡u cÃ³ nhiá»u váº¥n Ä‘á», cáº§n xá»­ lÃ½ ká»¹ lÆ°á»¡ng."
        else:
            quality_level = "â›” CRITICAL"
            recommendation = "Dá»¯ liá»‡u cÃ³ váº¥n Ä‘á» nghiÃªm trá»ng, cáº§n xem xÃ©t láº¡i nguá»“n dá»¯ liá»‡u."

        print(f"ğŸ“Š Quality Level: {quality_level}")
        print(f"ğŸ’¡ Recommendation: {recommendation}")

        # Detailed summary
        print(f"\nğŸ“‹ Chi tiáº¿t cÃ¡c váº¥n Ä‘á»:")
        print(f"   - Missing Data: {missing_summary['overall_missing_pct']:.2f}%")
        print(f"   - Duplicate Rows: {duplicate_summary['full_duplicate_pct']:.2f}%")
        print(f"   - Consistency Issues: {consistency_summary['total_issues']}")
        print(f"   - Invalid Dates: {temporal_summary['invalid_dates']}")
        print(f"   - Missing Dates: {temporal_summary['missing_dates_count']}")



        return {
            'quality_score': quality_score,
            'quality_level': quality_level,
            'recommendation': recommendation,
            'basic_info': basic_info,
            'missing_summary': missing_summary,
            'duplicate_summary': duplicate_summary,
            'consistency_summary': consistency_summary,
            'temporal_summary': temporal_summary,
        }

# =============================================================================
# USAGE FUNCTION
# =============================================================================

def assess_data_quality(df, target_col='LÆ°á»£ng mÆ°a', date_col='NgÃ y'):
    """
    Cháº¡y Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng dá»¯ liá»‡u toÃ n diá»‡n

    Args:
        df: DataFrame cáº§n Ä‘Ã¡nh giÃ¡
        target_col: TÃªn cá»™t biáº¿n má»¥c tiÃªu
        date_col: TÃªn cá»™t ngÃ y thÃ¡ng

    Returns:
        dict: BÃ¡o cÃ¡o chi tiáº¿t vá» cháº¥t lÆ°á»£ng dá»¯ liá»‡u
    """

    print("ğŸš€ STARTING COMPREHENSIVE DATA QUALITY ASSESSMENT")
    print("="*80)

    # Initialize assessment
    dqa = DataQualityAssessment(df, target_col, date_col)

    # Generate comprehensive report
    quality_report = dqa.generate_quality_report()

    print("\nâœ… DATA QUALITY ASSESSMENT COMPLETED")
    print("="*80)

    return quality_report

# =============================================================================
# RUN ASSESSMENT ON YOUR DATA
# =============================================================================

# Cháº¡y Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng dá»¯ liá»‡u cho DataFrame df_all
quality_report = assess_data_quality(df_all, target_col='LÆ°á»£ng mÆ°a', date_col='NgÃ y')

"""# Distribution Analysis"""

# -*- coding: utf-8 -*-
"""
Distribution Analysis Module for Weather Time Series Data
Purpose: Comprehensive distribution analysis for precipitation prediction EDA
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import normaltest, jarque_bera, skewtest, kurtosistest
import warnings
warnings.filterwarnings('ignore')

# Set plotting style for better visualization
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10

class DistributionAnalyzer:
    """
    Comprehensive Distribution Analysis for Weather Data
    """

    def __init__(self, df, target_col='LÆ°á»£ng mÆ°a'):
        """
        Initialize Distribution Analyzer

        Args:
            df: DataFrame to analyze
            target_col: Target variable column name
        """
        self.df = df.copy()
        self.target_col = target_col
        self.numerical_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()

        # Remove coordinate columns from main analysis
        coord_cols = ['VÄ© Ä‘á»™', 'Kinh Ä‘á»™']
        self.analysis_cols = [col for col in self.numerical_cols if col not in coord_cols]

        print("ğŸ“Š DISTRIBUTION ANALYSIS INITIALIZED")
        print("="*60)
        print(f"   - Total Features: {len(self.numerical_cols)}")
        print(f"   - Analysis Features: {len(self.analysis_cols)}")
        print(f"   - Target Variable: {self.target_col}")
        print(f"   - Sample Size: {len(self.df):,}")

    def descriptive_statistics_summary(self):
        """
        1. TÃ­nh toÃ¡n vÃ  hiá»ƒn thá»‹ thá»‘ng kÃª mÃ´ táº£ chi tiáº¿t cho táº¥t cáº£ features
        """
        print("\n" + "="*80)
        print("ğŸ“ˆ 1. DESCRIPTIVE STATISTICS SUMMARY")
        print("="*80)

        # Calculate comprehensive descriptive statistics
        desc_stats = []

        for col in self.analysis_cols:
            data = self.df[col].dropna()

            if len(data) == 0:
                continue

            # Basic statistics
            mean_val = data.mean()
            median_val = data.median()

            # Mode calculation (most frequent value)
            mode_result = data.mode()
            mode_val = mode_result.iloc[0] if len(mode_result) > 0 else np.nan

            # Shape statistics
            skewness = data.skew()
            kurt = data.kurtosis()

            # Variability statistics
            std_val = data.std()
            var_val = data.var()
            cv = (std_val / mean_val) * 100 if mean_val != 0 else np.inf

            # Range statistics
            range_val = data.max() - data.min()
            iqr = data.quantile(0.75) - data.quantile(0.25)

            # Percentiles
            p5 = data.quantile(0.05)
            p95 = data.quantile(0.95)

            stats_dict = {
                'Feature': col,
                'Count': len(data),
                'Mean': mean_val,
                'Median': median_val,
                'Mode': mode_val,
                'Std': std_val,
                'Variance': var_val,
                'CV(%)': cv,
                'Min': data.min(),
                'P5': p5,
                'Q1': data.quantile(0.25),
                'Q3': data.quantile(0.75),
                'P95': p95,
                'Max': data.max(),
                'Range': range_val,
                'IQR': iqr,
                'Skewness': skewness,
                'Kurtosis': kurt
            }

            desc_stats.append(stats_dict)

        # Create comprehensive DataFrame
        desc_df = pd.DataFrame(desc_stats)

        # Display main statistics
        print("ğŸ“Š Comprehensive Descriptive Statistics:")
        print("="*120)

        # Basic stats
        basic_cols = ['Feature', 'Count', 'Mean', 'Median', 'Mode', 'Std', 'Min', 'Max']
        print("\nğŸ”¢ Basic Statistics:")
        print(desc_df[basic_cols].round(4).to_string(index=False))

        # Shape stats
        shape_cols = ['Feature', 'Skewness', 'Kurtosis', 'CV(%)', 'Range', 'IQR']
        print("\nğŸ“ Shape & Variability Statistics:")
        print(desc_df[shape_cols].round(4).to_string(index=False))

        # Percentile stats
        percentile_cols = ['Feature', 'P5', 'Q1', 'Median', 'Q3', 'P95']
        print("\nğŸ“Š Percentile Statistics:")
        print(desc_df[percentile_cols].round(4).to_string(index=False))

        return desc_df

    def target_variable_deep_analysis(self):
        """
        2. PhÃ¢n tÃ­ch sÃ¢u biáº¿n má»¥c tiÃªu (LÆ°á»£ng mÆ°a)
        """
        print("\n" + "="*80)
        print(f"ğŸ¯ 2. TARGET VARIABLE DEEP ANALYSIS: {self.target_col}")
        print("="*80)

        target_data = self.df[self.target_col].dropna()

        # Basic descriptive statistics
        print("ğŸ“ˆ Basic Descriptive Statistics:")
        print(f"   - Count: {len(target_data):,}")
        print(f"   - Mean: {target_data.mean():.4f} mm")
        print(f"   - Median: {target_data.median():.4f} mm")
        print(f"   - Mode: {target_data.mode().iloc[0]:.4f} mm")
        print(f"   - Standard Deviation: {target_data.std():.4f} mm")
        print(f"   - Variance: {target_data.var():.4f}")
        print(f"   - Coefficient of Variation: {(target_data.std()/target_data.mean())*100:.2f}%")

        # Range and percentiles
        print(f"\nğŸ“Š Range and Percentiles:")
        print(f"   - Minimum: {target_data.min():.4f} mm")
        print(f"   - 5th Percentile: {target_data.quantile(0.05):.4f} mm")
        print(f"   - 25th Percentile (Q1): {target_data.quantile(0.25):.4f} mm")
        print(f"   - 50th Percentile (Median): {target_data.quantile(0.50):.4f} mm")
        print(f"   - 75th Percentile (Q3): {target_data.quantile(0.75):.4f} mm")
        print(f"   - 95th Percentile: {target_data.quantile(0.95):.4f} mm")
        print(f"   - Maximum: {target_data.max():.4f} mm")
        print(f"   - Range: {target_data.max() - target_data.min():.4f} mm")
        print(f"   - IQR: {target_data.quantile(0.75) - target_data.quantile(0.25):.4f} mm")

        # Shape statistics
        skewness = target_data.skew()
        kurt = target_data.kurtosis()

        print(f"\nğŸ“ Shape Statistics:")
        print(f"   - Skewness: {skewness:.4f}")
        if skewness > 1:
            skew_interpretation = "Highly right-skewed (lá»‡ch pháº£i máº¡nh)"
        elif skewness > 0.5:
            skew_interpretation = "Moderately right-skewed (lá»‡ch pháº£i vá»«a)"
        elif skewness > -0.5:
            skew_interpretation = "Approximately symmetric (gáº§n Ä‘á»‘i xá»©ng)"
        elif skewness > -1:
            skew_interpretation = "Moderately left-skewed (lá»‡ch trÃ¡i vá»«a)"
        else:
            skew_interpretation = "Highly left-skewed (lá»‡ch trÃ¡i máº¡nh)"

        print(f"     â†’ Interpretation: {skew_interpretation}")

        print(f"   - Kurtosis: {kurt:.4f}")
        if kurt > 3:
            kurt_interpretation = "Leptokurtic (nhá»n hÆ¡n normal)"
        elif kurt < -3:
            kurt_interpretation = "Platykurtic (tÃ¹ hÆ¡n normal)"
        else:
            kurt_interpretation = "Mesokurtic (gáº§n normal)"
        print(f"     â†’ Interpretation: {kurt_interpretation}")

        # Vietnamese Meteorological Standards Classification (24-hour basis)
        print(f"\nğŸŒ§ï¸ Vietnamese Meteorological Standards Classification (24h):")
        print("   Based on Vietnamese National Weather Service Standards")

        # Apply Vietnamese standards for 24-hour precipitation
        no_rain = (target_data == 0).sum()
        trace_rain = ((target_data > 0) & (target_data <= 0.6)).sum()
        light_rain = ((target_data > 0.6) & (target_data <= 6.0)).sum()
        moderate_rain = ((target_data > 6.0) & (target_data <= 16.0)).sum()
        heavy_rain = ((target_data > 16.0) & (target_data <= 50.0)).sum()
        very_heavy_rain = ((target_data > 50.0) & (target_data <= 100.0)).sum()
        extremely_heavy_rain = (target_data > 100.0).sum()

        total_days = len(target_data)

        print(f"   - KhÃ´ng mÆ°a (No Rain): {no_rain:,} days ({no_rain/total_days*100:.2f}%)")
        print(f"   - MÆ°a lÆ°á»£ng khÃ´ng Ä‘Ã¡ng ká»ƒ (0-0.6mm): {trace_rain:,} days ({trace_rain/total_days*100:.2f}%)")
        print(f"   - MÆ°a nhá» (0.6-6.0mm): {light_rain:,} days ({light_rain/total_days*100:.2f}%)")
        print(f"   - MÆ°a (6.0-16.0mm): {moderate_rain:,} days ({moderate_rain/total_days*100:.2f}%)")
        print(f"   - MÆ°a vá»«a (16.0-50.0mm): {heavy_rain:,} days ({heavy_rain/total_days*100:.2f}%)")
        print(f"   - MÆ°a to (50.0-100.0mm): {very_heavy_rain:,} days ({very_heavy_rain/total_days*100:.2f}%)")
        print(f"   - MÆ°a ráº¥t to (>100.0mm): {extremely_heavy_rain:,} days ({extremely_heavy_rain/total_days*100:.2f}%)")


        return {
            'basic_stats': {
                'count': len(target_data),
                'mean': target_data.mean(),
                'median': target_data.median(),
                'mode': target_data.mode().iloc[0],
                'std': target_data.std(),
                'skewness': skewness,
                'kurtosis': kurt
            },
            'intensity_distribution': {
                'no_rain': no_rain,
                'trace_rain': trace_rain,
                'light_rain': light_rain,
                'moderate_rain': moderate_rain,
                'heavy_rain': heavy_rain,
                'very_heavy_rain': very_heavy_rain,
                'extremely_heavy_rain': extremely_heavy_rain
            }
        }

    def distribution_shape_tests(self):
        """
        3. Kiá»ƒm tra thá»‘ng kÃª vá» hÃ¬nh dáº¡ng phÃ¢n phá»‘i
        """
        print("\n" + "="*80)
        print("ğŸ”¬ 3. STATISTICAL DISTRIBUTION SHAPE TESTS")
        print("="*80)

        test_results = []

        for col in self.analysis_cols:
            data = self.df[col].dropna()

            if len(data) < 8:  # Minimum sample size for tests
                continue

            # Sample data if too large for some tests
            sample_size = min(5000, len(data))
            sample_data = data.sample(sample_size) if len(data) > sample_size else data

            try:
                # Normality tests
                shapiro_stat, shapiro_p = stats.shapiro(sample_data)

                # D'Agostino's normality test
                dagostino_stat, dagostino_p = normaltest(sample_data)

                # Jarque-Bera test
                jb_stat, jb_p = jarque_bera(sample_data)

                # Skewness test
                skew_stat, skew_p = skewtest(data)

                # Kurtosis test
                kurt_stat, kurt_p = kurtosistest(data)

                # Anderson-Darling test for normality
                ad_stat, ad_critical, ad_sig = stats.anderson(sample_data, dist='norm')

                test_result = {
                    'Feature': col,
                    'Sample_Size': len(sample_data),
                    'Shapiro_Stat': shapiro_stat,
                    'Shapiro_P': shapiro_p,
                    'Shapiro_Normal': 'Yes' if shapiro_p > 0.05 else 'No',
                    'DAgostino_Stat': dagostino_stat,
                    'DAgostino_P': dagostino_p,
                    'DAgostino_Normal': 'Yes' if dagostino_p > 0.05 else 'No',
                    'JB_Stat': jb_stat,
                    'JB_P': jb_p,
                    'JB_Normal': 'Yes' if jb_p > 0.05 else 'No',
                    'Skew_Stat': skew_stat,
                    'Skew_P': skew_p,
                    'Skew_Normal': 'Yes' if skew_p > 0.05 else 'No',
                    'Kurt_Stat': kurt_stat,
                    'Kurt_P': kurt_p,
                    'Kurt_Normal': 'Yes' if kurt_p > 0.05 else 'No',
                    'AD_Stat': ad_stat,
                    'AD_Critical_5%': ad_critical[2],  # 5% significance level
                    'AD_Normal': 'Yes' if ad_stat < ad_critical[2] else 'No'
                }

                test_results.append(test_result)

            except Exception as e:
                print(f"   âš ï¸ Error testing {col}: {e}")
                continue

        test_df = pd.DataFrame(test_results)

        if len(test_df) > 0:
            print("ğŸ“Š Normality Test Results Summary:")
            print("="*100)

            # Main results table
            display_cols = ['Feature', 'Sample_Size', 'Shapiro_Normal', 'DAgostino_Normal',
                           'JB_Normal', 'Skew_Normal', 'Kurt_Normal', 'AD_Normal']
            print(test_df[display_cols].to_string(index=False))

            # Detailed results for target variable
            if self.target_col in test_df['Feature'].values:
                target_row = test_df[test_df['Feature'] == self.target_col].iloc[0]
                print(f"\nğŸ¯ Detailed Results for {self.target_col}:")
                print(f"   - Shapiro-Wilk: statistic={target_row['Shapiro_Stat']:.6f}, p-value={target_row['Shapiro_P']:.2e}")
                print(f"   - D'Agostino: statistic={target_row['DAgostino_Stat']:.6f}, p-value={target_row['DAgostino_P']:.2e}")
                print(f"   - Jarque-Bera: statistic={target_row['JB_Stat']:.6f}, p-value={target_row['JB_P']:.2e}")
                print(f"   - Skewness test: statistic={target_row['Skew_Stat']:.6f}, p-value={target_row['Skew_P']:.2e}")
                print(f"   - Kurtosis test: statistic={target_row['Kurt_Stat']:.6f}, p-value={target_row['Kurt_P']:.2e}")
                print(f"   - Anderson-Darling: statistic={target_row['AD_Stat']:.6f}, critical_5%={target_row['AD_Critical_5%']:.6f}")

            # Summary statistics
            normal_features = test_df[
                (test_df['Shapiro_Normal'] == 'Yes') &
                (test_df['DAgostino_Normal'] == 'Yes') &
                (test_df['JB_Normal'] == 'Yes')
            ]['Feature'].tolist()

            print(f"\nğŸ“ˆ Summary:")
            print(f"   - Features possibly normal: {len(normal_features)} / {len(test_df)}")
            if normal_features:
                print(f"   - Normal features: {normal_features}")
            else:
                print(f"   - No features follow normal distribution")


        non_param_results = []
        target_data = self.df[self.target_col].dropna()

        for col in self.analysis_cols:
            if col == self.target_col:
                continue

            feature_data = self.df[col].dropna()
            aligned_indices = feature_data.index.intersection(target_data.index)
            if len(aligned_indices) < 20:
                continue

            aligned_feature = feature_data.loc[aligned_indices]
            aligned_target = target_data.loc[aligned_indices]

            test_result = {'Feature': col}

            try:
                # Mann-Whitney U Test (Rain vs No Rain)
                no_rain_feature = aligned_feature[aligned_target == 0]
                rain_feature = aligned_feature[aligned_target > 0]

                if len(no_rain_feature) > 5 and len(rain_feature) > 5:
                    mw_stat, mw_p = stats.mannwhitneyu(no_rain_feature, rain_feature, alternative='two-sided')
                    test_result['MW_Statistic'] = mw_stat
                    test_result['MW_P_Value'] = mw_p
                    test_result['MW_Significant'] = 'Yes' if mw_p < 0.05 else 'No'
                else:
                    test_result['MW_Statistic'] = np.nan
                    test_result['MW_P_Value'] = np.nan
                    test_result['MW_Significant'] = 'N/A'

                # Kruskal-Wallis Test
                no_rain_group = aligned_feature[aligned_target == 0]
                light_rain_group = aligned_feature[(aligned_target > 0) & (aligned_target <= 2.5)]
                moderate_rain_group = aligned_feature[(aligned_target > 2.5) & (aligned_target <= 7.5)]
                heavy_rain_group = aligned_feature[aligned_target > 7.5]

                groups_with_data = [g for g in [no_rain_group, light_rain_group, moderate_rain_group, heavy_rain_group] if len(g) > 3]

                if len(groups_with_data) >= 3:
                    kw_stat, kw_p = stats.kruskal(*groups_with_data)
                    test_result['KW_Statistic'] = kw_stat
                    test_result['KW_P_Value'] = kw_p
                    test_result['KW_Significant'] = 'Yes' if kw_p < 0.05 else 'No'
                else:
                    test_result['KW_Statistic'] = np.nan
                    test_result['KW_P_Value'] = np.nan
                    test_result['KW_Significant'] = 'N/A'

                # Spearman Rank Correlation
                spearman_corr, spearman_p = stats.spearmanr(aligned_feature, aligned_target)
                test_result['Spearman_Corr'] = spearman_corr
                test_result['Spearman_P_Value'] = spearman_p
                test_result['Spearman_Significant'] = 'Yes' if spearman_p < 0.05 else 'No'

            except Exception as e:
                print(f"   âš ï¸ Error in non-parametric tests for {col}: {e}")
                continue

            non_param_results.append(test_result)

        # Display non-parametric results
        non_param_df = pd.DataFrame(non_param_results) if non_param_results else pd.DataFrame()

        if not non_param_df.empty:
            print("\nğŸ“Š Non-parametric Test Results:")
            print("="*120)

            # Mann-Whitney U Test results
            mw_cols = ['Feature', 'MW_Statistic', 'MW_P_Value', 'MW_Significant']
            print("\nğŸ” Mann-Whitney U Test (Rain vs No Rain):")
            mw_results = non_param_df[mw_cols].dropna(subset=['MW_Statistic'])
            if not mw_results.empty:
                print(mw_results.round(6).to_string(index=False))

            # Kruskal-Wallis Test results
            kw_cols = ['Feature', 'KW_Statistic', 'KW_P_Value', 'KW_Significant']
            print("\nğŸ” Kruskal-Wallis Test (Multiple Rainfall Groups):")
            kw_results = non_param_df[kw_cols].dropna(subset=['KW_Statistic'])
            if not kw_results.empty:
                print(kw_results.round(6).to_string(index=False))

            # Spearman Correlation results
            spear_cols = ['Feature', 'Spearman_Corr', 'Spearman_P_Value', 'Spearman_Significant']
            print("\nğŸ” Spearman Rank Correlation with Rainfall:")
            spear_results = non_param_df[spear_cols].dropna(subset=['Spearman_Corr'])
            if not spear_results.empty:
                print(spear_results.round(6).to_string(index=False))

        return {'normality_tests': test_df, 'non_parametric_tests': non_param_df}

    def visualize_distributions(self):
        """
        4. Táº¡o visualization cho phÃ¢n phá»‘i cÃ¡c features
        """
        print("\n" + "="*80)
        print("ğŸ“Š 4. DISTRIBUTION VISUALIZATIONS")
        print("="*80)

        # 4.1 Target Variable Comprehensive Visualization
        print("ğŸ¯ Creating comprehensive target variable visualization...")

        target_data = self.df[self.target_col].dropna()

        # Create comprehensive subplot for target variable
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle(f'Comprehensive Distribution Analysis: {self.target_col}', fontsize=16, fontweight='bold')

        # Histogram with KDE
        axes[0,0].hist(target_data, bins=50, density=True, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0,0].axvline(target_data.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {target_data.mean():.2f}')
        axes[0,0].axvline(target_data.median(), color='green', linestyle='--', linewidth=2, label=f'Median: {target_data.median():.2f}')
        # Add KDE
        from scipy.stats import gaussian_kde
        kde = gaussian_kde(target_data)
        x_range = np.linspace(target_data.min(), target_data.max(), 200)
        axes[0,0].plot(x_range, kde(x_range), 'orange', linewidth=2, label='KDE')
        axes[0,0].set_title('Histogram with KDE')
        axes[0,0].set_xlabel('Precipitation (mm)')
        axes[0,0].set_ylabel('Density')
        axes[0,0].legend()
        axes[0,0].grid(True, alpha=0.3)

        # Box plot
        axes[0,1].boxplot(target_data, vert=True, patch_artist=True,
                         boxprops=dict(facecolor='lightblue', alpha=0.7))
        axes[0,1].set_title('Box Plot')
        axes[0,1].set_ylabel('Precipitation (mm)')
        axes[0,1].grid(True, alpha=0.3)

        # Q-Q plot against normal distribution
        stats.probplot(target_data, dist="norm", plot=axes[0,2])
        axes[0,2].set_title('Q-Q Plot vs Normal Distribution')
        axes[0,2].grid(True, alpha=0.3)

        # Log-scale histogram (for better visualization of skewed data)
        log_data = np.log1p(target_data)  # log(1+x) to handle zeros
        axes[1,0].hist(log_data, bins=50, density=True, alpha=0.7, color='lightgreen', edgecolor='black')
        axes[1,0].axvline(log_data.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {log_data.mean():.2f}')
        axes[1,0].axvline(log_data.median(), color='green', linestyle='--', linewidth=2, label=f'Median: {log_data.median():.2f}')
        axes[1,0].set_title('Log-Transformed Distribution')
        axes[1,0].set_xlabel('log(1 + Precipitation)')
        axes[1,0].set_ylabel('Density')
        axes[1,0].legend()
        axes[1,0].grid(True, alpha=0.3)

        # Violin plot
        axes[1,1].violinplot([target_data], positions=[1], showmeans=True, showmedians=True)
        axes[1,1].set_title('Violin Plot')
        axes[1,1].set_ylabel('Precipitation (mm)')
        axes[1,1].set_xticks([1])
        axes[1,1].set_xticklabels([self.target_col])
        axes[1,1].grid(True, alpha=0.3)

        # Empirical CDF
        sorted_data = np.sort(target_data)
        y_vals = np.arange(1, len(sorted_data) + 1) / len(sorted_data)
        axes[1,2].plot(sorted_data, y_vals, 'b-', linewidth=2)
        axes[1,2].set_title('Empirical Cumulative Distribution')
        axes[1,2].set_xlabel('Precipitation (mm)')
        axes[1,2].set_ylabel('Cumulative Probability')
        axes[1,2].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

        # 4.2 All Features Distribution Overview
        print("\nğŸ“Š Creating distribution overview for all features...")

        # Calculate number of plots needed
        n_features = len(self.analysis_cols)
        n_cols = 4
        n_rows = (n_features + n_cols - 1) // n_cols

        fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))
        fig.suptitle('Distribution Overview: All Numerical Features', fontsize=16, fontweight='bold')

        # Flatten axes array for easier indexing
        if n_rows == 1:
            axes = [axes] if n_cols == 1 else axes
        else:
            axes = axes.flatten()

        for i, col in enumerate(self.analysis_cols):
            data = self.df[col].dropna()

            if len(data) > 0:
                # Create histogram with KDE
                axes[i].hist(data, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')

                # Add statistics
                axes[i].axvline(data.mean(), color='red', linestyle='--', linewidth=1, alpha=0.8)
                axes[i].axvline(data.median(), color='green', linestyle='--', linewidth=1, alpha=0.8)

                axes[i].set_title(f'{col}\nSkew: {data.skew():.2f}, Kurt: {data.kurtosis():.2f}', fontsize=10)
                axes[i].set_xlabel('Value')
                axes[i].set_ylabel('Density')
                axes[i].grid(True, alpha=0.3)

        # Hide empty subplots
        for i in range(len(self.analysis_cols), len(axes)):
            axes[i].set_visible(False)

        plt.tight_layout()
        plt.show()

        # 4.3 Skewness and Kurtosis Comparison
        print("\nğŸ“ Creating skewness and kurtosis comparison...")

        skew_kurt_data = []
        for col in self.analysis_cols:
            data = self.df[col].dropna()
            if len(data) > 0:
                skew_kurt_data.append({
                    'Feature': col,
                    'Skewness': data.skew(),
                    'Kurtosis': data.kurtosis(),
                    'Is_Target': col == self.target_col
                })

        skew_kurt_df = pd.DataFrame(skew_kurt_data)

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

        # Skewness plot
        colors = ['red' if is_target else 'skyblue' for is_target in skew_kurt_df['Is_Target']]
        bars1 = ax1.bar(range(len(skew_kurt_df)), skew_kurt_df['Skewness'], color=colors, alpha=0.7)
        ax1.axhline(y=0, color='black', linestyle='-', linewidth=0.8)
        ax1.axhline(y=1, color='orange', linestyle='--', linewidth=0.8, alpha=0.7, label='Moderate Skew')
        ax1.axhline(y=-1, color='orange', linestyle='--', linewidth=0.8, alpha=0.7)
        ax1.set_title('Skewness Comparison Across Features')
        ax1.set_xlabel('Features')
        ax1.set_ylabel('Skewness')
        ax1.set_xticks(range(len(skew_kurt_df)))
        ax1.set_xticklabels(skew_kurt_df['Feature'], rotation=45, ha='right')
        ax1.grid(True, alpha=0.3)
        ax1.legend()

        # Kurtosis plot
        bars2 = ax2.bar(range(len(skew_kurt_df)), skew_kurt_df['Kurtosis'], color=colors, alpha=0.7)
        ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.8, label='Normal Kurtosis')
        ax2.axhline(y=3, color='orange', linestyle='--', linewidth=0.8, alpha=0.7, label='High Kurtosis')
        ax2.axhline(y=-3, color='orange', linestyle='--', linewidth=0.8, alpha=0.7)
        ax2.set_title('Kurtosis Comparison Across Features')
        ax2.set_xlabel('Features')
        ax2.set_ylabel('Kurtosis')
        ax2.set_xticks(range(len(skew_kurt_df)))
        ax2.set_xticklabels(skew_kurt_df['Feature'], rotation=45, ha='right')
        ax2.grid(True, alpha=0.3)
        ax2.legend()

        plt.tight_layout()
        plt.show()

        return skew_kurt_df

    def generate_distribution_report(self):
        """
        5. Táº¡o bÃ¡o cÃ¡o tá»•ng há»£p vá» phÃ¢n phá»‘i
        """
        print("\n" + "="*80)
        print("ğŸ“‹ 5. DISTRIBUTION ANALYSIS SUMMARY REPORT")
        print("="*80)

        # Run all analyses
        desc_stats = self.descriptive_statistics_summary()
        target_analysis = self.target_variable_deep_analysis()
        distribution_tests = self.distribution_shape_tests()
        skew_kurt_comparison = self.visualize_distributions()

        # Extract results
        normality_tests = distribution_tests['normality_tests']
        non_parametric_tests = distribution_tests['non_parametric_tests']

        # Summary insights
        print(f"\nğŸ” KEY INSIGHTS:")

        # Target variable insights
        target_stats = target_analysis['basic_stats']
        print(f"\nğŸ¯ Target Variable ({self.target_col}):")
        print(f"   - Highly right-skewed (skew={target_stats['skewness']:.2f})")
        print(f"   - Mean > Median ({target_stats['mean']:.2f} > {target_stats['median']:.2f})")
        print(f"   - High variability (CV = {(target_stats['std']/target_stats['mean'])*100:.1f}%)")

        intensity_dist = target_analysis['intensity_distribution']
        total_obs = sum(intensity_dist.values())
        print(f"   - No rain days: {intensity_dist['no_rain']/total_obs*100:.1f}%")
        print(f"   - Light-moderate rain: {(intensity_dist['light_rain']+intensity_dist['moderate_rain'])/total_obs*100:.1f}%")
        print(f"   - Heavy rain events: {(intensity_dist['heavy_rain']+intensity_dist['very_heavy_rain'])/total_obs*100:.1f}%")

        # Distribution characteristics
        if len(normality_tests) > 0:
            normal_count = sum([
                1 for _, row in normality_tests.iterrows()
                if row['Shapiro_Normal'] == 'Yes' and row['JB_Normal'] == 'Yes'
            ])
            print(f"\nğŸ“Š Distribution Characteristics:")
            print(f"   - Normally distributed features: {normal_count}/{len(normality_tests)}")
            print(f"   - Most features are non-normal (typical for weather data)")

        # Skewness patterns
        if len(skew_kurt_comparison) > 0:
            high_skew = skew_kurt_comparison[abs(skew_kurt_comparison['Skewness']) > 1]
            print(f"\nğŸ“ Skewness Patterns:")
            print(f"   - Highly skewed features: {len(high_skew)}/{len(skew_kurt_comparison)}")
            print(f"   - Features requiring transformation: {high_skew['Feature'].tolist()}")

        # Non-parametric test insights
        if not non_parametric_tests.empty:
            significant_mw = non_parametric_tests[non_parametric_tests['MW_Significant'] == 'Yes']['Feature'].tolist()
            significant_kw = non_parametric_tests[non_parametric_tests['KW_Significant'] == 'Yes']['Feature'].tolist()
            significant_spear = non_parametric_tests[non_parametric_tests['Spearman_Significant'] == 'Yes']['Feature'].tolist()

            print(f"\nğŸ”¬ Non-parametric Test Results:")
            print(f"   - Features with significant rain/no-rain differences: {len(significant_mw)} ({significant_mw})")
            print(f"   - Features with significant group differences: {len(significant_kw)} ({significant_kw})")
            print(f"   - Features with significant rank correlation: {len(significant_spear)} ({significant_spear})")

        return {
            'descriptive_stats': desc_stats,
            'target_analysis': target_analysis,
            'normality_tests': normality_tests,
            'non_parametric_tests': non_parametric_tests,
            'skew_kurt_comparison': skew_kurt_comparison,
            'summary_insights': {
                'target_skewness': target_stats['skewness'],
                'zero_rain_percentage': intensity_dist['no_rain']/total_obs*100,
                'normal_features_count': normal_count if len(normality_tests) > 0 else 0,
                'high_skew_features': high_skew['Feature'].tolist() if len(skew_kurt_comparison) > 0 else []
            }
        }

# =============================================================================
# USAGE FUNCTION
# =============================================================================

def analyze_distributions(df, target_col='LÆ°á»£ng mÆ°a'):
    """
    Cháº¡y phÃ¢n tÃ­ch phÃ¢n phá»‘i toÃ n diá»‡n

    Args:
        df: DataFrame cáº§n phÃ¢n tÃ­ch
        target_col: TÃªn cá»™t biáº¿n má»¥c tiÃªu

    Returns:
        dict: BÃ¡o cÃ¡o chi tiáº¿t vá» phÃ¢n phá»‘i
    """

    print("ğŸš€ STARTING COMPREHENSIVE DISTRIBUTION ANALYSIS")
    print("="*80)

    # Initialize analyzer
    analyzer = DistributionAnalyzer(df, target_col)

    # Generate comprehensive report
    distribution_report = analyzer.generate_distribution_report()

    print("\nâœ… DISTRIBUTION ANALYSIS COMPLETED")
    print("="*80)

    return distribution_report

# =============================================================================
# RUN ANALYSIS ON YOUR DATA
# =============================================================================

# Cháº¡y phÃ¢n tÃ­ch phÃ¢n phá»‘i cho DataFrame df_all
distribution_results = analyze_distributions(df_all, target_col='LÆ°á»£ng mÆ°a')

# Hiá»ƒn thá»‹ key insights
print(f"\nğŸ”‘ KEY FINDINGS:")
insights = distribution_results['summary_insights']
print(f"   - Target skewness: {insights['target_skewness']:.2f}")
print(f"   - Zero rain percentage: {insights['zero_rain_percentage']:.1f}%")
print(f"   - Normal features: {insights['normal_features_count']}")
print(f"   - High skew features: {len(insights['high_skew_features'])}")

"""# Time-Series Analysis"""

# =============================================================================
# BASE ANALYZER CLASS
# =============================================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from scipy import stats
from scipy.fft import fft, fftfreq
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller, acf, pacf
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import warnings
warnings.filterwarnings('ignore')

# Set plotting style
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (15, 8)
plt.rcParams['font.size'] = 10



class BaseAnalyzer:
    """
    Base class cho táº¥t cáº£ analysis components
    """
    def __init__(self, df, target_col='LÆ°á»£ng mÆ°a', date_col='NgÃ y'):
        """Initialize base analyzer with common data preparation"""
        self.df = df.copy()
        self.target_col = target_col
        self.date_col = date_col
        self._prepare_data()

    def _prepare_data(self):
        """Common data preparation for all analyzers"""
        # Ensure datetime format
        if not pd.api.types.is_datetime64_any_dtype(self.df[self.date_col]):
            self.df[self.date_col] = pd.to_datetime(self.df[self.date_col], errors='coerce')

        # Get numerical columns (excluding coordinates)
        self.numerical_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()
        coord_cols = ['VÄ© Ä‘á»™', 'Kinh Ä‘á»™']
        self.analysis_cols = [col for col in self.numerical_cols if col not in coord_cols]

        # Sort by date
        self.df = self.df.sort_values(self.date_col).reset_index(drop=True)

        # Add time features
        self.df['Month'] = self.df[self.date_col].dt.month
        self.df['Year'] = self.df[self.date_col].dt.year
        self.df['DayOfYear'] = self.df[self.date_col].dt.dayofyear
        self.df['DayOfWeek'] = self.df[self.date_col].dt.dayofweek
        self.df['Season'] = self.df['Month'].map({12:1, 1:1, 2:1, 3:2, 4:2, 5:2,
                                                 6:3, 7:3, 8:3, 9:4, 10:4, 11:4})

    def analyze(self):
        """Abstract method - must implement in subclasses"""
        raise NotImplementedError("Subclasses must implement analyze() method")


# =============================================================================
# MAIN ORCHESTRATOR CLASS
# =============================================================================

class WeatherEDAAnalyzer:
    """Updated main orchestrator with new TemporalStructureAnalyzer"""

    def __init__(self, df, target_col='LÆ°á»£ng mÆ°a', date_col='NgÃ y'):
        self.df = df.copy()
        self.target_col = target_col
        self.date_col = date_col

        print("ğŸš€ WEATHER EDA ANALYZER INITIALIZED")
        print("="*70)
        print(f"   ğŸ“Š Dataset Shape: {self.df.shape}")
        print(f"   ğŸ¯ Target Variable: {self.target_col}")

    def run_temporal_structure_analysis(self):
        """Run comprehensive temporal structure analysis"""
        self.temporal_structure = TemporalStructureAnalyzer(self.df, self.target_col, self.date_col)
        return self.temporal_structure.analyze()

    # Keep other existing methods...
    def run_extreme_events_analysis(self):
        self.extreme_events = ExtremeEventsAnalyzer(self.df, self.target_col, self.date_col)
        return self.extreme_events.analyze()

    def run_stationarity_autocorr_analysis(self, mstl_results=None):
        """Run combined stationarity and autocorrelation analysis"""
        self.stationarity_autocorr = StationarityAutocorrelationAnalyzer(
            self.df, self.target_col, self.date_col, mstl_results
        )
        return self.stationarity_autocorr.analyze()

"""## Temporal Structure & Seasonality"""

# =============================================================================
# ENHANCED COMPONENT: TEMPORAL STRUCTURE ANALYZER
# Combines: Time Series Decomposition + Frequency Analysis + Advanced Analysis
# =============================================================================

class TemporalStructureAnalyzer(BaseAnalyzer):
    """
    Enhanced Component: Comprehensive Temporal Structure Analysis
    Combines temporal patterns, frequency domain, and advanced decomposition
    """

    # Update main analyze method to include wavelet analysis
    def analyze(self):
        """Main orchestration function - runs complete temporal analysis workflow"""
        print("\n" + "="*70)
        print("ğŸ• ENHANCED COMPONENT: TEMPORAL STRUCTURE ANALYSIS")
        print("="*70)

        # Steps 1-4 (existing code...)
        visual_results = self._visual_seasonal_pattern_analysis()
        frequency_results = self._frequency_domain_analysis()
        mstl_results = self._mstl_decomposition(frequency_results.get('dominant_periods', [7, 30, 365]))

        trend_analysis = {}
        seasonal_analysis = {}
        residual_analysis = {}
        wavelet_analysis = {}

        if mstl_results['success']:
            trend_analysis = self._trend_cyclical_analysis(mstl_results)
            seasonal_analysis = self._seasonal_components_analysis(mstl_results)
            residual_analysis = self._residual_analysis(mstl_results)

            # Step 5: NEW - Wavelet analysis
            wavelet_analysis = self._wavelet_analysis(mstl_results)

        # Combine all results
        results = {
            'visual_patterns': visual_results,
            'frequency_analysis': frequency_results,
            'mstl_decomposition': mstl_results,
            'trend_analysis': trend_analysis,
            'seasonal_analysis': seasonal_analysis,
            'residual_analysis': residual_analysis,
            'wavelet_analysis': wavelet_analysis,
            'component_name': 'TemporalStructureAnalyzer'
        }

        print(f"\nâœ… TEMPORAL STRUCTURE ANALYSIS COMPLETED")
        return results

    def _visual_seasonal_pattern_analysis(self):
        """
        Step 1: Visual seasonal pattern analysis (from TimeSeriesAnalyzer)
        """
        print("ğŸ” Step 1: Visual Seasonal Pattern Analysis")
        print("-" * 50)

        # Monthly statistics
        monthly_stats = self.df.groupby('Month')[self.target_col].agg(['mean', 'std', 'median', 'count']).round(4)

        # Plot seasonal patterns
        fig, axes = plt.subplots(2, 2, figsize=(16, 10))
        fig.suptitle('Visual Seasonal Patterns Analysis', fontsize=16, fontweight='bold')

        # Monthly box plot
        sns.boxplot(data=self.df, x='Month', y=self.target_col, ax=axes[0,0])
        axes[0,0].set_title('Monthly Distribution')
        axes[0,0].set_xlabel('Month')
        axes[0,0].set_ylabel('Precipitation (mm)')
        axes[0,0].grid(True, alpha=0.3)

        # Monthly average
        monthly_mean = self.df.groupby('Month')[self.target_col].mean()
        axes[0,1].bar(monthly_mean.index, monthly_mean.values, color='skyblue', alpha=0.7)
        axes[0,1].set_title('Average Monthly Precipitation')
        axes[0,1].set_xlabel('Month')
        axes[0,1].set_ylabel('Average Precipitation (mm)')
        axes[0,1].grid(True, alpha=0.3)

        # Daily pattern within year
        daily_pattern = self.df.groupby('DayOfYear')[self.target_col].mean()
        axes[1,0].plot(daily_pattern.index, daily_pattern.values, 'b-', linewidth=1, alpha=0.7)
        axes[1,0].set_title('Daily Pattern Throughout Year')
        axes[1,0].set_xlabel('Day of Year')
        axes[1,0].set_ylabel('Average Precipitation (mm)')
        axes[1,0].grid(True, alpha=0.3)

        # Yearly trend
        yearly_stats = self.df.groupby('Year')[self.target_col].mean()
        axes[1,1].plot(yearly_stats.index, yearly_stats.values, 'ro-', linewidth=2, markersize=6)
        axes[1,1].set_title('Yearly Trend')
        axes[1,1].set_xlabel('Year')
        axes[1,1].set_ylabel('Average Precipitation (mm)')
        axes[1,1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

        # Identify seasons
        wet_months = monthly_mean[monthly_mean > monthly_mean.quantile(0.75)].index.tolist()
        dry_months = monthly_mean[monthly_mean < monthly_mean.quantile(0.25)].index.tolist()

        print(f"   ğŸŒ§ï¸ Wet Season (months): {wet_months}")
        print(f"   â˜€ï¸ Dry Season (months): {dry_months}")

        return {
            'monthly_stats': monthly_stats,
            'monthly_mean': monthly_mean,
            'daily_pattern': daily_pattern,
            'yearly_stats': yearly_stats,
            'wet_months': wet_months,
            'dry_months': dry_months
        }

    def _frequency_domain_analysis(self):
        """
        Step 2: Frequency domain analysis to detect periods (from FrequencyAnalyzer)
        """
        print(f"\nğŸ”Š Step 2: Frequency Domain Analysis for Period Detection")
        print("-" * 50)

        # Prepare time series data
        ts_data = self.df.set_index(self.date_col).sort_index()
        target_ts = ts_data[self.target_col].dropna()

        if len(target_ts) < 365:
            print("   âš ï¸ Insufficient data for frequency analysis")
            return {'success': False, 'dominant_periods': [7, 30, 365]}

        # Remove trend for better frequency analysis
        detrended = target_ts - target_ts.rolling(window=30, center=True).mean()
        detrended = detrended.dropna()

        # Perform FFT
        fft_values = fft(detrended.values)
        frequencies = fftfreq(len(detrended), d=1)

        # Get positive frequencies
        positive_freq_idx = frequencies > 0
        positive_frequencies = frequencies[positive_freq_idx]
        positive_fft_magnitude = np.abs(fft_values[positive_freq_idx])
        periods = 1 / positive_frequencies

        # Find dominant periods
        peak_indices = np.argsort(positive_fft_magnitude)[-10:]
        dominant_periods = periods[peak_indices]

        # Filter reasonable periods (7 days to 2 years)
        reasonable_periods = dominant_periods[(dominant_periods >= 7) & (dominant_periods <= 730)]

        print(f"   ğŸ“Š Detected {len(reasonable_periods)} dominant periods for MSTL:")

        for period in reasonable_periods[::-1]:
            print(f"      - {period:.1f} days")

        dominant_periods_int = sorted(list(set(np.round(reasonable_periods).astype(int))))

        return {
            'dominant_periods': dominant_periods_int,
            'all_periods': periods,
            'magnitudes': positive_fft_magnitude,
            'success': True
        }

    def _mstl_decomposition(self, periods):
        """
        Step 3: Enhanced MSTL decomposition with detected periods
        """
        print(f"\nğŸ“Š Step 3: MSTL Decomposition with Detected Periods")
        print("-" * 50)

        try:
            from statsmodels.tsa.seasonal import MSTL

            # Prepare time series
            ts_data = self.df.set_index(self.date_col).sort_index()
            target_ts = ts_data[self.target_col].dropna()

            # Apply log transformation for better decomposition
            log_ts = np.log1p(target_ts)
            print(f"   ğŸ”„ Applied log1p transformation")

            # Run MSTL decomposition
            mstl = MSTL(log_ts, periods=periods)
            mstl_result = mstl.fit()

            print(f"   âœ… MSTL decomposition completed successfully")

            # Store results
            return {
                'mstl_obj': mstl_result,
                'log_ts': log_ts,
                'original_ts': target_ts,
                'periods': periods,
                'trend': mstl_result.trend,
                'seasonal': mstl_result.seasonal,
                'resid': mstl_result.resid,
                'success': True
            }

        except Exception as e:
            print(f"   âš ï¸ MSTL decomposition failed: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    def _trend_cyclical_analysis(self, mstl_results):
        """
        Step 4a: NEW - Trend and cyclical component analysis
        """
        print(f"\nğŸ“ˆ Step 4a: Trend & Cyclical Analysis")
        print("-" * 50)

        trend = mstl_results['trend']
        original_ts = mstl_results['original_ts']

        # Trend visualization and analysis
        fig, axes = plt.subplots(2, 1, figsize=(15, 10))
        fig.suptitle('Trend & Cyclical Component Analysis', fontsize=16, fontweight='bold')

        # Trend component
        axes[0].plot(trend.index, trend.values, 'g-', linewidth=2, label='Trend')
        axes[0].set_title('Long-term Trend Component')
        axes[0].set_ylabel('Log Trend')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)

        # Original vs trend
        axes[1].plot(original_ts.index, original_ts.values, 'b-', alpha=0.7, label='Original')
        trend_original_scale = np.expm1(trend)  # Convert back from log scale
        axes[1].plot(trend.index, trend_original_scale.values, 'g-', linewidth=2, label='Trend (original scale)')
        axes[1].set_title('Original Data vs Extracted Trend')
        axes[1].set_ylabel('Precipitation (mm)')
        axes[1].set_xlabel('Date')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

        # Quantitative trend analysis
        trend_clean = trend.dropna()
        if len(trend_clean) > 2:
            # Calculate trend slope (simple linear regression)
            x_numeric = np.arange(len(trend_clean))
            slope, intercept = np.polyfit(x_numeric, trend_clean.values, 1)

            trend_direction = "Increasing" if slope > 0 else "Decreasing" if slope < 0 else "Stable"
            print(f"   ğŸ“Š Trend Analysis:")
            print(f"      - Direction: {trend_direction}")
            print(f"      - Slope: {slope:.6f} log-units per day")
            print(f"      - Annual change: {slope * 365:.3f} log-units per year")

        # Cyclical patterns (detrended analysis)
        detrended = original_ts - trend_original_scale
        detrended_clean = detrended.dropna()

        if len(detrended_clean) > 100:
            print(f"   ğŸ”„ Cyclical patterns (after detrending):")
            print(f"      - Detrended data range: {detrended_clean.min():.2f} to {detrended_clean.max():.2f} mm")
            print(f"      - Variability: {detrended_clean.std():.2f} mm (std)")

        return {
            'trend_component': trend,
            'trend_direction': trend_direction if 'trend_direction' in locals() else 'Unknown',
            'trend_slope': slope if 'slope' in locals() else None,
            'detrended_data': detrended_clean if 'detrended_clean' in locals() else None
        }

    def _seasonal_components_analysis(self, mstl_results):
        """
        Step 4b: NEW - Individual seasonal components analysis (COMPLETELY FIXED)
        """
        print(f"\nğŸŒ€ Step 4b: Seasonal Components Analysis")
        print("-" * 50)

        mstl_obj = mstl_results['mstl_obj']
        periods = mstl_results['periods']
        # MSTL provides multiple seasonal components
        seasonal_components = {}

        try:
            # Extract individual seasonal components
            for i, period in enumerate(periods):
                component_name = f"seasonal_{period}d"
                # Access individual seasonal components from MSTL
                if hasattr(mstl_obj, 'seasonal_dict'):
                    seasonal_components[component_name] = mstl_obj.seasonal_dict[period]
                else:
                    # If not available, show combined seasonal
                    if i == 0:
                        seasonal_components['seasonal_combined'] = mstl_obj.seasonal

            # Visualization
            n_components = len(seasonal_components)
            if n_components > 0:
                fig, axes = plt.subplots(n_components, 1, figsize=(15, 4*n_components))
                if n_components == 1:
                    axes = [axes]

                fig.suptitle('Individual Seasonal Components', fontsize=16, fontweight='bold')

                for i, (comp_name, comp_data) in enumerate(seasonal_components.items()):
                    axes[i].plot(comp_data.index, comp_data.values, 'r-', linewidth=1.5)
                    axes[i].set_title(f'{comp_name.replace("_", " ").title()}')
                    axes[i].set_ylabel('Log Seasonal Effect')
                    axes[i].grid(True, alpha=0.3)

                    # COMPLETELY FIXED: Safe scalar extraction
                    try:
                        # Method 1: Use numpy/pandas operations and convert safely
                        comp_values = comp_data.values  # Get numpy array
                        comp_max = np.max(comp_values)
                        comp_min = np.min(comp_values)
                        comp_std = np.std(comp_values)

                        # Ensure scalar conversion
                        comp_range_val = float(comp_max - comp_min)
                        comp_std_val = float(comp_std)

                        axes[i].text(0.02, 0.95, f'Range: {comp_range_val:.4f}\nStd: {comp_std_val:.4f}',
                                  transform=axes[i].transAxes,
                                  bbox=dict(boxstyle="round", facecolor='wheat'),
                                  verticalalignment='top')
                    except Exception as text_error:
                        # Fallback: Simple component label without statistics
                        axes[i].text(0.02, 0.95, f'Component: {comp_name}',
                                  transform=axes[i].transAxes,
                                  bbox=dict(boxstyle="round", facecolor='lightgray'),
                                  verticalalignment='top')
                        print(f"      âš ï¸ Text annotation fallback for {comp_name}")

                axes[-1].set_xlabel('Date')
                plt.tight_layout()
                plt.show()

            print(f"   ğŸ“Š Seasonal Components Summary:")
            for comp_name, comp_data in seasonal_components.items():
                try:
                    # COMPLETELY FIXED: Safe scalar extraction for summary
                    comp_values = comp_data.values  # Get numpy array
                    strength_val = float(np.std(comp_values))  # Use numpy std directly
                    print(f"      - {comp_name}: strength = {strength_val:.4f}")
                except Exception as print_error:
                    # Fallback: Basic info without strength calculation
                    print(f"      - {comp_name}: component available (strength calculation skipped)")

        except Exception as e:
            print(f"   âš ï¸ Error in seasonal component analysis: {e}")
            # Fallback to combined seasonal component
            try:
                seasonal_components = {'seasonal_combined': mstl_obj.seasonal}
                print(f"   ğŸ“ Using combined seasonal component as fallback")
            except:
                seasonal_components = {}

        return {
            'seasonal_components': seasonal_components,
            'periods_analyzed': periods,
            'components_count': len(seasonal_components)
        }

    def _residual_analysis(self, mstl_results):
        """
        Step 4c: NEW & IMPORTANT - Residual diagnostic analysis
        """
        print(f"\nğŸ” Step 4c: Residual Diagnostic Analysis")
        print("-" * 50)

        residuals = mstl_results['resid'].dropna()

        # Residual statistics
        print(f"   ğŸ“Š Residual Statistics:")
        print(f"      - Mean: {residuals.mean():.6f}")
        print(f"      - Std: {residuals.std():.6f}")
        print(f"      - Skewness: {residuals.skew():.6f}")
        print(f"      - Kurtosis: {residuals.kurtosis():.6f}")

        # Residual diagnostic plots
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Residual Diagnostic Analysis', fontsize=16, fontweight='bold')

        # Time series plot of residuals
        axes[0,0].plot(residuals.index, residuals.values, 'purple', alpha=0.7, linewidth=1)
        axes[0,0].axhline(y=0, color='red', linestyle='--', alpha=0.7)
        axes[0,0].set_title('Residuals Over Time')
        axes[0,0].set_ylabel('Residual')
        axes[0,0].grid(True, alpha=0.3)

        # Residual histogram
        axes[0,1].hist(residuals.values, bins=50, alpha=0.7, color='purple', density=True)
        axes[0,1].set_title('Residual Distribution')
        axes[0,1].set_xlabel('Residual Value')
        axes[0,1].set_ylabel('Density')
        axes[0,1].grid(True, alpha=0.3)

        # ACF of residuals
        try:
            from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
            plot_acf(residuals, ax=axes[1,0], lags=40, alpha=0.05)
            axes[1,0].set_title('ACF of Residuals')
        except:
            axes[1,0].set_title('ACF Plot Not Available')

        # Q-Q plot
        try:
            from scipy.stats import probplot
            probplot(residuals.values, dist="norm", plot=axes[1,1])
            axes[1,1].set_title('Q-Q Plot (Normal)')
        except:
            axes[1,1].set_title('Q-Q Plot Not Available')

        plt.tight_layout()
        plt.show()

        # Ljung-Box test for autocorrelation in residuals
        try:
            from statsmodels.stats.diagnostic import acorr_ljungbox
            ljung_box_result = acorr_ljungbox(residuals, lags=10, return_df=True)
            significant_lags = ljung_box_result[ljung_box_result['lb_pvalue'] < 0.05]

            print(f"   ğŸ§ª Ljung-Box Test Results:")
            if len(significant_lags) > 0:
                print(f"      âš ï¸ Significant autocorrelation detected at {len(significant_lags)} lags")
                print(f"      - First significant lag: {significant_lags.index[0]}")
            else:
                print(f"      âœ… No significant autocorrelation in residuals")

        except Exception as e:
            print(f"   âš ï¸ Ljung-Box test failed: {e}")
            significant_lags = None

        return {
            'residuals': residuals,
            'residual_stats': {
                'mean': residuals.mean(),
                'std': residuals.std(),
                'skewness': residuals.skew(),
                'kurtosis': residuals.kurtosis()
            },
            'ljung_box_test': significant_lags,
            'autocorr_clean': len(significant_lags) == 0 if significant_lags is not None else None
        }

    def _wavelet_analysis(self, mstl_results=None):
        """
        Step 5: Enhanced Wavelet Analysis using PyWavelets
        """
        print(f"\nğŸŒŠ Step 5: Wavelet Analysis (PyWavelets)")
        print("-" * 50)

        try:
            import pywt

            # Prepare time series data
            ts_data = self.df.set_index(self.date_col).sort_index()
            target_ts = ts_data[self.target_col].dropna()

            # Trong hÃ m _wavelet_analysis

            analysis_duration_years = 3
            analysis_duration_days = 365 * analysis_duration_years

            if len(target_ts) > analysis_duration_days:
                target_ts = target_ts.tail(analysis_duration_days)
                print(f"   ğŸ“Š Using last {analysis_duration_years} years ({analysis_duration_days} days) for analysis")

            data = target_ts.values
            dates = target_ts.index

            print(f"   ğŸ”„ Running Continuous Wavelet Transform with PyWavelets...")

            # Define scales for periods 3-120 days
            scales = np.arange(3, 121, 2)  # Simplified scale range

            # Perform CWT using Morlet wavelet
            coefficients, frequencies = pywt.cwt(data, scales, 'morl')

            # Convert frequencies to periods
            periods = 1 / frequencies

            # Calculate power
            power = np.abs(coefficients) ** 2

            print(f"   âœ… PyWavelets analysis completed")

            # Visualization
            self._plot_pywt_results(data, dates, periods, power)

            # Analysis insights
            insights = self._extract_pywt_insights(periods, power, dates)

            return {
                'implemented': True,
                'library': 'PyWavelets',
                'coefficients': coefficients,
                'power': power,
                'periods': periods,
                'scales': scales,
                'insights': insights,
                'wavelet_type': 'Morlet'
            }

        except ImportError:
            print(f"   âš ï¸ PyWavelets not available, falling back to scipy.signal")
            return self._wavelet_analysis_fallback()
        except Exception as e:
            print(f"   âš ï¸ PyWavelets analysis failed: {e}")
            return {'implemented': False, 'error': str(e)}

    def _plot_pywt_results(self, data, dates, periods, power):
        """Plot PyWavelets results (simplified)"""
        fig, axes = plt.subplots(2, 1, figsize=(15, 10))
        fig.suptitle('PyWavelets Analysis', fontsize=16, fontweight='bold')

        # 1. Original time series
        axes[0].plot(dates, data, 'b-', linewidth=1.5)
        axes[0].set_title('Original Time Series')
        axes[0].set_ylabel('Precipitation (mm)')
        axes[0].grid(True, alpha=0.3)

        # 2. Wavelet power spectrum
        dates_num = np.arange(len(dates))
        T, P = np.meshgrid(dates_num, periods)

        log_power = np.log10(power + 1e-12)
        im = axes[1].contourf(T, P, log_power, levels=30, cmap='jet')
        axes[1].set_title('Wavelet Power Spectrum')
        axes[1].set_ylabel('Period (days)')
        axes[1].set_xlabel('Time Index')

        plt.colorbar(im, ax=axes[1])
        plt.tight_layout()
        plt.show()

    def _extract_pywt_insights(self, periods, power, dates):
        """Extract insights from PyWavelets analysis"""
        # Global power spectrum
        global_power = np.mean(power, axis=1)

        # Find dominant periods
        peak_indices = np.argsort(global_power)[-5:]  # Top 5
        dominant_periods = []

        for idx in peak_indices:
            if idx < len(periods):
                period = periods[idx]
                power_val = global_power[idx]
                dominant_periods.append({
                    'period': period,
                    'power': power_val
                })

        print(f"   ğŸ“Š PyWavelets Insights:")
        print(f"      - Dominant periods: {len(dominant_periods)}")
        for i, p in enumerate(dominant_periods[-3:], 1):  # Top 3
            print(f"        {i}. {p['period']:.1f} days")

        return {'dominant_periods': dominant_periods}

    def _wavelet_analysis_fallback(self):
        """Fallback to scipy.signal if pywt not available"""
        print(f"   ğŸ”„ Using scipy.signal fallback...")

        try:
            from scipy.signal import cwt, morlet2

            # Simplified fallback implementation
            ts_data = self.df.set_index(self.date_col).sort_index()
            target_ts = ts_data[self.target_col].dropna().tail(365)  # Last year only

            data = target_ts.values
            scales = np.logspace(1, 2, 20)  # Simplified scales

            coefficients = cwt(data, morlet2, scales)
            power = np.abs(coefficients) ** 2

            print(f"   âœ… Scipy fallback completed")

            return {
                'implemented': True,
                'library': 'scipy.signal (fallback)',
                'power': power,
                'scales': scales,
                'simplified': True
            }

        except Exception as e:
            print(f"   âŒ Fallback also failed: {e}")
            return {'implemented': False, 'error': 'Both pywt and scipy failed'}

# Initialize the enhanced analyzer
analyzer = WeatherEDAAnalyzer(df_all, target_col='LÆ°á»£ng mÆ°a', date_col='NgÃ y')

# Run comprehensive temporal structure analysis
temporal_results = analyzer.run_temporal_structure_analysis()

# Access results
if temporal_results['mstl_decomposition']['success']:
    print("âœ… Enhanced Temporal Structure Analysis completed:")
    print(f"   - Visual patterns: âœ…")
    print(f"   - Frequency analysis: âœ…")
    print(f"   - MSTL decomposition: âœ…")
    print(f"   - Trend analysis: âœ…")
    print(f"   - Seasonal analysis: âœ…")
    print(f"   - Residual diagnostics: âœ…")
    print(f"   - Wavelet Analysis using PyWavelets: âœ…")

"""## Stationarity & Autocorrelation"""

# =============================================================================
# COMPONENT 3: STATIONARITY & AUTOCORRELATION DIAGNOSTICS (COMBINED)
# =============================================================================

class StationarityAutocorrelationAnalyzer(BaseAnalyzer):
    """
    Pháº§n 3: Cháº©n Ä‘oÃ¡n TÃ­nh dá»«ng vÃ  Tá»± tÆ°Æ¡ng quan
    Káº¿t há»£p StationarityAnalyzer + AutocorrelationAnalyzer
    """

    def __init__(self, df, target_col='LÆ°á»£ng mÆ°a', date_col='NgÃ y', mstl_results=None):
        """Initialize with optional MSTL results from Part 2"""
        super().__init__(df, target_col, date_col)
        self.mstl_results = mstl_results

    def analyze(self):
        """
        Main workflow: Stationarity â†’ Autocorrelation â†’ Diagnostics
        """
        print("\n" + "="*80)
        print("ğŸ” PHáº¦N 3: STATIONARITY & AUTOCORRELATION DIAGNOSTICS")
        print("="*80)
        print("ğŸ¯ Má»¥c tiÃªu: Cháº©n Ä‘oÃ¡n tÃ­nh dá»«ng vÃ  cáº¥u trÃºc tá»± tÆ°Æ¡ng quan")

        # Prepare time series data
        ts_data = self.df.set_index(self.date_col).sort_index()
        target_ts = ts_data[self.target_col].dropna()

        if len(target_ts) < 50:
            print("   âš ï¸ Insufficient data for comprehensive diagnostics")
            return {'success': False, 'error': 'Insufficient data'}

        # Step 1: Stationarity Diagnostics
        stationarity_results = self._stationarity_diagnostics(target_ts)

        # Step 2: Autocorrelation Analysis
        autocorr_results = self._autocorrelation_analysis(target_ts)

        # Step 3: Residual Diagnostics (if MSTL results available)
        residual_results = self._residual_diagnostics() if self.mstl_results else None

        # Step 4: Comprehensive Synthesis
        synthesis = self._comprehensive_synthesis(stationarity_results, autocorr_results, residual_results)

        # Combine all results
        results = {
            'stationarity': stationarity_results,
            'autocorrelation': autocorr_results,
            'residual_diagnostics': residual_results,
            'synthesis': synthesis,
            'component_name': 'StationarityAutocorrelationAnalyzer'
        }

        return results

    def _stationarity_diagnostics(self, target_ts):
        """BÆ°á»›c 1: Cháº©n Ä‘oÃ¡n TÃ­nh dá»«ng"""
        print(f"\nğŸ” BÆ¯á»šC 1: CHáº¨N ÄOÃN TÃNH Dá»ªNG")
        print("="*60)

        # ADF Test
        adf_results = self._perform_adf_test(target_ts)

        # KPSS Test
        kpss_results = self._perform_kpss_test(target_ts)

        # Combined Assessment
        assessment = self._stationarity_assessment(adf_results, kpss_results)

        # Visual Analysis
        self._visual_stationarity_analysis(target_ts)

        return {
            'adf_test': adf_results,
            'kpss_test': kpss_results,
            'assessment': assessment,
            'success': True
        }

    def _autocorrelation_analysis(self, target_ts):
        """BÆ°á»›c 2: PhÃ¢n tÃ­ch Tá»± tÆ°Æ¡ng quan"""
        print(f"\nğŸ”„ BÆ¯á»šC 2: PHÃ‚N TÃCH Tá»° TÆ¯Æ NG QUAN")
        print("="*60)

        # ACF Analysis on original series
        acf_results = self._perform_acf_analysis(target_ts, "Original Series")

        # PACF Analysis on original series
        pacf_results = self._perform_pacf_analysis(target_ts, "Original Series")

        # SARIMA Suggestions
        sarima_suggestions = self._generate_sarima_suggestions(acf_results, pacf_results)

        # Visualization
        self._autocorrelation_visualization(target_ts, "Original Series")

        return {
            'acf_results': acf_results,
            'pacf_results': pacf_results,
            'sarima_suggestions': sarima_suggestions,
            'success': True
        }

    def _residual_diagnostics(self):
        """BÆ°á»›c 2b: Cháº©n Ä‘oÃ¡n Residual tá»« MSTL"""
        if not self.mstl_results or 'residual' not in self.mstl_results:
            print(f"\nâš ï¸ MSTL results not available for residual diagnostics")
            return {'success': False, 'error': 'No MSTL results'}

        print(f"\nğŸ§ª BÆ¯á»šC 2B: CHáº¨N ÄOÃN RESIDUAL MSTL")
        print("="*60)

        residual = self.mstl_results['residual'].dropna()

        if len(residual) < 50:
            return {'success': False, 'error': 'Insufficient residual data'}

        # ACF/PACF of residuals
        residual_acf = self._perform_acf_analysis(residual, "MSTL Residual")
        residual_pacf = self._perform_pacf_analysis(residual, "MSTL Residual")

        # Ljung-Box Test for residual randomness
        ljung_box_results = self._ljung_box_test(residual)

        # Residual visualization
        self._residual_visualization(residual)

        return {
            'residual_acf': residual_acf,
            'residual_pacf': residual_pacf,
            'ljung_box': ljung_box_results,
            'quality_assessment': self._assess_mstl_quality(residual_acf, ljung_box_results),
            'success': True
        }

    def _comprehensive_synthesis(self, stationarity_results, autocorr_results, residual_results):
        """BÆ°á»›c 4: Tá»•ng há»£p & Káº¿t luáº­n"""
        print(f"\nğŸ“‹ BÆ¯á»šC 4: Tá»”NG Há»¢P & Káº¾T LUáº¬N")
        print("="*60)

        # Extract key findings
        is_stationary = stationarity_results['assessment']['tests_agree']
        stationarity_type = stationarity_results['assessment']['stationarity_type']
        significant_lags = autocorr_results['acf_results']['significant_lags'][:5]
        seasonal_lags = [lag for lag, _ in significant_lags if lag in [7, 14, 30, 365]]

        # Generate synthesis report
        synthesis_report = {
            'stationarity_conclusion': stationarity_type,
            'differencing_needed': not is_stationary,
            'significant_lags': [lag for lag, _ in significant_lags],
            'seasonal_lags_detected': seasonal_lags,
            'model_recommendations': []
        }

        # Model recommendations
        if is_stationary:
            synthesis_report['model_recommendations'].append("ARMA models suitable")
        else:
            synthesis_report['model_recommendations'].append("ARIMA models recommended (d=1)")

        if seasonal_lags:
            synthesis_report['model_recommendations'].append("SARIMA models for seasonal patterns")

        # MSTL Quality Assessment
        if residual_results and residual_results['success']:
            mstl_quality = residual_results['quality_assessment']
            synthesis_report['mstl_decomposition_quality'] = mstl_quality

            if mstl_quality['overall_quality'] == 'Good':
                synthesis_report['model_recommendations'].append("MSTL decomposition captured most patterns")
            else:
                synthesis_report['model_recommendations'].append("Additional AR/MA terms may be needed")

        # Print synthesis
        print(f"   ğŸ“Š PHÃT HIá»†N CHÃNH:")
        print(f"      ğŸ”¹ TÃ­nh dá»«ng: {stationarity_type}")
        print(f"      ğŸ”¹ Cáº§n sai phÃ¢n: {'CÃ³' if synthesis_report['differencing_needed'] else 'KhÃ´ng'}")
        print(f"      ğŸ”¹ Lags quan trá»ng: {synthesis_report['significant_lags'][:5]}")
        print(f"      ğŸ”¹ Lags mÃ¹a vá»¥: {seasonal_lags}")

        print(f"\n   ğŸ’¡ KHUYáº¾N NGHá»Š MÃ” HÃŒNH:")
        for rec in synthesis_report['model_recommendations']:
            print(f"      â€¢ {rec}")

        return synthesis_report

    # Helper methods from original components
    def _perform_adf_test(self, ts):
        """ADF Test implementation"""
        try:
            from statsmodels.tsa.stattools import adfuller
            result = adfuller(ts, regression='ct', autolag='AIC')

            print(f"   ğŸ“Š ADF Test:")
            print(f"      - Statistic: {result[0]:.6f}")
            print(f"      - P-value: {result[1]:.6f}")
            print(f"      - Result: {'âœ… Stationary' if result[1] < 0.05 else 'âŒ Non-stationary'}")

            return {
                'statistic': result[0],
                'pvalue': result[1],
                'critical_values': result[4],
                'is_stationary': result[1] < 0.05,
                'success': True
            }
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def _perform_kpss_test(self, ts):
        """KPSS Test implementation"""
        try:
            from statsmodels.tsa.stattools import kpss
            result = kpss(ts, regression='ct')

            print(f"   ğŸ“Š KPSS Test:")
            print(f"      - Statistic: {result[0]:.6f}")
            print(f"      - P-value: {result[1]:.6f}")
            print(f"      - Result: {'âœ… Stationary' if result[1] > 0.05 else 'âŒ Non-stationary'}")

            return {
                'statistic': result[0],
                'pvalue': result[1],
                'critical_values': result[3],
                'is_stationary': result[1] > 0.05,
                'success': True
            }
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def _stationarity_assessment(self, adf, kpss):
        """Combined stationarity assessment"""
        adf_stat = adf.get('is_stationary', False)
        kpss_stat = kpss.get('is_stationary', False)

        if adf_stat and kpss_stat:
            conclusion = "âœ… STATIONARY"
            stationarity_type = "Stationary"
        elif adf_stat and not kpss_stat:
            conclusion = "ğŸŸ¡ DIFFERENCE STATIONARY"
            stationarity_type = "Difference Stationary"
        elif not adf_stat and kpss_stat:
            conclusion = "ğŸŸ¡ TREND STATIONARY"
            stationarity_type = "Trend Stationary"
        else:
            conclusion = "âŒ NON-STATIONARY"
            stationarity_type = "Non-stationary"

        print(f"   ğŸ¯ Káº¿t luáº­n: {conclusion}")

        return {
            'conclusion': conclusion,
            'stationarity_type': stationarity_type,
            'tests_agree': adf_stat == kpss_stat,
            'adf_agrees': adf_stat,
            'kpss_agrees': kpss_stat
        }

    def _perform_acf_analysis(self, ts, label):
        """ACF Analysis implementation"""
        try:
            max_lags = min(100, len(ts) // 4)
            acf_values, acf_confint = acf(ts, nlags=max_lags, alpha=0.05, fft=True)

            # Find significant lags
            significant_lags = []
            for lag in range(1, len(acf_values)):
                if abs(acf_values[lag]) > abs(acf_confint[lag, 1] - acf_values[lag]):
                    significant_lags.append((lag, acf_values[lag]))

            print(f"   ğŸ“ˆ ACF ({label}) - Significant lags: {len(significant_lags)}")

            return {
                'acf_values': acf_values,
                'significant_lags': significant_lags,
                'max_lags': max_lags,
                'success': True
            }
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def _perform_pacf_analysis(self, ts, label):
        """PACF Analysis implementation"""
        try:
            max_lags = min(100, len(ts) // 4)
            pacf_values, pacf_confint = pacf(ts, nlags=max_lags, alpha=0.05)

            # Find significant lags
            significant_lags = []
            for lag in range(1, len(pacf_values)):
                if abs(pacf_values[lag]) > abs(pacf_confint[lag, 1] - pacf_values[lag]):
                    significant_lags.append((lag, pacf_values[lag]))

            print(f"   ğŸ“ˆ PACF ({label}) - Significant lags: {len(significant_lags)}")

            return {
                'pacf_values': pacf_values,
                'significant_lags': significant_lags,
                'max_lags': max_lags,
                'success': True
            }
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def _generate_sarima_suggestions(self, acf_results, pacf_results):
        """Generate SARIMA parameter suggestions"""
        if not (acf_results['success'] and pacf_results['success']):
            return {'success': False}

        # Extract significant lags
        pacf_lags = [lag for lag, _ in pacf_results['significant_lags'][:5]]
        acf_lags = [lag for lag, _ in acf_results['significant_lags'][:5]]

        # Simple heuristics for SARIMA orders
        p = min(3, len(pacf_lags)) if pacf_lags else 1
        q = min(3, len(acf_lags)) if acf_lags else 1

        # Seasonal parameters (check for seasonal lags)
        seasonal_lags = [lag for lag in pacf_lags + acf_lags if lag in [7, 14, 30, 365]]
        P = 1 if seasonal_lags else 0
        Q = 1 if seasonal_lags else 0
        s = max(seasonal_lags) if seasonal_lags else 0

        suggestions = {
            'nonseasonal_ar': p,
            'nonseasonal_ma': q,
            'seasonal_ar': P,
            'seasonal_ma': Q,
            'seasonal_period': s,
            'suggested_models': [
                f'SARIMA({p},1,{q})x({P},1,{Q},{s})' if s > 0 else f'ARIMA({p},1,{q})',
                f'SARIMA({p},0,{q})x({P},0,{Q},{s})' if s > 0 else f'ARIMA({p},0,{q})'
            ],
            'success': True
        }

        print(f"   ğŸ¯ SARIMA Suggestions: {suggestions['suggested_models']}")
        return suggestions

    def _ljung_box_test(self, residual):
        """Ljung-Box test for residual randomness"""
        try:
            from statsmodels.stats.diagnostic import acorr_ljungbox

            lbvalue = acorr_ljungbox(residual, lags=min(20, len(residual)//4), return_df=True)
            pvalue_min = lbvalue['lb_pvalue'].min()

            print(f"   ğŸ§ª Ljung-Box Test:")
            print(f"      - Min p-value: {pvalue_min:.6f}")
            print(f"      - Result: {'âœ… Random residuals' if pvalue_min > 0.05 else 'âš ï¸ Some structure remains'}")

            return {
                'ljung_box_results': lbvalue,
                'min_pvalue': float(pvalue_min),
                'is_random': pvalue_min > 0.05,
                'success': True
            }
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def _assess_mstl_quality(self, residual_acf, ljung_box):
        """Assess MSTL decomposition quality"""
        if not (residual_acf['success'] and ljung_box['success']):
            return {'overall_quality': 'Unknown', 'reasons': ['Analysis failed']}

        significant_residual_lags = len(residual_acf['significant_lags'])
        is_random = ljung_box['is_random']

        if is_random and significant_residual_lags < 5:
            quality = 'Good'
            reasons = ['Residuals appear random', 'Few significant autocorrelations']
        elif is_random or significant_residual_lags < 10:
            quality = 'Moderate'
            reasons = ['Some structure may remain', 'Consider additional AR/MA terms']
        else:
            quality = 'Poor'
            reasons = ['Significant structure in residuals', 'MSTL may be insufficient']

        print(f"   ğŸ“Š MSTL Quality: {quality}")
        for reason in reasons:
            print(f"      â€¢ {reason}")

        return {
            'overall_quality': quality,
            'reasons': reasons,
            'significant_lags_count': significant_residual_lags,
            'residuals_random': is_random
        }

    def _visual_stationarity_analysis(self, target_ts):
        """Visual stationarity analysis"""
        window = min(365, len(target_ts) // 4)
        rolling_mean = target_ts.rolling(window=window).mean()
        rolling_std = target_ts.rolling(window=window).std()

        fig, axes = plt.subplots(2, 2, figsize=(16, 8))
        fig.suptitle('Stationarity Visual Diagnostics', fontsize=14, fontweight='bold')

        # Original + Rolling Mean
        axes[0,0].plot(target_ts.index, target_ts.values, 'b-', alpha=0.6, label='Original')
        axes[0,0].plot(rolling_mean.index, rolling_mean.values, 'r-', linewidth=2, label=f'Rolling Mean ({window}d)')
        axes[0,0].set_title('Time Series with Rolling Mean')
        axes[0,0].legend()
        axes[0,0].grid(True, alpha=0.3)

        # Rolling Std
        axes[0,1].plot(rolling_std.index, rolling_std.values, 'g-', linewidth=2)
        axes[0,1].set_title(f'Rolling Standard Deviation ({window}d)')
        axes[0,1].grid(True, alpha=0.3)

        # First Difference
        first_diff = target_ts.diff().dropna()
        axes[1,0].plot(first_diff.index, first_diff.values, 'purple', alpha=0.7)
        axes[1,0].set_title('First Difference')
        axes[1,0].grid(True, alpha=0.3)

        # Distribution Comparison
        axes[1,1].hist(target_ts.values, bins=30, alpha=0.7, label='Original', density=True)
        axes[1,1].hist(first_diff.values, bins=30, alpha=0.7, label='First Difference', density=True)
        axes[1,1].set_title('Distribution Comparison')
        axes[1,1].legend()
        axes[1,1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

    def _autocorrelation_visualization(self, ts, label):
        """ACF/PACF visualization"""
        max_lags = min(40, len(ts) // 4)

        fig, axes = plt.subplots(2, 2, figsize=(16, 8))
        fig.suptitle(f'Autocorrelation Analysis - {label}', fontsize=14, fontweight='bold')

        # ACF plot
        try:
            plot_acf(ts, lags=max_lags, ax=axes[0,0], alpha=0.05)
            axes[0,0].set_title(f'ACF - {label}')
            axes[0,0].grid(True, alpha=0.3)
        except:
            axes[0,0].text(0.5, 0.5, 'ACF Plot Error', ha='center', va='center', transform=axes[0,0].transAxes)

        # PACF plot
        try:
            plot_pacf(ts, lags=max_lags, ax=axes[0,1], alpha=0.05)
            axes[0,1].set_title(f'PACF - {label}')
            axes[0,1].grid(True, alpha=0.3)
        except:
            axes[0,1].text(0.5, 0.5, 'PACF Plot Error', ha='center', va='center', transform=axes[0,1].transAxes)

        # First difference ACF
        first_diff = ts.diff().dropna()
        if len(first_diff) > max_lags:
            try:
                plot_acf(first_diff, lags=max_lags, ax=axes[1,0], alpha=0.05)
                axes[1,0].set_title('ACF - First Difference')
                axes[1,0].grid(True, alpha=0.3)
            except:
                axes[1,0].text(0.5, 0.5, 'ACF Diff Error', ha='center', va='center', transform=axes[1,0].transAxes)

        # First difference PACF
        if len(first_diff) > max_lags:
            try:
                plot_pacf(first_diff, lags=max_lags, ax=axes[1,1], alpha=0.05)
                axes[1,1].set_title('PACF - First Difference')
                axes[1,1].grid(True, alpha=0.3)
            except:
                axes[1,1].text(0.5, 0.5, 'PACF Diff Error', ha='center', va='center', transform=axes[1,1].transAxes)

        plt.tight_layout()
        plt.show()

    def _residual_visualization(self, residual):
        """Residual diagnostics visualization"""
        max_lags = min(40, len(residual) // 4)

        fig, axes = plt.subplots(2, 2, figsize=(16, 8))
        fig.suptitle('MSTL Residual Diagnostics', fontsize=14, fontweight='bold')

        # Residual time series
        axes[0,0].plot(residual.index, residual.values, 'b-', alpha=0.7)
        axes[0,0].set_title('MSTL Residuals')
        axes[0,0].grid(True, alpha=0.3)

        # Residual distribution
        axes[0,1].hist(residual.values, bins=30, alpha=0.7, density=True)
        axes[0,1].set_title('Residual Distribution')
        axes[0,1].grid(True, alpha=0.3)

        # Residual ACF
        try:
            plot_acf(residual, lags=max_lags, ax=axes[1,0], alpha=0.05)
            axes[1,0].set_title('Residual ACF')
            axes[1,0].grid(True, alpha=0.3)
        except:
            axes[1,0].text(0.5, 0.5, 'ACF Error', ha='center', va='center', transform=axes[1,0].transAxes)

        # Residual PACF
        try:
            plot_pacf(residual, lags=max_lags, ax=axes[1,1], alpha=0.05)
            axes[1,1].set_title('Residual PACF')
            axes[1,1].grid(True, alpha=0.3)
        except:
            axes[1,1].text(0.5, 0.5, 'PACF Error', ha='center', va='center', transform=axes[1,1].transAxes)

        plt.tight_layout()
        plt.show()

# Initialize analyzer
analyzer = WeatherEDAAnalyzer(df_all, target_col='LÆ°á»£ng mÆ°a', date_col='NgÃ y')

# Run Pháº§n 3: Stationarity & Autocorrelation Diagnostics
# (CÃ³ thá»ƒ truyá»n mstl_results tá»« Pháº§n 2 náº¿u cÃ³)
diagnostics_results = analyzer.run_stationarity_autocorr_analysis()

# Access comprehensive results
if diagnostics_results.get('synthesis'):
    synthesis = diagnostics_results['synthesis']
    print(f"\nğŸ¯ Káº¾T QUáº¢ Tá»”NG Há»¢P:")
    print(f"   â€¢ TÃ¬nh tráº¡ng: {synthesis['stationarity_conclusion']}")
    print(f"   â€¢ Cáº§n sai phÃ¢n: {synthesis['differencing_needed']}")
    print(f"   â€¢ Lags quan trá»ng: {synthesis['significant_lags'][:5]}")
    print(f"   â€¢ Khuyáº¿n nghá»‹: {synthesis['model_recommendations']}")

"""## Extreme Even Analysis"""

# =============================================================================
# COMPONENT 2: EXTREME EVENTS ANALYZER
# =============================================================================

class ExtremeEventsAnalyzer(BaseAnalyzer):
    """
    Component 2: Extreme Events Analysis
    """

    def analyze(self):
        """Run comprehensive extreme events analysis"""
        print("\n" + "="*70)
        print("â›ˆï¸ COMPONENT 2: EXTREME EVENTS ANALYSIS")
        print("="*70)

        # Run extreme events analysis
        extreme_definition = self._define_extreme_events()
        seasonal_patterns = self._seasonal_extreme_patterns()

        # Combine results
        results = {
            'extreme_definition': extreme_definition,
            'seasonal_patterns': seasonal_patterns,
            'component_name': 'ExtremeEventsAnalyzer'
        }

        return results

    def _define_extreme_events(self):
        """2.1 Define extreme events and thresholds"""
        print("ğŸ” 2.1 Extreme Events Definition")
        print("-" * 50)

        # Define thresholds
        p95 = self.df[self.target_col].quantile(0.95)
        p99 = self.df[self.target_col].quantile(0.99)
        p99_9 = self.df[self.target_col].quantile(0.999)

        print(f"   ğŸ“Š Extreme Event Thresholds:")
        print(f"      - 95th percentile: {p95:.2f}mm")
        print(f"      - 99th percentile: {p99:.2f}mm")
        print(f"      - 99.9th percentile: {p99_9:.2f}mm")

        # Count extreme events
        extreme_95 = (self.df[self.target_col] > p95).sum()
        extreme_99 = (self.df[self.target_col] > p99).sum()
        extreme_99_9 = (self.df[self.target_col] > p99_9).sum()

        print(f"\n   ğŸ“ˆ Extreme Event Counts:")
        print(f"      - > 95th percentile: {extreme_95} events ({extreme_95/len(self.df)*100:.2f}%)")
        print(f"      - > 99th percentile: {extreme_99} events ({extreme_99/len(self.df)*100:.2f}%)")
        print(f"      - > 99.9th percentile: {extreme_99_9} events ({extreme_99_9/len(self.df)*100:.2f}%)")

        return {
            'thresholds': {'p95': p95, 'p99': p99, 'p99_9': p99_9},
            'extreme_counts': {'p95': extreme_95, 'p99': extreme_99, 'p99_9': extreme_99_9}
        }

    def _seasonal_extreme_patterns(self):
        """2.2 Seasonal patterns of extreme events"""
        print(f"\nğŸŒªï¸ 2.2 Seasonal Patterns of Extreme Events")
        print("-" * 50)

        # Use 95th percentile as extreme threshold
        p95 = self.df[self.target_col].quantile(0.95)
        extreme_by_month = self.df[self.df[self.target_col] > p95].groupby('Month').size()

        # Create visualization
        fig, axes = plt.subplots(2, 2, figsize=(16, 10))
        fig.suptitle('Extreme Events Analysis', fontsize=16, fontweight='bold')

        # Monthly extreme events
        axes[0,0].bar(extreme_by_month.index, extreme_by_month.values, color='red', alpha=0.7)
        axes[0,0].set_title(f'Extreme Events by Month (> {p95:.1f}mm)')
        axes[0,0].set_xlabel('Month')
        axes[0,0].set_ylabel('Number of Events')
        axes[0,0].grid(True, alpha=0.3)

        # Extreme events time series
        extreme_events = self.df[self.df[self.target_col] > p95]
        if len(extreme_events) > 0:
            axes[0,1].scatter(extreme_events[self.date_col], extreme_events[self.target_col],
                            color='red', alpha=0.7, s=30)
            axes[0,1].set_title('Extreme Events Over Time')
            axes[0,1].set_xlabel('Date')
            axes[0,1].set_ylabel('Precipitation (mm)')
            axes[0,1].grid(True, alpha=0.3)

        # Distribution of extreme events
        if len(extreme_events) > 0:
            axes[1,0].hist(extreme_events[self.target_col], bins=20, color='orange', alpha=0.7)
            axes[1,0].set_title('Distribution of Extreme Events')
            axes[1,0].set_xlabel('Precipitation (mm)')
            axes[1,0].set_ylabel('Frequency')
            axes[1,0].grid(True, alpha=0.3)

        # Return period analysis
        sorted_values = self.df[self.target_col].sort_values(ascending=False)
        return_periods = len(self.df) / (np.arange(1, len(sorted_values) + 1))
        axes[1,1].loglog(return_periods[:100], sorted_values.iloc[:100], 'bo-', markersize=4)
        axes[1,1].set_title('Return Period Analysis')
        axes[1,1].set_xlabel('Return Period (days)')
        axes[1,1].set_ylabel('Precipitation (mm)')
        axes[1,1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

        # Peak extreme month
        peak_month = extreme_by_month.idxmax() if len(extreme_by_month) > 0 else None
        if peak_month:
            print(f"   ğŸ“… Peak extreme events month: {peak_month}")

        return {
            'extreme_by_month': extreme_by_month,
            'extreme_events_data': extreme_events,
            'peak_month': peak_month,
            'return_periods': return_periods[:100].tolist(),
            'sorted_values': sorted_values.iloc[:100].tolist()
        }

# Initialize the analyzer
analyzer = WeatherEDAAnalyzer(df_all, target_col='LÆ°á»£ng mÆ°a', date_col='NgÃ y')

# Run Component 2: Extreme Events Analysis
extreme_results = analyzer.run_extreme_events_analysis()

# Access results
thresholds = extreme_results['extreme_definition']['thresholds']
extreme_counts = extreme_results['extreme_definition']['extreme_counts']
peak_month = extreme_results['seasonal_patterns']['peak_month']

print(f"P95 threshold: {thresholds['p95']:.2f}mm")
print(f"Extreme events (>P95): {extreme_counts['p95']}")
print(f"Peak extreme month: {peak_month}")

"""# Cross-Correlation & Multicollinearity Analysis"""

# =============================================================================
# ENHANCED PART B: COMPREHENSIVE CORRELATION ANALYSIS (CLEANED)
# =============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from scipy.spatial.distance import squareform
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from statsmodels.stats.outliers_influence import variance_inflation_factor
import networkx as nx
import warnings
warnings.filterwarnings('ignore')

# Set plotting style
plt.style.use('seaborn-v0_8')
sns.set_palette("RdYlBu_r")
plt.rcParams['figure.figsize'] = (15, 10)
plt.rcParams['font.size'] = 10

class CityLevelCorrelationAnalyzer:
    """
    Enhanced Comprehensive Correlation Analysis for City-Level Weather Data
    Unified: Static, Dynamic, Lagged, Multicollinearity & Network Analysis
    """

    def __init__(self, df, target_col='LÆ°á»£ng mÆ°a', date_col='NgÃ y'):
        """Initialize City-Level Correlation Analyzer"""
        self.df = df.copy()
        self.target_col = target_col
        self.date_col = date_col

        # Exclude non-predictive columns for city-level data
        exclude_cols = ['VÄ© Ä‘á»™', 'Kinh Ä‘á»™', 'NgÃ y', 'NhÃ³m']

        # Get numerical columns for analysis
        self.numerical_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()
        self.analysis_cols = [col for col in self.numerical_cols if col not in exclude_cols]

        # Separate predictors from target
        self.predictor_cols = [col for col in self.analysis_cols if col != self.target_col]

        print("ğŸ” CITY-LEVEL ADVANCED CORRELATION ANALYZER INITIALIZED")
        print("="*70)
        print(f"   ğŸ“Š Dataset Shape: {self.df.shape}")
        print(f"   ğŸ¯ Target Variable: {self.target_col}")
        print(f"   ğŸ“ Geographic Scope: Ho Chi Minh City (Single Location)")
        print(f"   ğŸ”¢ Total Features: {len(self.analysis_cols)}")
        print(f"   ğŸ“ˆ Predictor Features: {len(self.predictor_cols)}")

    def meteorological_correlation_matrix(self):
        """1. Specialized correlation matrix for meteorological variables"""
        print("\n" + "="*70)
        print("ğŸŒ¤ï¸ 1. METEOROLOGICAL CORRELATION MATRIX ANALYSIS")
        print("="*70)

        # Group features by meteorological categories
        feature_groups = {
            'Temperature': [col for col in self.analysis_cols if 'Nhiá»‡t Ä‘á»™' in col or 'Äiá»ƒm sÆ°Æ¡ng' in col or 'báº§u Æ°á»›t' in col],
            'Humidity': [col for col in self.analysis_cols if 'Äá»™ áº©m' in col],
            'Wind': [col for col in self.analysis_cols if 'giÃ³' in col or 'HÆ°á»›ng' in col or 'Tá»‘c Ä‘á»™' in col],
            'Pressure_Radiation': [col for col in self.analysis_cols if 'Ãp suáº¥t' in col or 'Bá»©c xáº¡' in col],
            'Precipitation': [self.target_col]
        }

        print(f"ğŸ“Š Meteorological Feature Groups:")
        for group, features in feature_groups.items():
            print(f"   - {group}: {len(features)} features")

        # Calculate correlations
        correlations = {}
        correlations['pearson'] = self.df[self.analysis_cols].corr(method='pearson')
        correlations['spearman'] = self.df[self.analysis_cols].corr(method='spearman')

        # Visualization
        fig, axes = plt.subplots(2, 2, figsize=(20, 16))
        fig.suptitle('Meteorological Cross-Correlation Analysis - Ho Chi Minh City',
                     fontsize=16, fontweight='bold')

        # Pearson correlation
        mask = np.triu(np.ones_like(correlations['pearson'], dtype=bool))
        sns.heatmap(correlations['pearson'], mask=mask, annot=True,
                   cmap='RdBu_r', center=0, square=True, linewidths=.5,
                   cbar_kws={"shrink": .8}, fmt='.2f', ax=axes[0,0])
        axes[0,0].set_title('Pearson Correlation (Linear Relationships)', fontweight='bold')

        # Spearman correlation
        sns.heatmap(correlations['spearman'], mask=mask, annot=True,
                   cmap='RdBu_r', center=0, square=True, linewidths=.5,
                   cbar_kws={"shrink": .8}, fmt='.2f', ax=axes[0,1])
        axes[0,1].set_title('Spearman Correlation (Monotonic Relationships)', fontweight='bold')

        # Target variable focus
        target_corr = correlations['pearson'][self.target_col].drop(self.target_col).sort_values(key=abs, ascending=False)
        bars = axes[1,0].barh(range(len(target_corr)), target_corr.values,
                             color=['red' if x > 0 else 'blue' for x in target_corr.values], alpha=0.7)
        axes[1,0].set_yticks(range(len(target_corr)))
        axes[1,0].set_yticklabels(target_corr.index, fontsize=9)
        axes[1,0].set_title(f'Correlations with {self.target_col}', fontweight='bold')
        axes[1,0].set_xlabel('Correlation Coefficient')
        axes[1,0].grid(True, alpha=0.3)

        # Nonlinearity detection
        diff_matrix = correlations['spearman'] - correlations['pearson']
        sns.heatmap(diff_matrix, mask=mask, annot=True,
                   cmap='RdYlGn', center=0, square=True, linewidths=.5,
                   cbar_kws={"shrink": .8}, fmt='.2f', ax=axes[1,1])
        axes[1,1].set_title('Nonlinearity Detection (Spearman - Pearson)', fontweight='bold')

        plt.tight_layout()
        plt.show()

        return correlations, feature_groups

    def temporal_correlation_dynamics(self):
        """
        2. UNIFIED Temporal Correlation Dynamics Analysis
        Combines: Seasonal, Rolling, Lagged Analysis
        """
        print("\n" + "="*70)
        print("ğŸ“… 2. ENHANCED TEMPORAL CORRELATION DYNAMICS")
        print("="*70)

        # Ensure datetime format
        if not pd.api.types.is_datetime64_any_dtype(self.df[self.date_col]):
            self.df[self.date_col] = pd.to_datetime(self.df[self.date_col])

        results = {}

        # 2.1 Seasonal Analysis
        print(f"ğŸŒ¤ï¸ 2.1 Seasonal Correlation Analysis")
        print("-" * 50)

        df_temp = self.df.copy()
        df_temp['Month'] = df_temp[self.date_col].dt.month
        df_temp['Season'] = df_temp['Month'].map({
            12: 'Dry', 1: 'Dry', 2: 'Dry',
            3: 'Pre-wet', 4: 'Pre-wet', 5: 'Pre-wet',
            6: 'Wet', 7: 'Wet', 8: 'Wet',
            9: 'Post-wet', 10: 'Post-wet', 11: 'Post-wet'
        })

        seasonal_correlations = {}
        seasons = ['Dry', 'Pre-wet', 'Wet', 'Post-wet']

        for season in seasons:
            season_data = df_temp[df_temp['Season'] == season]
            if len(season_data) > 30:
                seasonal_correlations[season] = season_data[self.analysis_cols].corr()[self.target_col].drop(self.target_col)

        if seasonal_correlations:
            correlation_df = pd.DataFrame(seasonal_correlations)

            plt.figure(figsize=(16, 10))

            # Seasonal correlation heatmap
            plt.subplot(2, 2, 1)
            sns.heatmap(correlation_df.T, annot=True, cmap='RdBu_r', center=0,
                       cbar_kws={"shrink": .8}, fmt='.2f')
            plt.title('Seasonal Correlation Patterns', fontweight='bold')

            # Top predictors seasonal changes
            plt.subplot(2, 2, 2)
            top_predictors = correlation_df.abs().max(axis=1).nlargest(5).index
            for predictor in top_predictors:
                plt.plot(seasons, [correlation_df.loc[predictor, season] for season in seasons],
                        'o-', linewidth=2, label=predictor[:15], alpha=0.8)
            plt.title('Top Predictors Seasonal Changes', fontweight='bold')
            plt.xlabel('Season')
            plt.ylabel('Correlation')
            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            plt.grid(True, alpha=0.3)
            plt.xticks(rotation=45)

            plt.tight_layout()
            plt.show()

        results['seasonal_correlations'] = seasonal_correlations

        # 2.2 Rolling Correlations (Simplified)
        print(f"\nğŸ“ˆ 2.2 Rolling Correlation Analysis")
        print("-" * 50)

        daily_agg = self.df.groupby(self.date_col)[self.analysis_cols].mean().reset_index()
        daily_agg = daily_agg.set_index(self.date_col).sort_index()
        daily_agg_clean = daily_agg.dropna()

        rolling_results = {}
        if len(daily_agg_clean) >= 60:
            static_corr = daily_agg_clean.corr()[self.target_col].sort_values(ascending=False)
            top_vars = static_corr.head(3).index.tolist()[1:3]  # Top 2 excluding target

            rolling_window = 30
            for var in top_vars:
                if var in daily_agg_clean.columns:
                    combined_df = daily_agg_clean[[self.target_col, var]].dropna()
                    rolling_corr = combined_df[self.target_col].rolling(window=rolling_window).corr(combined_df[var])
                    rolling_corr = rolling_corr.dropna()

                    if len(rolling_corr) > 0:
                        rolling_results[var] = {
                            'mean_correlation': rolling_corr.mean(),
                            'std_correlation': rolling_corr.std()
                        }

        results['rolling_correlations'] = rolling_results

        # 2.3 Lagged Correlations (Simplified)
        print(f"\nâ° 2.3 Lagged Correlation Analysis")
        print("-" * 50)

        lagged_results = {}
        if len(daily_agg_clean) >= 60:
            max_lags = 5
            static_corr = daily_agg_clean.corr()[self.target_col].sort_values(ascending=False)
            top_vars = static_corr.head(3).index.tolist()[1:3]  # Top 2 excluding target

            for var in top_vars:
                if var in daily_agg_clean.columns:
                    correlations = []
                    lags = range(-max_lags, max_lags + 1)

                    target_series = daily_agg_clean[self.target_col]
                    var_series = daily_agg_clean[var]

                    for lag in lags:
                        if lag == 0:
                            corr = target_series.corr(var_series)
                        elif lag > 0:
                            shifted_var = var_series.shift(lag)
                            valid_idx = target_series.index.intersection(shifted_var.dropna().index)
                            corr = target_series.loc[valid_idx].corr(shifted_var.loc[valid_idx])
                        else:
                            shifted_target = target_series.shift(-lag)
                            corr = shifted_target.corr(var_series)

                        correlations.append(corr if not np.isnan(corr) else 0)

                    # Find best lag
                    abs_correlations = [abs(c) for c in correlations]
                    best_lag_idx = np.argmax(abs_correlations)
                    best_lag = lags[best_lag_idx]
                    best_corr = correlations[best_lag_idx]

                    lagged_results[var] = {
                        'best_lag': best_lag,
                        'best_correlation': best_corr
                    }

        results['lagged_correlations'] = lagged_results

        return results

    def multicollinearity_advanced_analysis(self):
        """3. Advanced multicollinearity analysis"""
        print("\n" + "="*70)
        print("ğŸ” 3. ADVANCED MULTICOLLINEARITY ANALYSIS")
        print("="*70)

        X = self.df[self.predictor_cols].dropna()
        vif_data = []

        try:
            from statsmodels.tools.tools import add_constant
            X_with_const = add_constant(X)

            for i, col in enumerate(X.columns):
                vif_score = variance_inflation_factor(X_with_const.values, i+1)
                vif_data.append({
                    'Feature': col,
                    'VIF_Score': vif_score,
                    'Category': self._categorize_feature(col),
                    'Risk_Level': self._interpret_vif(vif_score)
                })

        except Exception as e:
            print(f"   âš ï¸ VIF calculation error: {e}")
            # Alternative: correlation-based detection
            corr_matrix = X.corr()
            for col in X.columns:
                max_corr = corr_matrix[col].drop(col).abs().max()
                vif_score = 1/(1-max_corr**2) if max_corr < 0.99 else 100
                vif_data.append({
                    'Feature': col,
                    'VIF_Score': vif_score,
                    'Category': self._categorize_feature(col),
                    'Risk_Level': self._interpret_vif(vif_score)
                })

        if vif_data:
            vif_df = pd.DataFrame(vif_data).sort_values('VIF_Score', ascending=False)
            print(f"ğŸ“Š Multicollinearity Analysis Results:")
            print(vif_df.head(10).to_string(index=False))

        return vif_df if vif_data else None

    def feature_interaction_network(self):
        """4. Network analysis focusing on feature interactions"""
        print("\n" + "="*70)
        print("ğŸ•¸ï¸ 4. FEATURE INTERACTION NETWORK")
        print("="*70)

        corr_matrix = self.df[self.analysis_cols].corr()
        G = nx.Graph()

        # Add nodes
        for feature in self.analysis_cols:
            category = self._categorize_feature(feature)
            G.add_node(feature, category=category, is_target=(feature == self.target_col))

        # Add edges for significant correlations
        correlation_threshold = 0.25
        for i, feature1 in enumerate(self.analysis_cols):
            for j, feature2 in enumerate(self.analysis_cols):
                if i < j:
                    corr_val = corr_matrix.loc[feature1, feature2]
                    if abs(corr_val) >= correlation_threshold:
                        G.add_edge(feature1, feature2, weight=abs(corr_val), correlation=corr_val)

        print(f"ğŸ•¸ï¸ Network: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")
        print(f"   Density: {nx.density(G):.3f}")

        return G, []

    def advanced_correlation_clustering_analysis(self):
        """
        6. Advanced Correlation Analysis using Clustering and PCA
        Utilizes: scipy.cluster, sklearn.preprocessing, sklearn.decomposition
        """
        print("\n" + "="*70)
        print("ğŸ”¬ 6. ADVANCED CORRELATION CLUSTERING & PCA ANALYSIS")
        print("="*70)

        # Prepare data
        X = self.df[self.predictor_cols].dropna()

        # 6.1 Standardize features
        print("ğŸ“Š 6.1 Feature Standardization")
        print("-" * 50)
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        X_scaled_df = pd.DataFrame(X_scaled, columns=self.predictor_cols, index=X.index)

        # 6.2 Hierarchical Clustering of Features
        print("ğŸŒ³ 6.2 Hierarchical Feature Clustering")
        print("-" * 50)

        # Calculate correlation distance matrix
        corr_matrix = X_scaled_df.corr()
        distance_matrix = 1 - np.abs(corr_matrix)
        condensed_distances = squareform(distance_matrix.values)

        # Perform hierarchical clustering
        linkage_matrix = linkage(condensed_distances, method='ward')

        # Get clusters
        n_clusters = min(5, len(self.predictor_cols)//3)  # Adaptive cluster number
        cluster_labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust')

        # Create feature clusters
        feature_clusters = {}
        for i, feature in enumerate(self.predictor_cols):
            cluster_id = cluster_labels[i]
            if cluster_id not in feature_clusters:
                feature_clusters[cluster_id] = []
            feature_clusters[cluster_id].append(feature)

        print(f"   ğŸ“Š Features grouped into {n_clusters} clusters:")
        for cluster_id, features in feature_clusters.items():
            print(f"   Cluster {cluster_id}: {len(features)} features")

        # 6.3 PCA Analysis
        print("\nğŸ¯ 6.3 Principal Component Analysis")
        print("-" * 50)

        pca = PCA()
        pca_result = pca.fit_transform(X_scaled)

        # Calculate correlation of PCs with target
        target_values = self.df.loc[X.index, self.target_col]
        pc_target_correlations = []

        for i in range(min(5, len(self.predictor_cols))):  # Top 5 PCs
            pc_corr = np.corrcoef(pca_result[:, i], target_values)[0, 1]
            pc_target_correlations.append({
                'PC': f'PC{i+1}',
                'Explained_Variance': pca.explained_variance_ratio_[i],
                'Target_Correlation': pc_corr
            })

        pc_df = pd.DataFrame(pc_target_correlations)
        print("   ğŸ“ˆ Principal Components vs Target:")
        print(pc_df.to_string(index=False, float_format='%.3f'))

        # 6.4 Visualization
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Advanced Correlation Analysis: Clustering & PCA', fontsize=16, fontweight='bold')

        # Dendrogram
        dendrogram(linkage_matrix, labels=self.predictor_cols, ax=axes[0,0],
                  orientation='top', leaf_rotation=90)
        axes[0,0].set_title('Feature Hierarchical Clustering', fontweight='bold')
        axes[0,0].tick_params(axis='x', labelsize=8)

        # Clustered correlation heatmap
        cluster_order = []
        for cluster_id in sorted(feature_clusters.keys()):
            cluster_order.extend(feature_clusters[cluster_id])

        reordered_corr = corr_matrix.loc[cluster_order, cluster_order]
        sns.heatmap(reordered_corr, ax=axes[0,1], cmap='RdBu_r', center=0,
                    square=True, linewidths=0.5, cbar_kws={"shrink": .8})
        axes[0,1].set_title('Clustered Correlation Matrix', fontweight='bold')
        axes[0,1].tick_params(axis='both', labelsize=8)

        # PCA explained variance
        cumsum_var = np.cumsum(pca.explained_variance_ratio_)
        axes[1,0].bar(range(1, len(cumsum_var[:10])+1), pca.explained_variance_ratio_[:10],
                      alpha=0.7, color='skyblue')
        axes[1,0].plot(range(1, len(cumsum_var[:10])+1), cumsum_var[:10],
                      'ro-', linewidth=2, markersize=6)
        axes[1,0].set_title('PCA Explained Variance', fontweight='bold')
        axes[1,0].set_xlabel('Principal Component')
        axes[1,0].set_ylabel('Explained Variance Ratio')
        axes[1,0].grid(True, alpha=0.3)

        # PC correlation with target
        pc_corrs = [abs(corr['Target_Correlation']) for corr in pc_target_correlations]
        axes[1,1].bar(range(1, len(pc_corrs)+1), pc_corrs, alpha=0.7, color='lightcoral')
        axes[1,1].set_title('Principal Components vs Target Correlation', fontweight='bold')
        axes[1,1].set_xlabel('Principal Component')
        axes[1,1].set_ylabel('|Correlation| with Target')
        axes[1,1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

        return {
            'feature_clusters': feature_clusters,
            'pca_results': pc_df,
            'explained_variance_ratio': pca.explained_variance_ratio_,
            'linkage_matrix': linkage_matrix
        }

    def generate_city_level_insights_report(self):
        """5. Generate comprehensive insights report"""
        print("\n" + "="*70)
        print("ğŸ“‹ 5. ENHANCED CITY-LEVEL CORRELATION INSIGHTS REPORT")
        print("="*70)

        # Run all analyses
        correlations, feature_groups = self.meteorological_correlation_matrix()
        temporal_results = self.temporal_correlation_dynamics()
        vif_results = self.multicollinearity_advanced_analysis()
        network, edges = self.feature_interaction_network()

        # NEW: Add advanced clustering analysis
        clustering_results = self.advanced_correlation_clustering_analysis()

        # Extract results
        seasonal_correlations = temporal_results['seasonal_correlations']
        rolling_correlations = temporal_results['rolling_correlations']
        lagged_correlations = temporal_results['lagged_correlations']

        # Executive summary
        print(f"\nğŸ¯ HO CHI MINH CITY WEATHER CORRELATION EXECUTIVE SUMMARY:")
        print("="*60)

        target_corr = correlations['pearson'][self.target_col].drop(self.target_col).abs().sort_values(ascending=False)
        strong_predictors = target_corr[target_corr > 0.3]
        moderate_predictors = target_corr[(target_corr > 0.2) & (target_corr <= 0.3)]

        print(f"ğŸŒ§ï¸ PRECIPITATION PREDICTION INSIGHTS:")
        print(f"   - Strong predictors (|r| > 0.3): {len(strong_predictors)}")
        print(f"   - Moderate predictors (0.2 < |r| â‰¤ 0.3): {len(moderate_predictors)}")

        if rolling_correlations:
            print(f"\nğŸ“ˆ DYNAMIC CORRELATION INSIGHTS:")
            for var, results in rolling_correlations.items():
                stability = "Stable" if results['std_correlation'] < 0.1 else "Variable"
                print(f"   - {var}: {stability} correlation")

        if lagged_correlations:
            print(f"\nâ° LAGGED CORRELATION INSIGHTS:")
            for var, results in lagged_correlations.items():
                print(f"   - {var}: best at {results['best_lag']} days (r={results['best_correlation']:.3f})")

        # NEW: Add clustering insights
        print(f"\nğŸ”¬ ADVANCED CLUSTERING INSIGHTS:")
        print(f"   - Feature clusters identified: {len(clustering_results['feature_clusters'])}")
        top_pc = clustering_results['pca_results'].iloc[0]
        print(f"   - Top PC explains {top_pc['Explained_Variance']:.1%} variance")
        print(f"   - Top PC correlation with target: {abs(top_pc['Target_Correlation']):.3f}")

        return {
            'correlations': correlations,
            'feature_groups': feature_groups,
            'seasonal_correlations': seasonal_correlations,
            'rolling_correlations': rolling_correlations,
            'lagged_correlations': lagged_correlations,
            'vif_results': vif_results,
            'network': network,
            'strong_predictors': strong_predictors,
            'moderate_predictors': moderate_predictors,
            'clustering_results': clustering_results  # NEW
        }

    def _categorize_feature(self, feature_name):
        """Helper function to categorize meteorological features"""
        if any(term in feature_name for term in ['Nhiá»‡t Ä‘á»™', 'Äiá»ƒm sÆ°Æ¡ng', 'báº§u Æ°á»›t']):
            return 'Temperature'
        elif 'Äá»™ áº©m' in feature_name:
            return 'Humidity'
        elif any(term in feature_name for term in ['giÃ³', 'HÆ°á»›ng', 'Tá»‘c Ä‘á»™']):
            return 'Wind'
        elif any(term in feature_name for term in ['Ãp suáº¥t', 'Bá»©c xáº¡']):
            return 'Pressure_Radiation'
        elif feature_name == self.target_col:
            return 'Precipitation'
        else:
            return 'Other'

    def _interpret_vif(self, vif_score):
        """Helper function to interpret VIF scores"""
        if vif_score < 5:
            return "Low"
        elif vif_score < 10:
            return "Moderate"
        else:
            return "High"

# =============================================================================
# EXECUTION
# =============================================================================

def run_city_level_correlation_analysis(df, target_col='LÆ°á»£ng mÆ°a', date_col='NgÃ y'):
    """Run comprehensive correlation analysis for city-level data"""
    print("ğŸš€ STARTING CLEANED CITY-LEVEL CORRELATION ANALYSIS")
    print("="*80)

    analyzer = CityLevelCorrelationAnalyzer(df, target_col, date_col)
    results = analyzer.generate_city_level_insights_report()

    print("\nâœ… CLEANED ANALYSIS COMPLETED")
    return results

# =============================================================================
# RUN CITY-LEVEL ANALYSIS
# =============================================================================

# Cháº¡y phÃ¢n tÃ­ch correlation cho dá»¯ liá»‡u TP.HCM
city_correlation_results = run_city_level_correlation_analysis(
    df_all,
    target_col='LÆ°á»£ng mÆ°a',
    date_col='NgÃ y'
)

print(f"\nğŸ‰ ENHANCED HO CHI MINH CITY CORRELATION ANALYSIS SUMMARY:")
print(f"   ğŸ’ª Strong Predictors: {len(city_correlation_results['strong_predictors'])}")
print(f"   ğŸ“Š Moderate Predictors: {len(city_correlation_results['moderate_predictors'])}")
print(f"   ğŸŒ¤ï¸ Meteorological Categories: {len(city_correlation_results['feature_groups'])}")
print(f"   ğŸ“ˆ Dynamic Correlations: {len(city_correlation_results['rolling_correlations'])}")  # NEW
print(f"   â° Lagged Correlations: {len(city_correlation_results['lagged_correlations'])}")    # NEW
print(f"   ğŸ•¸ï¸ Feature Network Density: {nx.density(city_correlation_results['network']):.3f}")

"""# Feature Engineering

## Feature Selection
"""

selected_features = [
    'Nhiá»‡t Ä‘á»™ tá»‘i Ä‘a 2m',      # T2M_MAX
    'Nhiá»‡t Ä‘á»™ tá»‘i thiá»ƒu 2m',    # T2M_MIN
    'Äá»™ áº©m tÆ°Æ¡ng Ä‘á»‘i 2m',       # RH2M
    'Äá»™ áº©m Ä‘áº¥t bá» máº·t',         # GWETTOP
    'HÆ°á»›ng giÃ³ 10m',            # WD10M
    'Ãp suáº¥t bá» máº·t',           # PS
    'Bá»©c xáº¡ sÃ³ng dÃ i xuá»‘ng'     # ALLSKY_SFC_LW_DWN
]

# Keep essential columns (date, target) + selected features
essential_columns = ['NgÃ y', 'LÆ°á»£ng mÆ°a']  # Date and target variable
final_columns = essential_columns + selected_features

# Filter dataset
df_selected = df_all[final_columns].copy()

print(f"ğŸ“Š Feature Selection completed:")
print(f"   Original shape: {df_all.shape}")
print(f"   Selected shape: {df_selected.shape}")
print(f"   Features selected: {len(selected_features)}")
print(f"   Selected features: {selected_features}")

"""## Temporal feature"""

import numpy as np
import pandas as pd

# Convert date column to datetime if not already
df_selected['NgÃ y'] = pd.to_datetime(df_selected['NgÃ y'])

# Extract basic time features
df_selected['Year'] = df_selected['NgÃ y'].dt.year
df_selected['Month'] = df_selected['NgÃ y'].dt.month
df_selected['DayofMonth'] = df_selected['NgÃ y'].dt.day
df_selected['DayofYear'] = df_selected['NgÃ y'].dt.dayofyear

# Create cyclical features for Month (1-12)
df_selected['Month_sin'] = np.sin(2 * np.pi * df_selected['Month'] / 12)
df_selected['Month_cos'] = np.cos(2 * np.pi * df_selected['Month'] / 12)

# Create cyclical features for Day of Year (1-365/366)
df_selected['DayOfYear_sin'] = np.sin(2 * np.pi * df_selected['DayofYear'] / 365.25)
df_selected['DayOfYear_cos'] = np.cos(2 * np.pi * df_selected['DayofYear'] / 365.25)

# Create Wet Season feature (May-November = months 5,6,7,8,9,10,11)
df_selected['Is_Wet_Season'] = df_selected['Month'].isin([5, 6, 7, 8, 9, 10, 11]).astype(int)

print("ğŸ•’ Time-based Feature Engineering completed:")
print(f"   Dataset shape: {df_selected.shape}")
print(f"   New time features added: 8")
print(f"   Wet season months (value=1): May-November")
print(f"   Dry season months (value=0): December-April")

# Display sample of new features
print("\nğŸ“Š Sample of new time features:")
print(df_selected[['NgÃ y', 'Year', 'Month', 'DayofMonth', 'DayofYear',
                   'Month_sin', 'Month_cos', 'DayOfYear_sin', 'DayOfYear_cos',
                   'Is_Wet_Season']].head(10))

# Check wet season distribution
print(f"\nğŸŒ§ï¸ Wet Season Distribution:")
print(df_selected['Is_Wet_Season'].value_counts())

"""## Lag and Rolling window Engineering

"""

# ==============================================================================
# LAG FEATURES & ROLLING WINDOW FEATURES
# ==============================================================================

# A. TARGET VARIABLE LAG & ROLLING FEATURES (HIGHEST PRIORITY)
print("ğŸ¯ Creating Target Variable Features (LÆ°á»£ng mÆ°a)...")

# Lag features for rainfall (1, 2, 3, 4, 5, 7 days)
for lag in [1, 2, 3, 4, 5, 7]:
    df_selected[f'Rainfall_lag_{lag}'] = df_selected['LÆ°á»£ng mÆ°a'].shift(lag)

# Rolling sum features (cumulative rainfall)
for window in [3, 7, 14, 30]:
    df_selected[f'Rainfall_sum_{window}d'] = df_selected['LÆ°á»£ng mÆ°a'].rolling(window=window).sum()

# Rolling statistical features
for window in [7, 14]:
    df_selected[f'Rainfall_mean_{window}d'] = df_selected['LÆ°á»£ng mÆ°a'].rolling(window=window).mean()
    df_selected[f'Rainfall_max_{window}d'] = df_selected['LÆ°á»£ng mÆ°a'].rolling(window=window).max()
    df_selected[f'Rainfall_std_{window}d'] = df_selected['LÆ°á»£ng mÆ°a'].rolling(window=window).std()

# B. IMPORTANT PREDICTOR VARIABLES LAG & ROLLING FEATURES
print("ğŸŒ¡ï¸ Creating Temperature Features...")

# Temperature features (T2M_MAX, T2M_MIN)
temp_vars = ['Nhiá»‡t Ä‘á»™ tá»‘i Ä‘a 2m', 'Nhiá»‡t Ä‘á»™ tá»‘i thiá»ƒu 2m']
for var in temp_vars:
    # Lag 1
    df_selected[f'{var}_lag_1'] = df_selected[var].shift(1)

    # Rolling statistics
    for window in [3, 7, 14]:
        df_selected[f'{var}_mean_{window}d'] = df_selected[var].rolling(window=window).mean()
        df_selected[f'{var}_max_{window}d'] = df_selected[var].rolling(window=window).max()
        df_selected[f'{var}_min_{window}d'] = df_selected[var].rolling(window=window).min()
        df_selected[f'{var}_std_{window}d'] = df_selected[var].rolling(window=window).std()

print("ğŸ’§ Creating Humidity Features...")

# Humidity features (RH2M, GWETTOP)
humidity_vars = ['Äá»™ áº©m tÆ°Æ¡ng Ä‘á»‘i 2m', 'Äá»™ áº©m Ä‘áº¥t bá» máº·t']
for var in humidity_vars:
    # Lag 1
    df_selected[f'{var}_lag_1'] = df_selected[var].shift(1)

    # Rolling statistics
    for window in [3, 7, 14]:
        df_selected[f'{var}_mean_{window}d'] = df_selected[var].rolling(window=window).mean()
        df_selected[f'{var}_std_{window}d'] = df_selected[var].rolling(window=window).std()

print("âœ… Lag & Rolling Window Features completed:")
print(f"   Final dataset shape: {df_selected.shape}")
print(f"   Target lag features: 6")
print(f"   Target rolling features: 10")
print(f"   Temperature features: 24")
print(f"   Humidity features: 12")
print(f"   Total new features: 52")

# Check for missing values (expected due to lag/rolling)
missing_count = df_selected.isnull().sum().sum()
print(f"   Missing values created: {missing_count} (due to lag/rolling operations)")

# ==============================================================================
# CHECK MISSING VALUES BY DATE
# ==============================================================================

# Find rows with any missing values
missing_rows = df_selected.isnull().any(axis=1)
missing_dates = df_selected[missing_rows]['NgÃ y']

print("ğŸ“… Dates with Missing Values:")
print(f"   Total dates with missing values: {len(missing_dates)}")
print(f"   Date range: {missing_dates.min()} to {missing_dates.max()}")

print("\nğŸ—“ï¸ First 20 dates with missing values:")
print(missing_dates.head(20).tolist())

if len(missing_dates) > 20:
    print(f"\n... and {len(missing_dates) - 20} more dates")

# Check which features have the most missing values
print(f"\nğŸ” Missing values by feature (top 10):")
missing_by_feature = df_selected.isnull().sum().sort_values(ascending=False)
print(missing_by_feature[missing_by_feature > 0].head(10))

# ==============================================================================
# DROP MISSING VALUES
# ==============================================================================

# Store original shape for comparison
original_shape = df_selected.shape

# Drop rows with any missing values
df_selected = df_selected.dropna()

# Reset index after dropping rows
df_selected = df_selected.reset_index(drop=True)

print("ğŸ—‘ï¸ Missing Values Removal completed:")
print(f"   Original shape: {original_shape}")
print(f"   Final shape: {df_selected.shape}")
print(f"   Rows dropped: {original_shape[0] - df_selected.shape[0]}")
print(f"   Remaining data: {df_selected.shape[0]} days")
print(f"   Missing values remaining: {df_selected.isnull().sum().sum()}")

# Check date range of final dataset
print(f"\nğŸ“… Final dataset date range:")
print(f"   Start date: {df_selected['NgÃ y'].min()}")
print(f"   End date: {df_selected['NgÃ y'].max()}")

"""## Multicolinear and cross-correlation checking for new feature"""

# ==============================================================================
# MULTICOLLINEARITY & CROSS-CORRELATION CHECKING (4 PARTS)
# ==============================================================================

# Define feature groups
print("ğŸ” Defining Feature Groups for Correlation Analysis...")

# 1. Original selected features (after feature selection)
original_features = ['NgÃ y', 'LÆ°á»£ng mÆ°a'] + [
    'Nhiá»‡t Ä‘á»™ tá»‘i Ä‘a 2m', 'Nhiá»‡t Ä‘á»™ tá»‘i thiá»ƒu 2m', 'Äá»™ áº©m tÆ°Æ¡ng Ä‘á»‘i 2m',
    'Äá»™ áº©m Ä‘áº¥t bá» máº·t', 'HÆ°á»›ng giÃ³ 10m', 'Ãp suáº¥t bá» máº·t', 'Bá»©c xáº¡ sÃ³ng dÃ i xuá»‘ng'
]

# 2. Temporal features
temporal_features = ['NgÃ y', 'LÆ°á»£ng mÆ°a'] + [
    'Year', 'Month', 'DayofMonth', 'DayofYear',
    'Month_sin', 'Month_cos', 'DayOfYear_sin', 'DayOfYear_cos', 'Is_Wet_Season'
]

# 3. Lag features
lag_columns = ['NgÃ y', 'LÆ°á»£ng mÆ°a'] + [col for col in df_selected.columns if '_lag_' in col]

# 4. Rolling window features
rolling_columns = ['NgÃ y', 'LÆ°á»£ng mÆ°a'] + [col for col in df_selected.columns
                   if any(pattern in col for pattern in ['_sum_', '_mean_', '_max_', '_std_'])
                   and col.endswith('d')]

print(f"ğŸ“Š Feature Groups Summary:")
print(f"   Original features: {len(original_features)-2}")
print(f"   Temporal features: {len(temporal_features)-2}")
print(f"   Lag features: {len(lag_columns)-2}")
print(f"   Rolling features: {len(rolling_columns)-2}")

# Part 1: Original Features Analysis
print("\n" + "="*50)
print("PART 1: ORIGINAL FEATURES CORRELATION ANALYSIS")
print("="*50)
df_original = df_selected[original_features].copy()
original_results = run_city_level_correlation_analysis(
    df_original,
    target_col='LÆ°á»£ng mÆ°a',
    date_col='NgÃ y'
)

# Part 2: Temporal Features Analysis
print("\n" + "="*50)
print("PART 2: TEMPORAL FEATURES CORRELATION ANALYSIS")
print("="*50)
df_temporal = df_selected[temporal_features].copy()
temporal_results = run_city_level_correlation_analysis(
    df_temporal,
    target_col='LÆ°á»£ng mÆ°a',
    date_col='NgÃ y'
)

# Part 3: Lag Features Analysis
print("\n" + "="*50)
print("PART 3: LAG FEATURES CORRELATION ANALYSIS")
print("="*50)
df_lag = df_selected[lag_columns].copy()
lag_results = run_city_level_correlation_analysis(
    df_lag,
    target_col='LÆ°á»£ng mÆ°a',
    date_col='NgÃ y'
)

# Part 4: Rolling Window Features Analysis
print("\n" + "="*50)
print("PART 4: ROLLING WINDOW FEATURES CORRELATION ANALYSIS")
print("="*50)
df_rolling = df_selected[rolling_columns].copy()
rolling_results = run_city_level_correlation_analysis(
    df_rolling,
    target_col='LÆ°á»£ng mÆ°a',
    date_col='NgÃ y'
)

print("\nâœ… All correlation analyses completed!")

"""# Modeling

## Tree-based model
"""

!pip install -q optuna

# ==============================================================================
# LIGHTGBM TWO-STAGE MODEL WITH OPTUNA OPTIMIZATION (FIXED)
# ==============================================================================

import lightgbm as lgb
import optuna
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import roc_auc_score, mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import pandas as pd

print("ğŸš€ LIGHTGBM TWO-STAGE MODEL EXPERIMENTATION (FIXED)")
print("="*60)

print(f"ğŸ“Š Data Setup:")
print(f"   Total features: {len(feature_cols)}")
print(f"   Total samples: {len(X)}")
print(f"   Rain days: {y_clf.sum()} ({y_clf.mean()*100:.1f}%)")
print(f"   No rain days: {(1-y_clf).sum()} ({(1-y_clf.mean())*100:.1f}%)")

# Fixed Optuna optimization functions
def objective_classification_fixed(trial):
    params = {
        'objective': 'binary',
        'metric': 'auc',
        'boosting_type': 'gbdt',
        'num_leaves': trial.suggest_int('num_leaves', 10, 100),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),
        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
        'verbosity': -1
    }

    auc_scores = []
    for train_idx, test_idx in tscv.split(X):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y_clf.iloc[train_idx], y_clf.iloc[test_idx]

        train_data = lgb.Dataset(X_train, label=y_train)
        model = lgb.train(params, train_data, num_boost_round=100)  # Removed verbose_eval

        y_pred = model.predict(X_test)
        auc = roc_auc_score(y_test, y_pred)
        auc_scores.append(auc)

    return np.mean(auc_scores)

def objective_regression_fixed(trial):
    params = {
        'objective': 'regression',
        'metric': 'mae',
        'boosting_type': 'gbdt',
        'num_leaves': trial.suggest_int('num_leaves', 10, 100),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),
        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
        'verbosity': -1
    }

    mae_scores = []
    for train_idx, test_idx in tscv.split(X):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y_reg.iloc[train_idx], y_reg.iloc[test_idx]

        rain_mask_train = y_clf.iloc[train_idx] == 1
        rain_mask_test = y_clf.iloc[test_idx] == 1

        if rain_mask_train.sum() > 0 and rain_mask_test.sum() > 0:
            X_train_rain = X_train[rain_mask_train]
            y_train_rain = y_train[rain_mask_train]
            X_test_rain = X_test[rain_mask_test]
            y_test_rain = y_test[rain_mask_test]

            train_data = lgb.Dataset(X_train_rain, label=y_train_rain)
            model = lgb.train(params, train_data, num_boost_round=100)  # Removed verbose_eval

            y_pred = model.predict(X_test_rain)
            mae = mean_absolute_error(y_test_rain, y_pred)
            mae_scores.append(mae)

    return np.mean(mae_scores) if mae_scores else float('inf')

# Optimize Classification Model
print("\nğŸ” Optimizing Classification Model...")
study_clf_fixed = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))
study_clf_fixed.optimize(objective_classification_fixed, n_trials=20)
best_params_clf_fixed = study_clf_fixed.best_params
print(f"   Best AUC: {study_clf_fixed.best_value:.4f}")

# Optimize Regression Model
print("\nğŸ” Optimizing Regression Model...")
study_reg_fixed = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))
study_reg_fixed.optimize(objective_regression_fixed, n_trials=20)
best_params_reg_fixed = study_reg_fixed.best_params
print(f"   Best MAE: {study_reg_fixed.best_value:.4f}")

# Final evaluation with best parameters
print("\nğŸ“Š Final Evaluation with Best Parameters:")
clf_results_fixed = {'AUC': []}
reg_results_fixed = {'MAE': [], 'RMSE': [], 'R2': []}

for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
    print(f"\nğŸ”„ Fold {fold + 1}/3")

    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
    y_train_clf, y_test_clf = y_clf.iloc[train_idx], y_clf.iloc[test_idx]
    y_train_reg, y_test_reg = y_reg.iloc[train_idx], y_reg.iloc[test_idx]

    # Classification Model
    best_params_clf_fixed.update({'objective': 'binary', 'metric': 'auc', 'verbosity': -1})
    train_data_clf = lgb.Dataset(X_train, label=y_train_clf)
    clf_model = lgb.train(best_params_clf_fixed, train_data_clf, num_boost_round=100)

    clf_pred = clf_model.predict(X_test)
    auc = roc_auc_score(y_test_clf, clf_pred)
    clf_results_fixed['AUC'].append(auc)

    # Regression Model
    rain_mask_train = y_train_clf == 1
    rain_mask_test = y_test_clf == 1

    if rain_mask_train.sum() > 0 and rain_mask_test.sum() > 0:
        X_train_rain = X_train[rain_mask_train]
        y_train_rain = y_train_reg[rain_mask_train]
        X_test_rain = X_test[rain_mask_test]
        y_test_rain = y_test_reg[rain_mask_test]

        best_params_reg_fixed.update({'objective': 'regression', 'metric': 'mae', 'verbosity': -1})
        train_data_reg = lgb.Dataset(X_train_rain, label=y_train_rain)
        reg_model = lgb.train(best_params_reg_fixed, train_data_reg, num_boost_round=100)

        reg_pred = reg_model.predict(X_test_rain)
        mae = mean_absolute_error(y_test_rain, reg_pred)
        rmse = np.sqrt(mean_squared_error(y_test_rain, reg_pred))
        r2 = r2_score(y_test_rain, reg_pred)

        reg_results_fixed['MAE'].append(mae)
        reg_results_fixed['RMSE'].append(rmse)
        reg_results_fixed['R2'].append(r2)

    print(f"   Classification AUC: {auc:.4f}")
    if rain_mask_test.sum() > 0:
        print(f"   Regression MAE: {mae:.4f}, RMSE: {rmse:.4f}, R2: {r2:.4f}")

# Summary Results
print("\n" + "="*60)
print("ğŸ“‹ LIGHTGBM FINAL RESULTS SUMMARY (FIXED)")
print("="*60)
print(f"Classification Model:")
print(f"   AUC: {np.mean(clf_results_fixed['AUC']):.4f} Â± {np.std(clf_results_fixed['AUC']):.4f}")

if reg_results_fixed['MAE']:
    print(f"\nRegression Model (on rainy days only):")
    print(f"   MAE:  {np.mean(reg_results_fixed['MAE']):.4f} Â± {np.std(reg_results_fixed['MAE']):.4f}")
    print(f"   RMSE: {np.mean(reg_results_fixed['RMSE']):.4f} Â± {np.std(reg_results_fixed['RMSE']):.4f}")
    print(f"   R2:   {np.mean(reg_results_fixed['R2']):.4f} Â± {np.std(reg_results_fixed['R2']):.4f}")

# ==============================================================================
# XGBOOST TWO-STAGE MODEL WITH OPTUNA OPTIMIZATION
# ==============================================================================

import xgboost as xgb
import optuna
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import roc_auc_score, mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import pandas as pd

print("ğŸš€ XGBOOST TWO-STAGE MODEL EXPERIMENTATION")
print("="*60)

# Use same data setup as LightGBM
print(f"ğŸ“Š Data Setup:")
print(f"   Total features: {len(feature_cols)}")
print(f"   Total samples: {len(X)}")
print(f"   Rain days: {y_clf.sum()} ({y_clf.mean()*100:.1f}%)")
print(f"   No rain days: {(1-y_clf).sum()} ({(1-y_clf.mean())*100:.1f}%)")

# Optuna optimization functions for XGBoost
def objective_classification_xgb(trial):
    params = {
        'objective': 'binary:logistic',
        'eval_metric': 'auc',
        'booster': 'gbtree',
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'n_estimators': trial.suggest_int('n_estimators', 50, 200),
        'subsample': trial.suggest_float('subsample', 0.4, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
        'verbosity': 0,
        'random_state': 42
    }

    auc_scores = []
    for train_idx, test_idx in tscv.split(X):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y_clf.iloc[train_idx], y_clf.iloc[test_idx]

        model = xgb.XGBClassifier(**params)
        model.fit(X_train, y_train, verbose=False)

        y_pred = model.predict_proba(X_test)[:, 1]
        auc = roc_auc_score(y_test, y_pred)
        auc_scores.append(auc)

    return np.mean(auc_scores)

def objective_regression_xgb(trial):
    params = {
        'objective': 'reg:squarederror',
        'eval_metric': 'mae',
        'booster': 'gbtree',
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'n_estimators': trial.suggest_int('n_estimators', 50, 200),
        'subsample': trial.suggest_float('subsample', 0.4, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
        'verbosity': 0,
        'random_state': 42
    }

    mae_scores = []
    for train_idx, test_idx in tscv.split(X):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y_reg.iloc[train_idx], y_reg.iloc[test_idx]

        # Only use rainy days for regression training
        rain_mask_train = y_clf.iloc[train_idx] == 1
        rain_mask_test = y_clf.iloc[test_idx] == 1

        if rain_mask_train.sum() > 0 and rain_mask_test.sum() > 0:
            X_train_rain = X_train[rain_mask_train]
            y_train_rain = y_train[rain_mask_train]
            X_test_rain = X_test[rain_mask_test]
            y_test_rain = y_test[rain_mask_test]

            model = xgb.XGBRegressor(**params)
            model.fit(X_train_rain, y_train_rain, verbose=False)

            y_pred = model.predict(X_test_rain)
            mae = mean_absolute_error(y_test_rain, y_pred)
            mae_scores.append(mae)

    return np.mean(mae_scores) if mae_scores else float('inf')

# Optimize Classification Model
print("\nğŸ” Optimizing XGBoost Classification Model...")
study_clf_xgb = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))
study_clf_xgb.optimize(objective_classification_xgb, n_trials=20)
best_params_clf_xgb = study_clf_xgb.best_params
print(f"   Best AUC: {study_clf_xgb.best_value:.4f}")

# Optimize Regression Model
print("\nğŸ” Optimizing XGBoost Regression Model...")
study_reg_xgb = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))
study_reg_xgb.optimize(objective_regression_xgb, n_trials=20)
best_params_reg_xgb = study_reg_xgb.best_params
print(f"   Best MAE: {study_reg_xgb.best_value:.4f}")

# Final evaluation with best parameters
print("\nğŸ“Š Final Evaluation with Best Parameters:")
clf_results_xgb = {'AUC': []}
reg_results_xgb = {'MAE': [], 'RMSE': [], 'R2': []}

for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
    print(f"\nğŸ”„ Fold {fold + 1}/3")

    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
    y_train_clf, y_test_clf = y_clf.iloc[train_idx], y_clf.iloc[test_idx]
    y_train_reg, y_test_reg = y_reg.iloc[train_idx], y_reg.iloc[test_idx]

    # Classification Model
    best_params_clf_xgb.update({
        'objective': 'binary:logistic',
        'eval_metric': 'auc',
        'verbosity': 0,
        'random_state': 42
    })
    clf_model_xgb = xgb.XGBClassifier(**best_params_clf_xgb)
    clf_model_xgb.fit(X_train, y_train_clf, verbose=False)

    # Classification predictions
    clf_pred_xgb = clf_model_xgb.predict_proba(X_test)[:, 1]
    auc = roc_auc_score(y_test_clf, clf_pred_xgb)
    clf_results_xgb['AUC'].append(auc)

    # Regression Model (only on rainy days)
    rain_mask_train = y_train_clf == 1
    rain_mask_test = y_test_clf == 1

    if rain_mask_train.sum() > 0 and rain_mask_test.sum() > 0:
        X_train_rain = X_train[rain_mask_train]
        y_train_rain = y_train_reg[rain_mask_train]
        X_test_rain = X_test[rain_mask_test]
        y_test_rain = y_test_reg[rain_mask_test]

        best_params_reg_xgb.update({
            'objective': 'reg:squarederror',
            'eval_metric': 'mae',
            'verbosity': 0,
            'random_state': 42
        })
        reg_model_xgb = xgb.XGBRegressor(**best_params_reg_xgb)
        reg_model_xgb.fit(X_train_rain, y_train_rain, verbose=False)

        # Regression predictions
        reg_pred_xgb = reg_model_xgb.predict(X_test_rain)
        mae = mean_absolute_error(y_test_rain, reg_pred_xgb)
        rmse = np.sqrt(mean_squared_error(y_test_rain, reg_pred_xgb))
        r2 = r2_score(y_test_rain, reg_pred_xgb)

        reg_results_xgb['MAE'].append(mae)
        reg_results_xgb['RMSE'].append(rmse)
        reg_results_xgb['R2'].append(r2)

    print(f"   Classification AUC: {auc:.4f}")
    if rain_mask_test.sum() > 0:
        print(f"   Regression MAE: {mae:.4f}, RMSE: {rmse:.4f}, R2: {r2:.4f}")

# Summary Results
print("\n" + "="*60)
print("ğŸ“‹ XGBOOST FINAL RESULTS SUMMARY")
print("="*60)
print(f"Classification Model:")
print(f"   AUC: {np.mean(clf_results_xgb['AUC']):.4f} Â± {np.std(clf_results_xgb['AUC']):.4f}")

if reg_results_xgb['MAE']:
    print(f"\nRegression Model (on rainy days only):")
    print(f"   MAE:  {np.mean(reg_results_xgb['MAE']):.4f} Â± {np.std(reg_results_xgb['MAE']):.4f}")
    print(f"   RMSE: {np.mean(reg_results_xgb['RMSE']):.4f} Â± {np.std(reg_results_xgb['RMSE']):.4f}")
    print(f"   R2:   {np.mean(reg_results_xgb['R2']):.4f} Â± {np.std(reg_results_xgb['R2']):.4f}")

"""## Times series model"""

# ==============================================================================
# STATIONARITY TESTING FOR EXOGENOUS FEATURES (ADF & KPSS)
# ==============================================================================

from statsmodels.tsa.stattools import adfuller, kpss
import warnings
warnings.filterwarnings('ignore')

# Define exogenous features (original selected features)
exogenous_features = [
    'Nhiá»‡t Ä‘á»™ tá»‘i Ä‘a 2m',      # T2M_MAX
    'Nhiá»‡t Ä‘á»™ tá»‘i thiá»ƒu 2m',    # T2M_MIN
    'Äá»™ áº©m tÆ°Æ¡ng Ä‘á»‘i 2m',       # RH2M
    'Äá»™ áº©m Ä‘áº¥t bá» máº·t',         # GWETTOP
    'HÆ°á»›ng giÃ³ 10m',            # WD10M
    'Ãp suáº¥t bá» máº·t',           # PS
    'Bá»©c xáº¡ sÃ³ng dÃ i xuá»‘ng'     # ALLSKY_SFC_LW_DWN
]

print("ğŸ“Š STATIONARITY TESTING FOR EXOGENOUS FEATURES")
print("="*60)

stationarity_results = {}

for feature in exogenous_features:
    print(f"\nğŸ” Testing: {feature}")

    # ADF Test
    adf_result = adfuller(df_selected[feature].dropna())
    adf_stationary = adf_result[1] < 0.05

    # KPSS Test
    kpss_result = kpss(df_selected[feature].dropna())
    kpss_stationary = kpss_result[1] > 0.05

    # Overall conclusion
    if adf_stationary and kpss_stationary:
        conclusion = "STATIONARY"
    elif not adf_stationary and not kpss_stationary:
        conclusion = "NON-STATIONARY"
    else:
        conclusion = "INCONCLUSIVE"

    # Store results
    stationarity_results[feature] = {
        'ADF_pvalue': adf_result[1],
        'ADF_stationary': adf_stationary,
        'KPSS_pvalue': kpss_result[1],
        'KPSS_stationary': kpss_stationary,
        'Conclusion': conclusion
    }

    print(f"   ADF p-value: {adf_result[1]:.6f} ({'Stationary' if adf_stationary else 'Non-stationary'})")
    print(f"   KPSS p-value: {kpss_result[1]:.6f} ({'Stationary' if kpss_stationary else 'Non-stationary'})")
    print(f"   â¤ Conclusion: {conclusion}")

# Summary
print("\n" + "="*60)
print("ğŸ“‹ STATIONARITY SUMMARY:")
stationary_count = sum(1 for r in stationarity_results.values() if r['Conclusion'] == 'STATIONARY')
non_stationary_count = sum(1 for r in stationarity_results.values() if r['Conclusion'] == 'NON-STATIONARY')
inconclusive_count = sum(1 for r in stationarity_results.values() if r['Conclusion'] == 'INCONCLUSIVE')

print(f"   Stationary features: {stationary_count}")
print(f"   Non-stationary features: {non_stationary_count}")
print(f"   Inconclusive features: {inconclusive_count}")

# Chá»‰ cáº§n import SARIMAX
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.stattools import adfuller
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import TimeSeriesSplit
import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

# Prepare data
print("ğŸš€ TIME SERIES MODELS EXPERIMENTATION")
print("="*60)

# Use original features for exogenous variables (7 features)
exog_features = [
    'Nhiá»‡t Ä‘á»™ tá»‘i Ä‘a 2m', 'Nhiá»‡t Ä‘á»™ tá»‘i thiá»ƒu 2m', 'Äá»™ áº©m tÆ°Æ¡ng Ä‘á»‘i 2m',
    'Äá»™ áº©m Ä‘áº¥t bá» máº·t', 'HÆ°á»›ng giÃ³ 10m', 'Ãp suáº¥t bá» máº·t', 'Bá»©c xáº¡ sÃ³ng dÃ i xuá»‘ng'
]

# Target variable
target = df_selected['LÆ°á»£ng mÆ°a']
exog_data_original = df_selected[exog_features]

# ==============================================================================
# >>>>>>>>>>>> START OF FIX 1: MAKE EXOGENOUS VARIABLES STATIONARY <<<<<<<<<<<<
# ==============================================================================
exog_data_stationary = pd.DataFrame(index=exog_data_original.index)

print("ğŸ©º Checking and transforming exogenous variables for stationarity...")
for col in exog_data_original.columns:
    # ADF test to check for unit root
    adf_pvalue = adfuller(exog_data_original[col].dropna())[1]
    if adf_pvalue >= 0.05:
        # If not stationary, apply first difference
        print(f"   - Column '{col}' is non-stationary (p={adf_pvalue:.3f}). Applying differencing.")
        exog_data_stationary[col] = exog_data_original[col].diff()
    else:
        # If stationary, use as is
        print(f"   - Column '{col}' is stationary (p={adf_pvalue:.3f}).")
        exog_data_stationary[col] = exog_data_original[col]

# Drop NaNs created by differencing
exog_data_stationary = exog_data_stationary.dropna()
# Align target variable with the new stationary exogenous data
target = target.loc[exog_data_stationary.index]
print("   âœ… All exogenous variables are now stationary.")
# ==============================================================================
# >>>>>>>>>>>>>>>>>>>>>>>>>>>> END OF FIX 1 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
# ==============================================================================


# Time Series Cross-Validation setup
tscv = TimeSeriesSplit(n_splits=5)
print(f"\nğŸ“Š Data Setup:")
print(f"   Total samples after alignment: {len(target)}")
print(f"   CV splits: 5")

# Results storage
results = {
    'ARIMA': {'MAE': [], 'RMSE': [], 'R2': []},
    'SARIMA': {'MAE': [], 'RMSE': [], 'R2': []},
    'ARIMAX': {'MAE': [], 'RMSE': [], 'R2': []},
    'SARIMAX': {'MAE': [], 'RMSE': [], 'R2': []}
}

# Cross-validation loop
for fold, (train_idx, test_idx) in enumerate(tscv.split(target)):
    print(f"\nğŸ”„ Fold {fold + 1}/5")

    # Split data
    y_train, y_test = target.iloc[train_idx], target.iloc[test_idx]
    # Use the stationary exogenous data
    X_train, X_test = exog_data_stationary.iloc[train_idx], exog_data_stationary.iloc[test_idx]

    # Define model orders
    order = (3, 0, 3)
    seasonal_order = (1, 1, 1, 7)

    # ==============================================================================
    # >>>>>>>>>>>>>>>>>>>>>>>>>>>> START OF FIX 2 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
    # ==============================================================================
    # Use SARIMAX for all models for consistency

    # 1. ARIMA
    try:
        model = SARIMAX(y_train, order=order).fit(disp=False)
        pred = model.forecast(steps=len(y_test))
        # ... (evaluation code is the same)
        mae = mean_absolute_error(y_test, pred); rmse = np.sqrt(mean_squared_error(y_test, pred)); r2 = r2_score(y_test, pred)
        results['ARIMA']['MAE'].append(mae); results['ARIMA']['RMSE'].append(rmse); results['ARIMA']['R2'].append(r2)
        print(f"   ARIMA: MAE={mae:.4f}, RMSE={rmse:.4f}, R2={r2:.4f}")
    except Exception as e:
        print(f"   ARIMA: Failed - {e}")

    # 2. SARIMA
    try:
        model = SARIMAX(y_train, order=order, seasonal_order=seasonal_order).fit(disp=False)
        pred = model.forecast(steps=len(y_test))
        # ... (evaluation code is the same)
        mae = mean_absolute_error(y_test, pred); rmse = np.sqrt(mean_squared_error(y_test, pred)); r2 = r2_score(y_test, pred)
        results['SARIMA']['MAE'].append(mae); results['SARIMA']['RMSE'].append(rmse); results['SARIMA']['R2'].append(r2)
        print(f"   SARIMA: MAE={mae:.4f}, RMSE={rmse:.4f}, R2={r2:.4f}")
    except Exception as e:
        print(f"   SARIMA: Failed - {e}")

    # 3. ARIMAX
    try:
        model = SARIMAX(y_train, exog=X_train, order=order).fit(disp=False)
        pred = model.forecast(steps=len(y_test), exog=X_test)
        # ... (evaluation code is the same)
        mae = mean_absolute_error(y_test, pred); rmse = np.sqrt(mean_squared_error(y_test, pred)); r2 = r2_score(y_test, pred)
        results['ARIMAX']['MAE'].append(mae); results['ARIMAX']['RMSE'].append(rmse); results['ARIMAX']['R2'].append(r2)
        print(f"   ARIMAX: MAE={mae:.4f}, RMSE={rmse:.4f}, R2={r2:.4f}")
    except Exception as e:
        print(f"   ARIMAX: Failed - {e}")

    # 4. SARIMAX
    try:
        model = SARIMAX(y_train, exog=X_train, order=order, seasonal_order=seasonal_order).fit(disp=False)
        pred = model.forecast(steps=len(y_test), exog=X_test)
        # ... (evaluation code is the same)
        mae = mean_absolute_error(y_test, pred); rmse = np.sqrt(mean_squared_error(y_test, pred)); r2 = r2_score(y_test, pred)
        results['SARIMAX']['MAE'].append(mae); results['SARIMAX']['RMSE'].append(rmse); results['SARIMAX']['R2'].append(r2)
        print(f"   SARIMAX: MAE={mae:.4f}, RMSE={rmse:.4f}, R2={r2:.4f}")
    except Exception as e:
        print(f"   SARIMAX: Failed - {e}")


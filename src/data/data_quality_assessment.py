# -*- coding: utf-8 -*-
"""
Data Quality Assessment Module for Weather Time Series Data
Purpose: Comprehensive data quality evaluation for precipitation prediction project
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class DataQualityAssessment:
    """
    Comprehensive Data Quality Assessment for Weather Data
    """

    def __init__(self, df, target_col='L∆∞·ª£ng m∆∞a', date_col='Ng√†y'):
        """
        Initialize Data Quality Assessment

        Args:
            df: DataFrame to assess
            target_col: Target variable column name
            date_col: Date column name
        """
        self.df = df.copy()
        self.target_col = target_col
        self.date_col = date_col
        self.n_rows = len(self.df)
        self.n_cols = len(self.df.columns)

        print("üîç DATA QUALITY ASSESSMENT INITIALIZED")
        print("="*60)
        print(f"üìä Dataset Overview:")
        print(f"   - Shape: {self.df.shape}")
        print(f"   - Target Variable: {self.target_col}")
        print(f"   - Date Column: {self.date_col}")
        print(f"   - Memory Usage: {self.df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

    def basic_info_analysis(self):
        """
        1. Ph√¢n t√≠ch th√¥ng tin c∆° b·∫£n v·ªÅ dataset
        """
        print("\n" + "="*60)
        print("üìã 1. BASIC DATASET INFORMATION")
        print("="*60)

        # Dataset shape and size
        print(f"üìè K√≠ch th∆∞·ªõc Dataset:")
        print(f"   - S·ªë d√≤ng (observations): {self.n_rows:,}")
        print(f"   - S·ªë c·ªôt (variables): {self.n_cols}")
        print(f"   - T·ªïng √¥ d·ªØ li·ªáu: {self.n_rows * self.n_cols:,}")

        # Data types analysis
        print(f"\nüìä Ph√¢n t√≠ch ki·ªÉu d·ªØ li·ªáu:")
        dtype_counts = self.df.dtypes.value_counts()
        for dtype, count in dtype_counts.items():
            print(f"   - {dtype}: {count} columns ({count/self.n_cols*100:.1f}%)")

        # Column categories
        self.numerical_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()
        self.categorical_cols = self.df.select_dtypes(include=['object']).columns.tolist()
        self.datetime_cols = self.df.select_dtypes(include=['datetime64']).columns.tolist()

        print(f"\nüìÇ Ph√¢n lo·∫°i c·ªôt:")
        print(f"   - Numerical columns: {len(self.numerical_cols)}")
        print(f"   - Categorical columns: {len(self.categorical_cols)}")
        print(f"   - Datetime columns: {len(self.datetime_cols)}")

        return {
            'shape': self.df.shape,
            'memory_usage_mb': self.df.memory_usage(deep=True).sum() / 1024**2,
            'dtype_counts': dtype_counts,
            'column_categories': {
                'numerical': self.numerical_cols,
                'categorical': self.categorical_cols,
                'datetime': self.datetime_cols
            }
        }

    def missing_values_analysis(self):
        """
        2. Ph√¢n t√≠ch chi ti·∫øt v·ªÅ missing values
        """
        print("\n" + "="*60)
        print("‚ùì 2. MISSING VALUES ANALYSIS")
        print("="*60)

        # Calculate missing values for each column
        missing_data = []
        for col in self.df.columns:
            missing_count = self.df[col].isnull().sum()
            missing_percentage = (missing_count / self.n_rows) * 100
            non_null_count = self.df[col].notna().sum()
            unique_count = self.df[col].nunique()

            missing_data.append({
                'Column': col,
                'Missing_Count': missing_count,
                'Missing_Percentage': missing_percentage,
                'Non_Null_Count': non_null_count,
                'Unique_Values': unique_count,
                'Data_Type': str(self.df[col].dtype)
            })

        missing_df = pd.DataFrame(missing_data)
        missing_df = missing_df.sort_values('Missing_Percentage', ascending=False)

        # Overall missing data statistics
        total_missing = self.df.isnull().sum().sum()
        total_cells = self.n_rows * self.n_cols
        overall_missing_pct = (total_missing / total_cells) * 100

        print(f"üìä T·ªïng quan Missing Data:")
        print(f"   - T·ªïng s·ªë √¥ thi·∫øu d·ªØ li·ªáu: {total_missing:,}")
        print(f"   - T·ª∑ l·ªá thi·∫øu d·ªØ li·ªáu t·ªïng th·ªÉ: {overall_missing_pct:.2f}%")

        # Columns with missing data
        cols_with_missing = missing_df[missing_df['Missing_Count'] > 0]

        # Initialize empty DataFrames to avoid UnboundLocalError
        high_missing = pd.DataFrame()
        medium_missing = pd.DataFrame()
        low_missing = pd.DataFrame()

        if len(cols_with_missing) > 0:
            print(f"\nüö® C√°c c·ªôt c√≥ d·ªØ li·ªáu thi·∫øu ({len(cols_with_missing)} c·ªôt):")
            print(cols_with_missing[['Column', 'Missing_Count', 'Missing_Percentage', 'Data_Type']].to_string(index=False))

            # Categorize missing data severity
            high_missing = cols_with_missing[cols_with_missing['Missing_Percentage'] > 50]
            medium_missing = cols_with_missing[(cols_with_missing['Missing_Percentage'] > 10) &
                                             (cols_with_missing['Missing_Percentage'] <= 50)]
            low_missing = cols_with_missing[cols_with_missing['Missing_Percentage'] <= 10]

            print(f"\nüìã Ph√¢n lo·∫°i m·ª©c ƒë·ªô thi·∫øu d·ªØ li·ªáu:")
            print(f"   - Thi·∫øu nghi√™m tr·ªçng (>50%): {len(high_missing)} c·ªôt")
            print(f"   - Thi·∫øu trung b√¨nh (10-50%): {len(medium_missing)} c·ªôt")
            print(f"   - Thi·∫øu √≠t (<10%): {len(low_missing)} c·ªôt")

            if len(high_missing) > 0:
                print(f"   ‚ö†Ô∏è C·ªôt thi·∫øu nghi√™m tr·ªçng: {high_missing['Column'].tolist()}")
        else:
            print("‚úÖ Kh√¥ng c√≥ c·ªôt n√†o thi·∫øu d·ªØ li·ªáu!")

        # Check target variable missing data
        if self.target_col in missing_df['Column'].values:
            target_missing = missing_df[missing_df['Column'] == self.target_col]['Missing_Percentage'].iloc[0]
            if target_missing > 0:
                print(f"\nüéØ Bi·∫øn m·ª•c ti√™u '{self.target_col}' thi·∫øu {target_missing:.2f}% d·ªØ li·ªáu")
            else:
                print(f"\n‚úÖ Bi·∫øn m·ª•c ti√™u '{self.target_col}' kh√¥ng thi·∫øu d·ªØ li·ªáu")

        return missing_df, {
            'total_missing': total_missing,
            'overall_missing_pct': overall_missing_pct,
            'columns_with_missing': len(cols_with_missing),
            'high_missing_cols': high_missing['Column'].tolist() if len(high_missing) > 0 else [],
            'medium_missing_cols': medium_missing['Column'].tolist() if len(medium_missing) > 0 else [],
            'low_missing_cols': low_missing['Column'].tolist() if len(low_missing) > 0 else []
        }

    def duplicate_analysis(self):
        """
        3. Ph√¢n t√≠ch d·ªØ li·ªáu tr√πng l·∫∑p
        """
        print("\n" + "="*60)
        print("üîÑ 3. DUPLICATE DATA ANALYSIS")
        print("="*60)

        # Full row duplicates
        full_duplicates = self.df.duplicated().sum()
        full_duplicate_pct = (full_duplicates / self.n_rows) * 100

        print(f"üìä D√≤ng tr√πng l·∫∑p ho√†n to√†n:")
        print(f"   - S·ªë d√≤ng tr√πng l·∫∑p: {full_duplicates:,}")
        print(f"   - T·ª∑ l·ªá tr√πng l·∫∑p: {full_duplicate_pct:.2f}%")

        # Check for duplicates excluding certain columns (like ID columns)
        key_columns = [col for col in self.df.columns
                      if col not in ['Vƒ© ƒë·ªô', 'Kinh ƒë·ªô']]  # Exclude coordinate columns

        key_duplicates = 0
        if len(key_columns) < len(self.df.columns):
            key_duplicates = self.df.duplicated(subset=key_columns).sum()
            key_duplicate_pct = (key_duplicates / self.n_rows) * 100

            print(f"\nüìä D√≤ng tr√πng l·∫∑p (lo·∫°i tr·ª´ t·ªça ƒë·ªô):")
            print(f"   - S·ªë d√≤ng tr√πng l·∫∑p: {key_duplicates:,}")
            print(f"   - T·ª∑ l·ªá tr√πng l·∫∑p: {key_duplicate_pct:.2f}%")

        # Check for date + location duplicates (if applicable)
        date_location_duplicates = 0
        if self.date_col in self.df.columns and 'Qu·∫≠n Huy·ªán' in self.df.columns:
            date_location_duplicates = self.df.duplicated(subset=[self.date_col, 'Qu·∫≠n Huy·ªán']).sum()
            date_location_dup_pct = (date_location_duplicates / self.n_rows) * 100

            print(f"\nüìä Tr√πng l·∫∑p ng√†y + ƒë·ªãa ƒëi·ªÉm:")
            print(f"   - S·ªë d√≤ng tr√πng l·∫∑p: {date_location_duplicates:,}")
            print(f"   - T·ª∑ l·ªá tr√πng l·∫∑p: {date_location_dup_pct:.2f}%")

            if date_location_duplicates > 0:
                print("   ‚ö†Ô∏è C√≥ th·ªÉ c√≥ d·ªØ li·ªáu ƒëo ƒë·∫°c tr√πng l·∫∑p cho c√πng ng√†y v√† ƒë·ªãa ƒëi·ªÉm!")

        # Check for potential duplicate patterns in target variable
        if self.target_col in self.df.columns:
            target_value_counts = self.df[self.target_col].value_counts()
            most_common_value = target_value_counts.iloc[0]
            most_common_pct = (most_common_value / self.n_rows) * 100

            print(f"\nüéØ Ph√¢n t√≠ch gi√° tr·ªã tr√πng l·∫∑p c·ªßa bi·∫øn m·ª•c ti√™u:")
            print(f"   - Gi√° tr·ªã ph·ªï bi·∫øn nh·∫•t: {target_value_counts.index[0]}")
            print(f"   - S·ªë l·∫ßn xu·∫•t hi·ªán: {most_common_value:,} ({most_common_pct:.2f}%)")

            # Check for excessive zeros in precipitation data
            if target_value_counts.index[0] == 0:
                zero_pct = (target_value_counts.iloc[0] / self.n_rows) * 100
                print(f"   üìä T·ª∑ l·ªá ng√†y kh√¥ng m∆∞a: {zero_pct:.2f}%")
                if zero_pct > 70:
                    print("   ‚ö†Ô∏è T·ª∑ l·ªá ng√†y kh√¥ng m∆∞a r·∫•t cao - c·∫ßn xem x√©t!")

        return {
            'full_duplicates': full_duplicates,
            'full_duplicate_pct': full_duplicate_pct,
            'key_duplicates': key_duplicates,
            'date_location_duplicates': date_location_duplicates
        }


    def data_consistency_check(self):
        """
        5. Ki·ªÉm tra t√≠nh nh·∫•t qu√°n c·ªßa d·ªØ li·ªáu
        """
        print("\n" + "="*60)
        print("üîç 4. DATA CONSISTENCY CHECK")
        print("="*60)

        consistency_issues = []

        # Check for negative values where they shouldn't exist
        weather_vars_positive = [
            'L∆∞·ª£ng m∆∞a', 'ƒê·ªô ·∫©m t∆∞∆°ng ƒë·ªëi 2m', 'T·ªëc ƒë·ªô gi√≥ 10m', 'T·ªëc ƒë·ªô gi√≥ 2m',
            'T·ªëc ƒë·ªô gi√≥ t·ªëi ƒëa 10m', 'T·ªëc ƒë·ªô gi√≥ t·ªëi ƒëa 2m'
        ]

        print("üîç Ki·ªÉm tra gi√° tr·ªã √¢m cho c√°c bi·∫øn ph·∫£i d∆∞∆°ng:")
        for col in weather_vars_positive:
            if col in self.df.columns:
                negative_count = (self.df[col] < 0).sum()
                if negative_count > 0:
                    negative_pct = (negative_count / len(self.df)) * 100
                    print(f"   ‚ö†Ô∏è {col}: {negative_count} gi√° tr·ªã √¢m ({negative_pct:.2f}%)")
                    consistency_issues.append(f"{col} has {negative_count} negative values")
                else:
                    print(f"   ‚úÖ {col}: Kh√¥ng c√≥ gi√° tr·ªã √¢m")

        # Check humidity ranges (should be 0-100%)
        humidity_cols = ['ƒê·ªô ·∫©m t∆∞∆°ng ƒë·ªëi 2m']
        print(f"\nüîç Ki·ªÉm tra ph·∫°m vi ƒë·ªô ·∫©m (0-100%):")
        for col in humidity_cols:
            if col in self.df.columns:
                data = self.df[col].dropna()
                out_of_range = data[(data < 0) | (data > 100)]
                if len(out_of_range) > 0:
                    print(f"   ‚ö†Ô∏è {col}: {len(out_of_range)} gi√° tr·ªã ngo√†i ph·∫°m vi [0,100]")
                    print(f"      - Min: {data.min():.2f}, Max: {data.max():.2f}")
                    consistency_issues.append(f"{col} has values outside [0,100] range")
                else:
                    print(f"   ‚úÖ {col}: T·∫•t c·∫£ gi√° tr·ªã trong ph·∫°m vi [0,100]")

        # Check wind direction ranges (should be 0-360 degrees)
        wind_direction_cols = ['H∆∞·ªõng gi√≥ 10m', 'H∆∞·ªõng gi√≥ 2m']
        print(f"\nüîç Ki·ªÉm tra ph·∫°m vi h∆∞·ªõng gi√≥ (0-360 ƒë·ªô):")
        for col in wind_direction_cols:
            if col in self.df.columns:
                data = self.df[col].dropna()
                out_of_range = data[(data < 0) | (data > 360)]
                if len(out_of_range) > 0:
                    print(f"   ‚ö†Ô∏è {col}: {len(out_of_range)} gi√° tr·ªã ngo√†i ph·∫°m vi [0,360]")
                    print(f"      - Min: {data.min():.2f}, Max: {data.max():.2f}")
                    consistency_issues.append(f"{col} has values outside [0,360] range")
                else:
                    print(f"   ‚úÖ {col}: T·∫•t c·∫£ gi√° tr·ªã trong ph·∫°m vi [0,360]")

        # Check temperature consistency (min <= avg <= max)
        temp_cols = {
            'min': 'Nhi·ªát ƒë·ªô t·ªëi thi·ªÉu 2m',
            'avg': 'Nhi·ªát ƒë·ªô 2m',
            'max': 'Nhi·ªát ƒë·ªô t·ªëi ƒëa 2m'
        }

        if all(col in self.df.columns for col in temp_cols.values()):
            print(f"\nüîç Ki·ªÉm tra t√≠nh nh·∫•t qu√°n nhi·ªát ƒë·ªô (min ‚â§ avg ‚â§ max):")
            temp_min = self.df[temp_cols['min']]
            temp_avg = self.df[temp_cols['avg']]
            temp_max = self.df[temp_cols['max']]

            # Check min <= avg
            min_avg_violations = (temp_min > temp_avg).sum()
            # Check avg <= max
            avg_max_violations = (temp_avg > temp_max).sum()
            # Check min <= max
            min_max_violations = (temp_min > temp_max).sum()

            total_violations = min_avg_violations + avg_max_violations + min_max_violations

            if total_violations > 0:
                print(f"   ‚ö†Ô∏è Ph√°t hi·ªán {total_violations} vi ph·∫°m t√≠nh nh·∫•t qu√°n nhi·ªát ƒë·ªô:")
                if min_avg_violations > 0:
                    print(f"      - T_min > T_avg: {min_avg_violations} tr∆∞·ªùng h·ª£p")
                if avg_max_violations > 0:
                    print(f"      - T_avg > T_max: {avg_max_violations} tr∆∞·ªùng h·ª£p")
                if min_max_violations > 0:
                    print(f"      - T_min > T_max: {min_max_violations} tr∆∞·ªùng h·ª£p")
                consistency_issues.append(f"Temperature consistency violations: {total_violations}")
            else:
                print(f"   ‚úÖ T·∫•t c·∫£ d·ªØ li·ªáu nhi·ªát ƒë·ªô nh·∫•t qu√°n")

        # Check for extremely unrealistic values
        print(f"\nüîç Ki·ªÉm tra gi√° tr·ªã c·ª±c ƒëoan kh√¥ng th·ª±c t·∫ø:")

        # Precipitation > 500mm/day (extremely rare)
        if self.target_col in self.df.columns:
            extreme_rain = (self.df[self.target_col] > 500).sum()
            if extreme_rain > 0:
                print(f"   ‚ö†Ô∏è {self.target_col}: {extreme_rain} ng√†y c√≥ m∆∞a >500mm (c·ª±c ƒëoan)")
                max_rain = self.df[self.target_col].max()
                print(f"      - L∆∞·ª£ng m∆∞a t·ªëi ƒëa: {max_rain:.2f}mm")
                consistency_issues.append(f"Extreme precipitation: {extreme_rain} days >500mm")
            else:
                print(f"   ‚úÖ {self.target_col}: Kh√¥ng c√≥ gi√° tr·ªã c·ª±c ƒëoan")

        # Temperature extremes
        if 'Nhi·ªát ƒë·ªô 2m' in self.df.columns:
            temp_data = self.df['Nhi·ªát ƒë·ªô 2m'].dropna()
            extreme_hot = (temp_data > 45).sum()  # >45¬∞C
            extreme_cold = (temp_data < 10).sum()  # <10¬∞C for tropical climate

            if extreme_hot > 0 or extreme_cold > 0:
                print(f"   ‚ö†Ô∏è Nhi·ªát ƒë·ªô c·ª±c ƒëoan:")
                if extreme_hot > 0:
                    print(f"      - Qu√° n√≥ng (>45¬∞C): {extreme_hot} ng√†y")
                if extreme_cold > 0:
                    print(f"      - Qu√° l·∫°nh (<10¬∞C): {extreme_cold} ng√†y")
                consistency_issues.append(f"Extreme temperatures: {extreme_hot} hot, {extreme_cold} cold days")
            else:
                print(f"   ‚úÖ Nhi·ªát ƒë·ªô: Kh√¥ng c√≥ gi√° tr·ªã c·ª±c ƒëoan")

        return {
            'consistency_issues': consistency_issues,
            'total_issues': len(consistency_issues)
        }

    def temporal_quality_check(self):
        """
        5. Ki·ªÉm tra ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu th·ªùi gian
        """
        print("\n" + "="*60)
        print("üìÖ 5. TEMPORAL DATA QUALITY CHECK")
        print("="*60)

        if self.date_col not in self.df.columns:
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y c·ªôt ng√†y '{self.date_col}'")
            return {
                'invalid_dates': 0,
                'date_range_days': 0,
                'missing_dates_count': 0,
                'duplicate_dates': 0
            }

        # Ensure datetime format
        df_temp = self.df.copy()
        if not pd.api.types.is_datetime64_any_dtype(df_temp[self.date_col]):
            print(f"üîÑ Chuy·ªÉn ƒë·ªïi c·ªôt '{self.date_col}' sang datetime...")
            df_temp[self.date_col] = pd.to_datetime(df_temp[self.date_col], errors='coerce')

        # Check for invalid dates
        invalid_dates = df_temp[self.date_col].isnull().sum()
        if invalid_dates > 0:
            print(f"‚ö†Ô∏è Ph√°t hi·ªán {invalid_dates} ng√†y kh√¥ng h·ª£p l·ªá")
        else:
            print(f"‚úÖ T·∫•t c·∫£ ng√†y ƒë·ªÅu h·ª£p l·ªá")

        # Date range analysis
        valid_dates = df_temp[self.date_col].dropna()
        missing_dates_count = 0
        duplicate_dates = 0
        date_range_days = 0

        if len(valid_dates) > 0:
            min_date = valid_dates.min()
            max_date = valid_dates.max()
            date_range_days = (max_date - min_date).days

            print(f"\nüìä Ph·∫°m vi th·ªùi gian:")
            print(f"   - Ng√†y b·∫Øt ƒë·∫ßu: {min_date.strftime('%Y-%m-%d')}")
            print(f"   - Ng√†y k·∫øt th√∫c: {max_date.strftime('%Y-%m-%d')}")
            print(f"   - T·ªïng kho·∫£ng: {date_range_days:,} ng√†y ({date_range_days/365.25:.1f} nƒÉm)")

            # Check for missing dates in sequence
            expected_dates = pd.date_range(start=min_date, end=max_date, freq='D')
            actual_dates = set(valid_dates.dt.date)
            expected_dates_set = set(expected_dates.date)
            missing_dates = expected_dates_set - actual_dates
            missing_dates_count = len(missing_dates)

            if missing_dates:
                print(f"\n‚ö†Ô∏è Thi·∫øu {len(missing_dates)} ng√†y trong chu·ªói th·ªùi gian:")
                if len(missing_dates) <= 10:
                    for date in sorted(missing_dates)[:10]:
                        print(f"   - {date}")
                else:
                    print(f"   - Hi·ªÉn th·ªã 10 ng√†y ƒë·∫ßu: {sorted(missing_dates)[:10]}")
            else:
                print(f"\n‚úÖ Chu·ªói th·ªùi gian li√™n t·ª•c, kh√¥ng thi·∫øu ng√†y n√†o")

            # Check for duplicate dates
            duplicate_dates = valid_dates[valid_dates.duplicated()].nunique()
            if duplicate_dates > 0:
                print(f"\n‚ö†Ô∏è Ph√°t hi·ªán {duplicate_dates} ng√†y b·ªã tr√πng l·∫∑p")
            else:
                print(f"\n‚úÖ Kh√¥ng c√≥ ng√†y tr√πng l·∫∑p")

        return {
            'invalid_dates': invalid_dates,
            'date_range_days': date_range_days,
            'missing_dates_count': missing_dates_count,
            'duplicate_dates': duplicate_dates
        }

    def generate_quality_report(self):
        # Run all assessments
        basic_info = self.basic_info_analysis()
        missing_df, missing_summary = self.missing_values_analysis()
        duplicate_summary = self.duplicate_analysis()
        consistency_summary = self.data_consistency_check()
        temporal_summary = self.temporal_quality_check()

        # Overall quality score calculation
        quality_score = 100.0

        # Deduct points for issues
        if missing_summary['overall_missing_pct'] > 0:
            quality_score -= min(missing_summary['overall_missing_pct'] * 2, 30)

        if duplicate_summary['full_duplicate_pct'] > 0:
            quality_score -= min(duplicate_summary['full_duplicate_pct'] * 3, 20)

        if consistency_summary['total_issues'] > 0:
            quality_score -= min(consistency_summary['total_issues'] * 5, 25)

        if temporal_summary and temporal_summary['invalid_dates'] > 0:
            invalid_date_pct = (temporal_summary['invalid_dates'] / self.n_rows) * 100
            quality_score -= min(invalid_date_pct * 2, 15)

        quality_score = max(quality_score, 0)  # Ensure non-negative

        print(f"\nüèÜ OVERALL DATA QUALITY SCORE: {quality_score:.1f}/100")

        # Quality interpretation
        if quality_score >= 90:
            quality_level = "üü¢ EXCELLENT"
            recommendation = "D·ªØ li·ªáu c√≥ ch·∫•t l∆∞·ª£ng r·∫•t t·ªët, c√≥ th·ªÉ ti·∫øn h√†nh ph√¢n t√≠ch ngay."
        elif quality_score >= 80:
            quality_level = "üü° GOOD"
            recommendation = "D·ªØ li·ªáu c√≥ ch·∫•t l∆∞·ª£ng t·ªët, c√≥ th·ªÉ c·∫ßn m·ªôt s·ªë x·ª≠ l√Ω nh·ªè."
        elif quality_score >= 70:
            quality_level = "üü† FAIR"
            recommendation = "D·ªØ li·ªáu c·∫ßn ƒë∆∞·ª£c x·ª≠ l√Ω v√† l√†m s·∫°ch tr∆∞·ªõc khi ph√¢n t√≠ch."
        elif quality_score >= 60:
            quality_level = "üî¥ POOR"
            recommendation = "D·ªØ li·ªáu c√≥ nhi·ªÅu v·∫•n ƒë·ªÅ, c·∫ßn x·ª≠ l√Ω k·ªπ l∆∞·ª°ng."
        else:
            quality_level = "‚õî CRITICAL"
            recommendation = "D·ªØ li·ªáu c√≥ v·∫•n ƒë·ªÅ nghi√™m tr·ªçng, c·∫ßn xem x√©t l·∫°i ngu·ªìn d·ªØ li·ªáu."

        print(f"üìä Quality Level: {quality_level}")
        print(f"üí° Recommendation: {recommendation}")

        # Detailed summary
        print(f"\nüìã Chi ti·∫øt c√°c v·∫•n ƒë·ªÅ:")
        print(f"   - Missing Data: {missing_summary['overall_missing_pct']:.2f}%")
        print(f"   - Duplicate Rows: {duplicate_summary['full_duplicate_pct']:.2f}%")
        print(f"   - Consistency Issues: {consistency_summary['total_issues']}")
        print(f"   - Invalid Dates: {temporal_summary['invalid_dates']}")
        print(f"   - Missing Dates: {temporal_summary['missing_dates_count']}")



        return {
            'quality_score': quality_score,
            'quality_level': quality_level,
            'recommendation': recommendation,
            'basic_info': basic_info,
            'missing_summary': missing_summary,
            'duplicate_summary': duplicate_summary,
            'consistency_summary': consistency_summary,
            'temporal_summary': temporal_summary,
        }

# =============================================================================
# USAGE FUNCTION
# =============================================================================

def assess_data_quality(df, target_col='L∆∞·ª£ng m∆∞a', date_col='Ng√†y'):
    """
    Ch·∫°y ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu to√†n di·ªán

    Args:
        df: DataFrame c·∫ßn ƒë√°nh gi√°
        target_col: T√™n c·ªôt bi·∫øn m·ª•c ti√™u
        date_col: T√™n c·ªôt ng√†y th√°ng

    Returns:
        dict: B√°o c√°o chi ti·∫øt v·ªÅ ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu
    """

    print("üöÄ STARTING COMPREHENSIVE DATA QUALITY ASSESSMENT")
    print("="*80)

    # Initialize assessment
    dqa = DataQualityAssessment(df, target_col, date_col)

    # Generate comprehensive report
    quality_report = dqa.generate_quality_report()

    print("\n‚úÖ DATA QUALITY ASSESSMENT COMPLETED")
    print("="*80)

    return quality_report

# =============================================================================
# RUN ASSESSMENT ON YOUR DATA
# =============================================================================

# Ch·∫°y ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu cho DataFrame df_all
quality_report = assess_data_quality(df_all, target_col='L∆∞·ª£ng m∆∞a', date_col='Ng√†y')


